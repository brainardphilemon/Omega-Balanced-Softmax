{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchvision scikit-learn tqdm pandas matplotlib seaborn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Single-cell: strict dataset download + full-dataset update evaluation on Kaggle\n# Paste this whole cell into a Kaggle notebook and run.\n\n\nimport os, time, math, random, gc, sys\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\n\nprint(\"Torch\", torch.__version__)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE, torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:11:08.708038Z","iopub.execute_input":"2025-11-13T15:11:08.708511Z","iopub.status.idle":"2025-11-13T15:11:12.466557Z","shell.execute_reply.started":"2025-11-13T15:11:08.708487Z","shell.execute_reply":"2025-11-13T15:11:12.465816Z"}},"outputs":[{"name":"stdout","text":"Torch 2.6.0+cu124\nDevice: cuda Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# ----------------------\n# Strict download helper ‚Äî HALTS on failure\n# ----------------------\ndef try_download_torchvision(name, root='./data', retries=5, delay=2):\n    root = os.path.abspath(root)\n    os.makedirs(root, exist_ok=True)\n    for attempt in range(1, retries+1):\n        try:\n            if name == 'mnist':\n                datasets.MNIST(root, download=True, train=True, transform=transforms.ToTensor())\n                datasets.MNIST(root, download=True, train=False, transform=transforms.ToTensor())\n            elif name == 'fashionmnist':\n                datasets.FashionMNIST(root, download=True, train=True, transform=transforms.ToTensor())\n                datasets.FashionMNIST(root, download=True, train=False, transform=transforms.ToTensor())\n            elif name == 'svhn':\n                datasets.SVHN(root, download=True, split='train', transform=transforms.ToTensor())\n                datasets.SVHN(root, download=True, split='test', transform=transforms.ToTensor())\n            elif name == 'cifar10':\n                datasets.CIFAR10(root, download=True, train=True, transform=transforms.ToTensor())\n                datasets.CIFAR10(root, download=True, train=False, transform=transforms.ToTensor())\n            elif name == 'cifar100':\n                datasets.CIFAR100(root, download=True, train=True, transform=transforms.ToTensor())\n                datasets.CIFAR100(root, download=True, train=False, transform=transforms.ToTensor())\n            else:\n                raise ValueError(f\"Unknown dataset: {name}\")\n            print(f\"[‚úÖ OK] {name} downloaded and saved under {root}\")\n            return True\n        except Exception as e:\n            print(f\"[‚ö†Ô∏è attempt {attempt}/{retries}] {name} failed: {e}\")\n            if attempt < retries:\n                time.sleep(delay)\n    print(f\"[‚ùå FAILED] Could not download '{name}' after {retries} attempts.\")\n    print(\"üõë Halting execution. Kaggle may block external hosts (e.g., SVHN).\")\n    sys.exit(1)\n\n# ----------------------\n# Loss functions (unchanged)\n# ----------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma; self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma; self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights[y].to(logits.device)\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n\n        # -----------------------------------------------------------------\n        # ‚úÖ **THE FIX**: Convert the float (1.0 / tau) to a tensor \n        # *before* passing it to torch.log.\n        log_val = torch.log(torch.tensor(1.0 / tau, dtype=torch.float32))\n        # -----------------------------------------------------------------\n\n        raw = torch.where(p > tau,\n                          torch.log(1.0 / p),\n                          log_val + k * (p - tau)) # Use the new tensor variable\n                          \n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n        \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n        \n    def forward(self, logits, y, epoch=None, Ew=5):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        \n        # Ensure gamma is on the same device as logits\n        g = self.gamma_per_class.to(logits.device)[y] \n        \n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        return ( - ((1-pt)**g) * torch.log(pt) ).mean()\n\n# ----------------------\n# Long-tail helper\n# ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\n# ----------------------\n# Dataset loader ‚Äî STRICT, REAL, 3-CHANNEL\n# ----------------------\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n    try_download_torchvision(name_l, root=root)\n\n    if name_l == 'mnist':\n        tr = transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n        ])\n        train = datasets.MNIST(root, train=True, download=False, transform=tr)\n        test = datasets.MNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'fashionmnist':\n        tr = transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n        ])\n        train = datasets.FashionMNIST(root, train=True, download=False, transform=tr)\n        test = datasets.FashionMNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'svhn':\n        tr = transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x if x.size(0) == 3 else x.repeat(3, 1, 1))\n        ])\n        train = datasets.SVHN(root, split='train', download=False, transform=tr)\n        test = datasets.SVHN(root, split='test', download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'cifar10':\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor()\n        ])\n        test_tr = transforms.ToTensor()\n        base_train = datasets.CIFAR10(root, train=True, download=False, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=False, transform=test_tr)\n        C = 10\n        # ‚úÖ Critical fix: assign train = base_train for balanced case\n        train = base_train\n\n    elif name_l == 'cifar100':\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor()\n        ])\n        test_tr = transforms.ToTensor()\n        base_train = datasets.CIFAR100(root, train=True, download=False, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=False, transform=test_tr)\n        C = 100\n        # ‚úÖ Critical fix: assign train = base_train for balanced case\n        train = base_train\n\n    else:\n        raise ValueError(f\"Unsupported dataset: {name}\")\n\n    # Handle imbalance (CIFAR only)\n    if name_l in ('cifar10', 'cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        counts = cls_counts\n    else:\n        # Extract labels\n        if hasattr(train, 'targets'):\n            targets = np.array(train.targets)\n        elif hasattr(train, 'labels'):  # SVHN\n            targets = np.array(train.labels)\n        else:\n            targets = np.array([train[i][1] for i in range(len(train))])\n        counts = np.bincount(targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí Train: {len(train)}, Test: {len(test)}, Classes: {C}, Counts (first 10): {counts[:min(10, len(counts))]}\")\n    return train, test, counts\n\n# ----------------------\n# Exact full-dataset update via gradient accumulation\n# ----------------------\ndef train_batchwise_emulate_full(model, loss_fn, loader, opt, epoch=None, Ew=5, loss_tag=None):\n    model.train()\n    opt.zero_grad(set_to_none=True)\n    total_samples = 0\n    \n    # Use tqdm for progress, as this loop is now the main time sink\n    for xb, yb in tqdm(loader, desc=f\"Epoch {epoch+1} Train\", leave=False):\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        \n        logits = model(xb)\n        \n        if loss_tag == 'cdg':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=Ew)\n        else:\n            loss = loss_fn(logits, yb)\n            \n        bs = xb.size(0)\n        total_samples += bs\n        \n        (loss * bs).backward()  # accumulate scaled grads\n        \n        # -----------------------------------------------------------------\n        # ‚úÖ **THE FIX**: Explicitly free memory after each backward pass\n        # This is crucial to prevent the computation graph from\n        # holding onto memory across mini-batches.\n        del xb, yb, logits, loss\n        torch.cuda.empty_cache()\n        # -----------------------------------------------------------------\n\n    # Normalize to full-batch average\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.div_(total_samples)\n            \n    opt.step()\n\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE, non_blocking=True)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    preds, tg = np.array(preds), np.array(tg)\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\n# ----------------------\n# Model builder\n# ----------------------\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)  # avoids deprecation warning\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:39:08.567378Z","iopub.execute_input":"2025-11-13T15:39:08.567939Z","iopub.status.idle":"2025-11-13T15:39:08.598320Z","shell.execute_reply.started":"2025-11-13T15:39:08.567913Z","shell.execute_reply":"2025-11-13T15:39:08.597633Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ----------------------\n# Experiment config\n# ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nDATASETS = [\n    ('cifar10', 1),\n    ('cifar10', 100),\n    ('cifar100', 1),\n    ('cifar100', 100)\n]\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG': lambda counts: CDG_Focal(counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5)\n}\n\nEPOCHS = 1\nLR = 0.1\nTRAIN_BATCH = 256\n# larger = faster, still exact\nSEED = 0\nEw = 5\n\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\nsummary_rows = []\nprint(\"üöÄ Starting FULL-DATASET exact update evaluation (batchwise emulation)...\")\nprint(f\"Datasets: {[f'{d} (IF={i})' for d,i in DATASETS]}\")\n\nfor ds_name, IF in DATASETS:\n    print(\"\\n\" + \"=\"*65)\n    print(f\"üìÅ Dataset: {ds_name} | Imbalance Factor: {IF}\")\n    \n    train_ds, test_ds, counts = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    num_classes = len(counts)\n    \n    # Dataloaders (exact full-update compatible)\n    train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, \n                              shuffle=True, num_workers=2, pin_memory=True)\n    train_eval_loader = DataLoader(train_ds, batch_size=512, \n                                   shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=512, \n                             shuffle=False, num_workers=2, pin_memory=True)\n\n    for loss_name, loss_ctor in LOSSES.items():\n        print(f\"\\nüîç Loss: {loss_name}\")\n        torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n        \n        model = get_model(num_classes).to(DEVICE)\n        loss_fn = loss_ctor(counts)\n        loss_fn = loss_fn.to(DEVICE)\n        opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n        # Save CDG gamma if applicable\n        if loss_name == 'CDG' and hasattr(loss_fn, 'gamma_per_class'):\n            gamma_path = os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_gamma.npy\")\n            np.save(gamma_path, loss_fn.gamma_per_class.cpu().numpy())\n            print(f\"  ‚Üí Saved CDG gamma to {gamma_path}\")\n\n        rows = []\n        for ep in range(EPOCHS):\n            t0 = time.time()\n            \n            # ‚úÖ EXACT full-dataset update (gradient accumulation)\n            train_batchwise_emulate_full(\n                model, loss_fn, train_loader, opt,\n                epoch=ep, Ew=Ew, loss_tag=('cdg' if loss_name == 'CDG' else None)\n            )\n            \n            # Compute train loss (accurate)\n            model.eval()\n            train_loss = 0.0\n            with torch.no_grad():\n                for xb, yb in train_eval_loader:\n                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n                    logits = model(xb)\n                    l = loss_fn(logits, yb, epoch=ep, Ew=Ew) if loss_name == 'CDG' else loss_fn(logits, yb)\n                    train_loss += l.item() * xb.size(0)\n            train_loss /= len(train_ds)\n            model.train()\n            \n            # Evaluate\n            val_acc, macrof1 = evaluate_model(model, test_loader)\n            elapsed = time.time() - t0\n            scheduler.step()\n            \n            print(f\"  Epoch {ep+1}/{EPOCHS} | Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Macro F1: {macrof1:.4f} | {elapsed:.1f}s\")\n            rows.append({\n                'epoch': ep,\n                'train_loss': train_loss,\n                'val_acc': val_acc,\n                'macro_f1': macrof1\n            })\n\n        # Save results\n        pd.DataFrame(rows).to_csv(os.path.join(OUT, f\"{ds_name}_IF{IF}_{loss_name}.csv\"), index=False)\n        summary_rows.append({\n            'dataset': ds_name,\n            'IF': IF,\n            'loss': loss_name,\n            'val_acc': rows[-1]['val_acc'],\n            'macro_f1': rows[-1]['macro_f1']\n        })\n\n# ----------------------\n# Final summary & plots\n# ----------------------\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"summary_table.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(\"\\n\" + \"=\"*65)\nprint(\"‚úÖ All experiments completed!\")\nprint(f\"üìä Summary saved to: {summary_path}\")\n\n# Plot CDG gamma curves\ngamma_files = [f for f in os.listdir(OUT) if f.endswith('_CDG_gamma.npy')]\nfor gf in gamma_files:\n    arr = np.load(os.path.join(OUT, gf))\n    plt.figure(figsize=(6,2.5))\n    plt.plot(arr, marker='o', markersize=3, linewidth=1.2)\n    plt.title(f\"CDG Gamma ‚Äî {gf.replace('_CDG_gamma.npy', '')}\", fontsize=10)\n    plt.xlabel(\"Class\"); plt.ylabel(\"Œ≥\")\n    plt.grid(True, ls=':', alpha=0.6)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUT, gf.replace('.npy', '.png')), dpi=150)\n    plt.close()\n\n# Summary plot\nplt.figure(figsize=(12,5))\nsns.barplot(data=summary_df, x='loss', y='val_acc', hue='dataset')\nplt.title(\"Final Validation Accuracy (Exact Full-Dataset Updates)\", fontsize=14)\nplt.xticks(rotation=30)\nplt.ylabel(\"Accuracy\")\nplt.legend(title='Dataset (IF)', bbox_to_anchor=(1.02, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(os.path.join(OUT, 'summary_valacc_bar.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nüìÅ All outputs saved in:\\n{OUT}\")\nprint(\"üí° Note: All updates are EXACT full-dataset gradients (via accumulation), not mini-batch SGD.\")","metadata":{"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_327/1352289390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDATASET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'DATASET' is not defined"],"ename":"NameError","evalue":"name 'DATASET' is not defined","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# ‚úÖ Minimal code to resume training from CIFAR-10 onward\n# Assumes MNIST, FashionMNIST, SVHN already completed.\n# Uses existing /kaggle/working/data and /kaggle/working/loss_eval_results\n\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ----------------------\n# Required helpers (minimal)\n# ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    \n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights.to(logits.device)[y]\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        # Fix: ensure scalar is a tensor\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=5):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        return (- ((1-pt)**g) * torch.log(pt)).mean()\n\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef train_batchwise_emulate_full(model, loss_fn, loader, opt, epoch=None, Ew=5, loss_tag=None):\n    model.train()\n    opt.zero_grad(set_to_none=True)\n    total_samples = 0\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        logits = model(xb)\n        if loss_tag == 'cdg':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=Ew)\n        else:\n            loss = loss_fn(logits, yb)\n        bs = xb.size(0)\n        total_samples += bs\n        (loss * bs).backward()\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.div_(total_samples)\n    opt.step()\n\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE, non_blocking=True)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    preds, tg = np.array(preds), np.array(tg)\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n\n    if name_l == 'mnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.MNIST(root, train=True, download=False, transform=tr)\n        test = datasets.MNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'fashionmnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.FashionMNIST(root, train=True, download=False, transform=tr)\n        test = datasets.FashionMNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'svhn':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x if x.size(0)==3 else x.repeat(3,1,1)])\n        train = datasets.SVHN(root, split='train', download=False, transform=tr)\n        test = datasets.SVHN(root, split='test', download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'cifar10':\n        train_tr = transforms.Compose([transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n        test_tr = transforms.ToTensor()\n        base_train = datasets.CIFAR10(root, train=True, download=False, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=False, transform=test_tr)\n        C = 10\n        train = base_train\n\n    elif name_l == 'cifar100':\n        train_tr = transforms.Compose([transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n        test_tr = transforms.ToTensor()\n        base_train = datasets.CIFAR100(root, train=True, download=False, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=False, transform=test_tr)\n        C = 100\n        train = base_train\n\n    else:\n        raise ValueError(f\"Unsupported: {name}\")\n\n    if name_l in ('cifar10','cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        counts = cls_counts\n    else:\n        if hasattr(train, 'targets'):\n            targets = np.array(train.targets)\n        elif hasattr(train, 'labels'):\n            targets = np.array(train.labels)\n        else:\n            targets = np.array([train[i][1] for i in range(len(train))])\n        counts = np.bincount(targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí {name} (IF={imb_factor}): train={len(train)}, test={len(test)}, classes={C}\")\n    return train, test, counts\n\n# ----------------------\n# Config\n# ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG': lambda counts: CDG_Focal(counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5)\n}\n\nEPOCHS = 1\nLR = 0.1\nTRAIN_BATCH = 512\nSEED = 0\nEw = 5\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\nDATASETS_TO_RUN = [\n    ('cifar10', 1),\n    ('cifar10', 100),\n    ('cifar100', 1),\n    ('cifar100', 100)\n]\n\nsummary_rows = []\n\nprint(\"üöÄ Resuming training for remaining datasets...\")\nfor ds_name, IF in DATASETS_TO_RUN:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"üìÅ {ds_name} | IF={IF}\")\n    \n    try:\n        train_ds, test_ds, counts = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    except Exception as e:\n        print(f\"‚ùå Failed to load {ds_name}: {e}\")\n        continue\n\n    num_classes = len(counts)\n    train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n\n    for loss_name, loss_ctor in LOSSES.items():\n        print(f\"  üîç {loss_name}\")\n        torch.manual_seed(SEED)\n        np.random.seed(SEED)\n        random.seed(SEED)\n        \n        model = get_model(num_classes).to(DEVICE)\n        loss_fn = loss_ctor(counts)\n        opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n        if loss_name == 'CDG' and hasattr(loss_fn, 'gamma_per_class'):\n            np.save(os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n        for ep in range(EPOCHS):\n            t0 = time.time()\n            train_batchwise_emulate_full(model, loss_fn, train_loader, opt, \n                                         epoch=ep, Ew=Ew, loss_tag=('cdg' if loss_name=='CDG' else None))\n            \n            model.eval()\n            train_loss = 0.0\n            train_eval_loader = DataLoader(train_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n            with torch.no_grad():\n                for xb, yb in train_eval_loader:\n                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n                    logits = model(xb)\n                    if loss_name == 'CDG':\n                        l = loss_fn(logits, yb, epoch=ep, Ew=Ew)\n                    else:\n                        l = loss_fn(logits, yb)\n                    train_loss += l.item() * len(xb)\n            train_loss /= len(train_ds)\n            model.train()\n            \n            val_acc, macrof1 = evaluate_model(model, test_loader)\n            scheduler.step()\n            \n            print(f\"    Ep {ep+1}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f} | {time.time()-t0:.1f}s\")\n\n            if ep == EPOCHS - 1:\n                summary_rows.append({\n                    'dataset': ds_name,\n                    'IF': IF,\n                    'loss': loss_name,\n                    'val_acc': val_acc,\n                    'macro_f1': macrof1\n                })\n\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"summary_cifar_remaining.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ Remaining datasets completed. Summary saved to:\\n{summary_path}\")\n\nfull_summary_path = os.path.join(OUT, \"summary_table.csv\")\nif os.path.exists(full_summary_path):\n    prev = pd.read_csv(full_summary_path)\n    full = pd.concat([prev, summary_df], ignore_index=True)\n    full.to_csv(full_summary_path, index=False)\n    print(f\"‚úÖ Appended to full summary: {full_summary_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:51:20.877487Z","iopub.execute_input":"2025-11-13T15:51:20.878120Z","iopub.status.idle":"2025-11-13T15:52:43.587886Z","shell.execute_reply.started":"2025-11-13T15:51:20.878089Z","shell.execute_reply":"2025-11-13T15:52:43.587149Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ Resuming training for remaining datasets...\n\n============================================================\nüìÅ cifar10 | IF=1\n  ‚Üí cifar10 (IF=1): train=50000, test=10000, classes=10\n  üîç CE\n    Ep 1/1 | Loss: 4.3650 | Acc: 0.1262 | F1: 0.0580 | 15.0s\n  üîç Focal_g1\n    Ep 1/1 | Loss: 4.8829 | Acc: 0.1255 | F1: 0.0548 | 14.7s\n  üîç CBF_b0.999_g1\n    Ep 1/1 | Loss: 4.8829 | Acc: 0.1255 | F1: 0.0548 | 15.5s\n  üîç CDG\n    Ep 1/1 | Loss: 4.3435 | Acc: 0.1262 | F1: 0.0580 | 15.1s\n\n============================================================\nüìÅ cifar10 | IF=100\n  ‚Üí cifar10 (IF=100): train=12408, test=10000, classes=10\n  üîç CE\n    Ep 1/1 | Loss: 3.2030 | Acc: 0.1000 | F1: 0.0182 | 4.7s\n  üîç Focal_g1\n    Ep 1/1 | Loss: 3.5244 | Acc: 0.1000 | F1: 0.0182 | 4.6s\n  üîç CBF_b0.999_g1\n    Ep 1/1 | Loss: 0.5366 | Acc: 0.0983 | F1: 0.0202 | 4.8s\n  üîç CDG\n    Ep 1/1 | Loss: 3.2030 | Acc: 0.1000 | F1: 0.0182 | 4.9s\n\n============================================================\nüìÅ cifar100 | IF=1\n‚ùå Failed to load cifar100: Dataset not found or corrupted. You can use download=True to download it\n\n============================================================\nüìÅ cifar100 | IF=100\n‚ùå Failed to load cifar100: Dataset not found or corrupted. You can use download=True to download it\n\n‚úÖ Remaining datasets completed. Summary saved to:\n/kaggle/working/loss_eval_results/summary_cifar_remaining.csv\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ‚úÖ Minimal code to resume training from CIFAR-10 onward\n# Assumes MNIST, FashionMNIST, SVHN already completed.\n# Downloads CIFAR-10/100 if missing.\n\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ----------------------\n# Required helpers (minimal)\n# ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    \n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights.to(logits.device)[y]  # ‚úÖ Fix: move to device before indexing\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        # ‚úÖ Fix: log() requires tensor, not float\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=5):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        return (- ((1-pt)**g) * torch.log(pt)).mean()\n\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef train_batchwise_emulate_full(model, loss_fn, loader, opt, epoch=None, Ew=5, loss_tag=None):\n    model.train()\n    opt.zero_grad(set_to_none=True)\n    total_samples = 0\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        logits = model(xb)\n        if loss_tag == 'cdg':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=Ew)\n        else:\n            loss = loss_fn(logits, yb)\n        bs = xb.size(0)\n        total_samples += bs\n        (loss * bs).backward()\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.div_(total_samples)\n    opt.step()\n\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE, non_blocking=True)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    preds, tg = np.array(preds), np.array(tg)\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n\n    if name_l == 'mnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.MNIST(root, train=True, download=False, transform=tr)\n        test = datasets.MNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'fashionmnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.FashionMNIST(root, train=True, download=False, transform=tr)\n        test = datasets.FashionMNIST(root, train=False, download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'svhn':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x if x.size(0)==3 else x.repeat(3,1,1)])\n        train = datasets.SVHN(root, split='train', download=False, transform=tr)\n        test = datasets.SVHN(root, split='test', download=False, transform=tr)\n        C = 10\n\n    elif name_l == 'cifar10':\n        train_tr = transforms.Compose([transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n        test_tr = transforms.ToTensor()\n        # ‚úÖ Enable download\n        base_train = datasets.CIFAR10(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=True, transform=test_tr)\n        C = 10\n        train = base_train\n\n    elif name_l == 'cifar100':\n        train_tr = transforms.Compose([transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n        test_tr = transforms.ToTensor()\n        # ‚úÖ Enable download\n        base_train = datasets.CIFAR100(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=True, transform=test_tr)\n        C = 100\n        train = base_train\n\n    else:\n        raise ValueError(f\"Unsupported: {name}\")\n\n    # Handle imbalance\n    if name_l in ('cifar10','cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        counts = cls_counts\n    else:\n        if hasattr(train, 'targets'):\n            targets = np.array(train.targets)\n        elif hasattr(train, 'labels'):\n            targets = np.array(train.labels)\n        else:\n            targets = np.array([train[i][1] for i in range(len(train))])\n        counts = np.bincount(targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí {name} (IF={imb_factor}): train={len(train)}, test={len(test)}, classes={C}\")\n    return train, test, counts\n\n# ----------------------\n# Config\n# ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG': lambda counts: CDG_Focal(counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5)\n}\n\nEPOCHS = 6\nLR = 0.1\nTRAIN_BATCH = 512\nSEED = 0\nEw = 5\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\nDATASETS_TO_RUN = [\n   ('mnist', 1),\n  ('fashionmnist', 1),\n  ('svhn', 1),\n  ('cifar10', 1),\n  ('cifar10', 100),\n  ('cifar100', 1),\n  ('cifar100', 100)\n]\n\nsummary_rows = []\n\nprint(\"üöÄ Resuming training for remaining datasets...\")\nfor ds_name, IF in DATASETS_TO_RUN:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"üìÅ {ds_name} | IF={IF}\")\n    \n    try:\n        train_ds, test_ds, counts = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    except Exception as e:\n        print(f\"‚ùå Failed to load {ds_name}: {e}\")\n        continue\n\n    num_classes = len(counts)\n    train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n\n    for loss_name, loss_ctor in LOSSES.items():\n        print(f\"  üîç {loss_name}\")\n        torch.manual_seed(SEED)\n        np.random.seed(SEED)\n        random.seed(SEED)\n        \n        model = get_model(num_classes).to(DEVICE)\n        loss_fn = loss_ctor(counts)\n        opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n        if loss_name == 'CDG' and hasattr(loss_fn, 'gamma_per_class'):\n            np.save(os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n        for ep in range(EPOCHS):\n            t0 = time.time()\n            train_batchwise_emulate_full(model, loss_fn, train_loader, opt, \n                                         epoch=ep, Ew=Ew, loss_tag=('cdg' if loss_name=='CDG' else None))\n            \n            # Evaluate train loss accurately\n            model.eval()\n            train_loss = 0.0\n            train_eval_loader = DataLoader(train_ds, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n            with torch.no_grad():\n                for xb, yb in train_eval_loader:\n                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n                    logits = model(xb)\n                    if loss_name == 'CDG':\n                        l = loss_fn(logits, yb, epoch=ep, Ew=Ew)\n                    else:\n                        l = loss_fn(logits, yb)\n                    train_loss += l.item() * len(xb)\n            train_loss /= len(train_ds)\n            model.train()\n            \n            val_acc, macrof1 = evaluate_model(model, test_loader)\n            scheduler.step()\n            \n            print(f\"    Ep {ep+1}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f} | {time.time()-t0:.1f}s\")\n\n            if ep == EPOCHS - 1:\n                summary_rows.append({\n                    'dataset': ds_name,\n                    'IF': IF,\n                    'loss': loss_name,\n                    'val_acc': val_acc,\n                    'macro_f1': macrof1\n                })\n\n# Save summary\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"summary_cifar_remaining.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ Remaining datasets completed. Summary saved to:\\n{summary_path}\")\n\n# Optional: merge with full summary\nfull_summary_path = os.path.join(OUT, \"summary_table.csv\")\nif os.path.exists(full_summary_path):\n    prev = pd.read_csv(full_summary_path)\n    full = pd.concat([prev, summary_df], ignore_index=True)\n    full.to_csv(full_summary_path, index=False)\n    print(f\"‚úÖ Appended to full summary: {full_summary_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:40:50.258800Z","iopub.execute_input":"2025-11-13T16:40:50.259561Z","iopub.status.idle":"2025-11-13T16:41:53.477475Z","shell.execute_reply.started":"2025-11-13T16:40:50.259522Z","shell.execute_reply":"2025-11-13T16:41:53.476387Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ Resuming training for remaining datasets...\n\n============================================================\nüìÅ mnist | IF=1\n  ‚Üí mnist (IF=1): train=60000, test=10000, classes=10\n  üîç CE\n    Ep 1/6 | Loss: 5.4154 | Acc: 0.1845 | F1: 0.0903 | 14.3s\n    Ep 2/6 | Loss: 12.8347 | Acc: 0.1773 | F1: 0.0955 | 14.3s\n    Ep 3/6 | Loss: 2.1353 | Acc: 0.5089 | F1: 0.5159 | 14.3s\n    Ep 4/6 | Loss: 0.4306 | Acc: 0.8829 | F1: 0.8824 | 14.4s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_327/4004777311.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             train_batchwise_emulate_full(model, loss_fn, train_loader, opt, \n\u001b[0m\u001b[1;32m    262\u001b[0m                                          epoch=ep, Ew=Ew, loss_tag=('cdg' if loss_name=='CDG' else None))\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_327/4004777311.py\u001b[0m in \u001b[0;36mtrain_batchwise_emulate_full\u001b[0;34m(model, loss_fn, loader, opt, epoch, Ew, loss_tag)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"# ‚úÖ FULL WORKING CODE: All datasets, 100 epochs, early stopping, CDG fixed\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ---------------------- HELPER FUNCTIONS ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    \n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights.to(logits.device)[y]\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\n# ‚úÖ FIXED CDG_Focal: Numerically stable + device-safe\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype, device=p.device))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=5):\n        # ‚úÖ Numerically stable: use log_softmax directly\n        log_p = F.log_softmax(logits, dim=1)\n        log_pt = log_p.gather(1, y[:, None]).squeeze()\n        pt = torch.exp(log_pt).clamp(min=self.eps, max=1.0 - self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        loss = - ((1.0 - pt) ** g) * log_pt\n        return loss.mean()\n\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n# ‚úÖ FIXED: Added gradient clipping to prevent explosion\ndef train_one_epoch(model, loss_fn, loader, opt, epoch=None, Ew=5, loss_tag=None):\n    model.train()\n    total_loss = 0.0\n    total_samples = 0\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        logits = model(xb)\n        if loss_tag == 'cdg':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=Ew)\n        else:\n            loss = loss_fn(logits, yb)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        # üîë CRITICAL: Prevent gradient explosion\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n        total_loss += loss.item() * xb.size(0)\n        total_samples += xb.size(0)\n    return total_loss / total_samples\n\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE, non_blocking=True)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    preds, tg = np.array(preds), np.array(tg)\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n\n    if name_l == 'mnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.MNIST(root, train=True, download=True, transform=tr)\n        test = datasets.MNIST(root, train=False, download=True, transform=tr)\n        C = 10\n\n    elif name_l == 'fashionmnist':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x.repeat(3,1,1)])\n        train = datasets.FashionMNIST(root, train=True, download=True, transform=tr)\n        test = datasets.FashionMNIST(root, train=False, download=True, transform=tr)\n        C = 10\n\n    elif name_l == 'svhn':\n        tr = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), lambda x: x if x.size(0)==3 else x.repeat(3,1,1)])\n        train = datasets.SVHN(root, split='train', download=True, transform=tr)\n        test = datasets.SVHN(root, split='test', download=True, transform=tr)\n        C = 10\n\n    elif name_l == 'cifar10':\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        test_tr = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        base_train = datasets.CIFAR10(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=True, transform=test_tr)\n        C = 10\n        train = base_train\n\n    elif name_l == 'cifar100':\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n        ])\n        test_tr = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n        ])\n        base_train = datasets.CIFAR100(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=True, transform=test_tr)\n        C = 100\n        train = base_train\n\n    else:\n        raise ValueError(f\"Unsupported: {name}\")\n\n    if name_l in ('cifar10','cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        counts = cls_counts\n    else:\n        if hasattr(train, 'targets'):\n            targets = np.array(train.targets)\n        elif hasattr(train, 'labels'):\n            targets = np.array(train.labels)\n        else:\n            targets = np.array([train[i][1] for i in range(len(train))])\n        counts = np.bincount(targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí {name} (IF={imb_factor}): train={len(train)}, test={len(test)}, classes={C}\")\n    return train, test, counts\n\n# ---------------------- CONFIG ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG': lambda counts: CDG_Focal(counts, tau=1.0, k=0.0, gamma_min=0.75, gamma_max=2.5)\n}\n\nEPOCHS = 100\nPATIENCE = 5\nLR = 0.01  # Stable LR for all losses\nTRAIN_BATCH = 256\nSEED = 0\nEw = 5\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\n# ‚úÖ ALL DATASETS (as per your current run)\nDATASETS_TO_RUN = [\n    ('mnist', 1),\n    ('fashionmnist', 1),\n    ('svhn', 1),\n    ('cifar10', 1),\n    ('cifar10', 100),\n    ('cifar100', 1),\n    ('cifar100', 100)\n]\n\nsummary_rows = []\nprint(\"üöÄ Starting full training with early stopping (100 epochs, patience=5)...\")\n\n# ---------------------- MAIN LOOP ----------------------\nfor ds_name, IF in DATASETS_TO_RUN:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"üìÅ {ds_name} | IF={IF}\")\n    \n    try:\n        train_ds, test_ds, counts = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    except Exception as e:\n        print(f\"‚ùå Failed to load {ds_name}: {e}\")\n        continue\n\n    num_classes = len(counts)\n    train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\n    for loss_name, loss_ctor in LOSSES.items():\n        print(f\"\\n  üîç Training with {loss_name}\")\n        torch.manual_seed(SEED)\n        np.random.seed(SEED)\n        random.seed(SEED)\n        \n        model = get_model(num_classes).to(DEVICE)\n        loss_fn = loss_ctor(counts)\n        opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n        best_val_acc = 0.0\n        epochs_no_improve = 0\n        best_model_path = os.path.join(OUT, f\"{ds_name}_IF{IF}_{loss_name}_best.pth\")\n\n        # Save CDG gamma if applicable\n        if loss_name == 'CDG' and hasattr(loss_fn, 'gamma_per_class'):\n            np.save(os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n        for ep in range(EPOCHS):\n            t0 = time.time()\n            train_loss = train_one_epoch(\n                model, loss_fn, train_loader, opt,\n                epoch=ep, Ew=Ew, loss_tag=('cdg' if loss_name=='CDG' else None)\n            )\n            \n            val_acc, macrof1 = evaluate_model(model, test_loader)\n            scheduler.step()\n            \n            # Early stopping\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                best_macrof1 = macrof1\n                best_epoch = ep\n                epochs_no_improve = 0\n                torch.save(model.state_dict(), best_model_path)\n                improved = \"‚úÖ\"\n            else:\n                epochs_no_improve += 1\n                improved = \"  \"\n            \n            print(f\"    Ep {ep+1:3d}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f} | {time.time()-t0:.1f}s {improved}\")\n            \n            if epochs_no_improve >= PATIENCE:\n                print(f\"    ‚èπÔ∏è  Early stopping at epoch {ep+1}. Best: {best_epoch+1}\")\n                break\n\n        # Final evaluation with best model\n        model.load_state_dict(torch.load(best_model_path))\n        final_acc, final_f1 = evaluate_model(model, test_loader)\n        print(f\"    üèÜ Final (best) | Acc: {final_acc:.4f} | F1: {final_f1:.4f}\")\n\n        summary_rows.append({\n            'dataset': ds_name,\n            'IF': IF,\n            'loss': loss_name,\n            'val_acc': final_acc,\n            'macro_f1': final_f1,\n            'best_epoch': best_epoch + 1\n        })\n\n# ---------------------- SAVE RESULTS ----------------------\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"summary_full_results.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ Training completed! Results saved to:\\n{summary_path}\")\n\n# Append to master summary if exists\nfull_summary_path = os.path.join(OUT, \"summary_table.csv\")\nif os.path.exists(full_summary_path):\n    prev = pd.read_csv(full_summary_path)\n    full = pd.concat([prev, summary_df], ignore_index=True)\n    full.to_csv(full_summary_path, index=False)\n    print(f\"‚úÖ Appended to master summary: {full_summary_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:03:37.418624Z","iopub.execute_input":"2025-11-13T17:03:37.418943Z","iopub.status.idle":"2025-11-13T19:59:10.842628Z","shell.execute_reply.started":"2025-11-13T17:03:37.418909Z","shell.execute_reply":"2025-11-13T19:59:10.841750Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ Starting full training with early stopping (100 epochs, patience=5)...\n\n============================================================\nüìÅ mnist | IF=1\n  ‚Üí mnist (IF=1): train=60000, test=10000, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 0.3073 | Acc: 0.9743 | F1: 0.9742 | 11.0s ‚úÖ\n    Ep   2/100 | Loss: 0.0452 | Acc: 0.9852 | F1: 0.9851 | 10.8s ‚úÖ\n    Ep   3/100 | Loss: 0.0276 | Acc: 0.9903 | F1: 0.9903 | 10.8s ‚úÖ\n    Ep   4/100 | Loss: 0.0166 | Acc: 0.9899 | F1: 0.9899 | 10.7s   \n    Ep   5/100 | Loss: 0.0122 | Acc: 0.9908 | F1: 0.9908 | 10.9s ‚úÖ\n    Ep   6/100 | Loss: 0.0086 | Acc: 0.9908 | F1: 0.9907 | 10.7s   \n    Ep   7/100 | Loss: 0.0058 | Acc: 0.9917 | F1: 0.9916 | 10.9s ‚úÖ\n    Ep   8/100 | Loss: 0.0045 | Acc: 0.9846 | F1: 0.9844 | 10.7s   \n    Ep   9/100 | Loss: 0.0028 | Acc: 0.9924 | F1: 0.9923 | 10.9s ‚úÖ\n    Ep  10/100 | Loss: 0.0027 | Acc: 0.9917 | F1: 0.9916 | 10.7s   \n    Ep  11/100 | Loss: 0.0011 | Acc: 0.9925 | F1: 0.9924 | 10.8s ‚úÖ\n    Ep  12/100 | Loss: 0.0009 | Acc: 0.9924 | F1: 0.9923 | 10.8s   \n    Ep  13/100 | Loss: 0.0005 | Acc: 0.9925 | F1: 0.9924 | 10.7s   \n    Ep  14/100 | Loss: 0.0002 | Acc: 0.9930 | F1: 0.9929 | 10.8s ‚úÖ\n    Ep  15/100 | Loss: 0.0002 | Acc: 0.9929 | F1: 0.9928 | 10.7s   \n    Ep  16/100 | Loss: 0.0002 | Acc: 0.9928 | F1: 0.9927 | 10.7s   \n    Ep  17/100 | Loss: 0.0002 | Acc: 0.9926 | F1: 0.9925 | 10.7s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9930 | F1: 0.9929 | 10.8s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9934 | 10.8s ‚úÖ\n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 10.7s   \n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 10.7s   \n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9932 | 10.7s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9932 | 10.8s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9934 | F1: 0.9933 | 10.7s   \n    ‚èπÔ∏è  Early stopping at epoch 24. Best: 19\n    üèÜ Final (best) | Acc: 0.9935 | F1: 0.9934\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 0.2436 | Acc: 0.9736 | F1: 0.9736 | 10.8s ‚úÖ\n    Ep   2/100 | Loss: 0.0317 | Acc: 0.9840 | F1: 0.9839 | 10.8s ‚úÖ\n    Ep   3/100 | Loss: 0.0187 | Acc: 0.9862 | F1: 0.9860 | 10.8s ‚úÖ\n    Ep   4/100 | Loss: 0.0110 | Acc: 0.9888 | F1: 0.9888 | 10.8s ‚úÖ\n    Ep   5/100 | Loss: 0.0064 | Acc: 0.9888 | F1: 0.9888 | 10.8s   \n    Ep   6/100 | Loss: 0.0033 | Acc: 0.9898 | F1: 0.9897 | 10.9s ‚úÖ\n    Ep   7/100 | Loss: 0.0032 | Acc: 0.9906 | F1: 0.9905 | 10.8s ‚úÖ\n    Ep   8/100 | Loss: 0.0017 | Acc: 0.9913 | F1: 0.9913 | 10.8s ‚úÖ\n    Ep   9/100 | Loss: 0.0009 | Acc: 0.9919 | F1: 0.9919 | 10.8s ‚úÖ\n    Ep  10/100 | Loss: 0.0006 | Acc: 0.9934 | F1: 0.9933 | 10.9s ‚úÖ\n    Ep  11/100 | Loss: 0.0004 | Acc: 0.9930 | F1: 0.9929 | 10.8s   \n    Ep  12/100 | Loss: 0.0001 | Acc: 0.9930 | F1: 0.9929 | 10.7s   \n    Ep  13/100 | Loss: 0.0001 | Acc: 0.9928 | F1: 0.9927 | 10.8s   \n    Ep  14/100 | Loss: 0.0001 | Acc: 0.9929 | F1: 0.9928 | 10.7s   \n    Ep  15/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9931 | 10.7s   \n    ‚èπÔ∏è  Early stopping at epoch 15. Best: 10\n    üèÜ Final (best) | Acc: 0.9934 | F1: 0.9933\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 0.2435 | Acc: 0.9771 | F1: 0.9769 | 11.2s ‚úÖ\n    Ep   2/100 | Loss: 0.0316 | Acc: 0.9839 | F1: 0.9838 | 11.1s ‚úÖ\n    Ep   3/100 | Loss: 0.0177 | Acc: 0.9884 | F1: 0.9882 | 11.2s ‚úÖ\n    Ep   4/100 | Loss: 0.0112 | Acc: 0.9881 | F1: 0.9881 | 11.0s   \n    Ep   5/100 | Loss: 0.0059 | Acc: 0.9896 | F1: 0.9896 | 11.2s ‚úÖ\n    Ep   6/100 | Loss: 0.0036 | Acc: 0.9901 | F1: 0.9900 | 11.1s ‚úÖ\n    Ep   7/100 | Loss: 0.0024 | Acc: 0.9912 | F1: 0.9911 | 11.1s ‚úÖ\n    Ep   8/100 | Loss: 0.0015 | Acc: 0.9904 | F1: 0.9904 | 11.0s   \n    Ep   9/100 | Loss: 0.0008 | Acc: 0.9909 | F1: 0.9908 | 11.0s   \n    Ep  10/100 | Loss: 0.0004 | Acc: 0.9913 | F1: 0.9912 | 11.1s ‚úÖ\n    Ep  11/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 11.1s ‚úÖ\n    Ep  12/100 | Loss: 0.0001 | Acc: 0.9914 | F1: 0.9913 | 11.1s   \n    Ep  13/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 11.2s ‚úÖ\n    Ep  14/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9921 | 11.0s ‚úÖ\n    Ep  15/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 11.0s   \n    Ep  16/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 11.2s   \n    Ep  17/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 11.1s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 11.0s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 11.0s   \n    ‚èπÔ∏è  Early stopping at epoch 19. Best: 14\n    üèÜ Final (best) | Acc: 0.9922 | F1: 0.9921\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 0.3078 | Acc: 0.9782 | F1: 0.9781 | 11.3s ‚úÖ\n    Ep   2/100 | Loss: 0.0432 | Acc: 0.9844 | F1: 0.9844 | 11.2s ‚úÖ\n    Ep   3/100 | Loss: 0.0238 | Acc: 0.9896 | F1: 0.9895 | 11.2s ‚úÖ\n    Ep   4/100 | Loss: 0.0126 | Acc: 0.9867 | F1: 0.9866 | 11.1s   \n    Ep   5/100 | Loss: 0.0072 | Acc: 0.9856 | F1: 0.9855 | 11.1s   \n    Ep   6/100 | Loss: 0.0042 | Acc: 0.9911 | F1: 0.9911 | 11.1s ‚úÖ\n    Ep   7/100 | Loss: 0.0031 | Acc: 0.9917 | F1: 0.9916 | 11.1s ‚úÖ\n    Ep   8/100 | Loss: 0.0021 | Acc: 0.9889 | F1: 0.9888 | 11.0s   \n    Ep   9/100 | Loss: 0.0015 | Acc: 0.9906 | F1: 0.9906 | 11.0s   \n    Ep  10/100 | Loss: 0.0008 | Acc: 0.9918 | F1: 0.9918 | 11.1s ‚úÖ\n    Ep  11/100 | Loss: 0.0007 | Acc: 0.9921 | F1: 0.9920 | 11.3s ‚úÖ\n    Ep  12/100 | Loss: 0.0003 | Acc: 0.9928 | F1: 0.9927 | 11.1s ‚úÖ\n    Ep  13/100 | Loss: 0.0002 | Acc: 0.9927 | F1: 0.9927 | 11.1s   \n    Ep  14/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9932 | 11.1s ‚úÖ\n    Ep  15/100 | Loss: 0.0001 | Acc: 0.9934 | F1: 0.9934 | 11.1s ‚úÖ\n    Ep  16/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9933 | 11.1s   \n    Ep  17/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9933 | 10.9s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9934 | F1: 0.9934 | 11.0s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9935 | 11.2s ‚úÖ\n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9936 | F1: 0.9936 | 11.1s ‚úÖ\n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9935 | 11.2s   \n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9935 | 11.0s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9934 | 11.1s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9935 | 11.1s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9932 | 11.0s   \n    ‚èπÔ∏è  Early stopping at epoch 25. Best: 20\n    üèÜ Final (best) | Acc: 0.9936 | F1: 0.9936\n\n============================================================\nüìÅ fashionmnist | IF=1\n  ‚Üí fashionmnist (IF=1): train=60000, test=10000, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 0.6092 | Acc: 0.8346 | F1: 0.8342 | 10.8s ‚úÖ\n    Ep   2/100 | Loss: 0.3302 | Acc: 0.8770 | F1: 0.8746 | 10.9s ‚úÖ\n    Ep   3/100 | Loss: 0.2770 | Acc: 0.8877 | F1: 0.8874 | 10.9s ‚úÖ\n    Ep   4/100 | Loss: 0.2385 | Acc: 0.8788 | F1: 0.8768 | 10.8s   \n    Ep   5/100 | Loss: 0.2067 | Acc: 0.8869 | F1: 0.8863 | 10.7s   \n    Ep   6/100 | Loss: 0.1795 | Acc: 0.8776 | F1: 0.8791 | 10.7s   \n    Ep   7/100 | Loss: 0.1564 | Acc: 0.8943 | F1: 0.8938 | 10.8s ‚úÖ\n    Ep   8/100 | Loss: 0.1348 | Acc: 0.8876 | F1: 0.8874 | 10.7s   \n    Ep   9/100 | Loss: 0.1191 | Acc: 0.8965 | F1: 0.8965 | 10.8s ‚úÖ\n    Ep  10/100 | Loss: 0.1042 | Acc: 0.8931 | F1: 0.8916 | 10.8s   \n    Ep  11/100 | Loss: 0.0946 | Acc: 0.8917 | F1: 0.8908 | 10.8s   \n    Ep  12/100 | Loss: 0.0846 | Acc: 0.8961 | F1: 0.8948 | 10.8s   \n    Ep  13/100 | Loss: 0.0755 | Acc: 0.8919 | F1: 0.8932 | 10.8s   \n    Ep  14/100 | Loss: 0.0661 | Acc: 0.8766 | F1: 0.8752 | 10.7s   \n    ‚èπÔ∏è  Early stopping at epoch 14. Best: 9\n    üèÜ Final (best) | Acc: 0.8965 | F1: 0.8965\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 0.4551 | Acc: 0.8404 | F1: 0.8404 | 10.8s ‚úÖ\n    Ep   2/100 | Loss: 0.2252 | Acc: 0.8477 | F1: 0.8471 | 10.9s ‚úÖ\n    Ep   3/100 | Loss: 0.1813 | Acc: 0.8703 | F1: 0.8685 | 10.8s ‚úÖ\n    Ep   4/100 | Loss: 0.1521 | Acc: 0.8787 | F1: 0.8798 | 10.8s ‚úÖ\n    Ep   5/100 | Loss: 0.1311 | Acc: 0.8884 | F1: 0.8880 | 10.9s ‚úÖ\n    Ep   6/100 | Loss: 0.1122 | Acc: 0.8822 | F1: 0.8837 | 10.7s   \n    Ep   7/100 | Loss: 0.0955 | Acc: 0.8873 | F1: 0.8862 | 10.8s   \n    Ep   8/100 | Loss: 0.0811 | Acc: 0.8849 | F1: 0.8844 | 10.8s   \n    Ep   9/100 | Loss: 0.0719 | Acc: 0.8944 | F1: 0.8943 | 10.9s ‚úÖ\n    Ep  10/100 | Loss: 0.0648 | Acc: 0.8927 | F1: 0.8922 | 10.8s   \n    Ep  11/100 | Loss: 0.0568 | Acc: 0.8952 | F1: 0.8951 | 10.8s ‚úÖ\n    Ep  12/100 | Loss: 0.0500 | Acc: 0.8925 | F1: 0.8916 | 10.7s   \n    Ep  13/100 | Loss: 0.0449 | Acc: 0.8886 | F1: 0.8890 | 10.8s   \n    Ep  14/100 | Loss: 0.0406 | Acc: 0.8830 | F1: 0.8811 | 10.7s   \n    Ep  15/100 | Loss: 0.0388 | Acc: 0.9009 | F1: 0.9008 | 10.8s ‚úÖ\n    Ep  16/100 | Loss: 0.0342 | Acc: 0.8844 | F1: 0.8860 | 10.7s   \n    Ep  17/100 | Loss: 0.0331 | Acc: 0.8983 | F1: 0.8983 | 10.8s   \n    Ep  18/100 | Loss: 0.0296 | Acc: 0.8960 | F1: 0.8961 | 10.7s   \n    Ep  19/100 | Loss: 0.0288 | Acc: 0.8953 | F1: 0.8956 | 10.7s   \n    Ep  20/100 | Loss: 0.0255 | Acc: 0.8973 | F1: 0.8978 | 10.7s   \n    ‚èπÔ∏è  Early stopping at epoch 20. Best: 15\n    üèÜ Final (best) | Acc: 0.9009 | F1: 0.9008\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 0.4525 | Acc: 0.8448 | F1: 0.8454 | 11.0s ‚úÖ\n    Ep   2/100 | Loss: 0.2228 | Acc: 0.8422 | F1: 0.8408 | 11.0s   \n    Ep   3/100 | Loss: 0.1805 | Acc: 0.8791 | F1: 0.8797 | 11.4s ‚úÖ\n    Ep   4/100 | Loss: 0.1523 | Acc: 0.8779 | F1: 0.8792 | 11.0s   \n    Ep   5/100 | Loss: 0.1313 | Acc: 0.8811 | F1: 0.8790 | 11.2s ‚úÖ\n    Ep   6/100 | Loss: 0.1108 | Acc: 0.8835 | F1: 0.8832 | 11.2s ‚úÖ\n    Ep   7/100 | Loss: 0.0948 | Acc: 0.8914 | F1: 0.8902 | 11.1s ‚úÖ\n    Ep   8/100 | Loss: 0.0816 | Acc: 0.8769 | F1: 0.8747 | 11.1s   \n    Ep   9/100 | Loss: 0.0716 | Acc: 0.8935 | F1: 0.8933 | 11.1s ‚úÖ\n    Ep  10/100 | Loss: 0.0614 | Acc: 0.8967 | F1: 0.8960 | 11.1s ‚úÖ\n    Ep  11/100 | Loss: 0.0558 | Acc: 0.8863 | F1: 0.8845 | 11.1s   \n    Ep  12/100 | Loss: 0.0492 | Acc: 0.8927 | F1: 0.8929 | 11.0s   \n    Ep  13/100 | Loss: 0.0456 | Acc: 0.8829 | F1: 0.8844 | 11.0s   \n    Ep  14/100 | Loss: 0.0400 | Acc: 0.8786 | F1: 0.8782 | 11.1s   \n    Ep  15/100 | Loss: 0.0382 | Acc: 0.8992 | F1: 0.8990 | 11.1s ‚úÖ\n    Ep  16/100 | Loss: 0.0345 | Acc: 0.8904 | F1: 0.8895 | 11.0s   \n    Ep  17/100 | Loss: 0.0328 | Acc: 0.9000 | F1: 0.8993 | 11.1s ‚úÖ\n    Ep  18/100 | Loss: 0.0299 | Acc: 0.8987 | F1: 0.8992 | 11.0s   \n    Ep  19/100 | Loss: 0.0260 | Acc: 0.8948 | F1: 0.8943 | 11.0s   \n    Ep  20/100 | Loss: 0.0267 | Acc: 0.8998 | F1: 0.8997 | 11.1s   \n    Ep  21/100 | Loss: 0.0241 | Acc: 0.9027 | F1: 0.9022 | 11.1s ‚úÖ\n    Ep  22/100 | Loss: 0.0221 | Acc: 0.8961 | F1: 0.8956 | 11.2s   \n    Ep  23/100 | Loss: 0.0216 | Acc: 0.8965 | F1: 0.8949 | 10.9s   \n    Ep  24/100 | Loss: 0.0185 | Acc: 0.8994 | F1: 0.8985 | 11.0s   \n    Ep  25/100 | Loss: 0.0177 | Acc: 0.8987 | F1: 0.8981 | 11.1s   \n    Ep  26/100 | Loss: 0.0179 | Acc: 0.8961 | F1: 0.8964 | 11.1s   \n    ‚èπÔ∏è  Early stopping at epoch 26. Best: 21\n    üèÜ Final (best) | Acc: 0.9027 | F1: 0.9022\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 0.6060 | Acc: 0.8487 | F1: 0.8477 | 11.2s ‚úÖ\n    Ep   2/100 | Loss: 0.3152 | Acc: 0.8678 | F1: 0.8678 | 11.2s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():Exception ignored in: \n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^^\n^ ^^ ^ ^ ^ ^\n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n ^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^ ^ ^ ^ \n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n     ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^^ ^ ^ ^  ^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError: ^can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\nTraceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    self._shutdown_workers()^\n\nAssertionError  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n:     can only test a child processif w.is_alive():\n\n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n ^    ^self._shutdown_workers()^\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^^if w.is_alive():\n^ ^ ^ ^\n    File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n      assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^  ^ ^ ^ ^ ^  ^^ ^ ^ ^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^^ ^  ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   3/100 | Loss: 0.2406 | Acc: 0.8681 | F1: 0.8653 | 12.0s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n     Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^^\n^ ^^ ^  ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^  ^ ^ \nAssertionError :  can only test a child process\n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    ^^self._shutdown_workers()\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    \nassert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n: can only test a child process \n  Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \nTraceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers()\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^ ^^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ \n AssertionError :  can only test a child process \n      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   4/100 | Loss: 0.1870 | Acc: 0.8788 | F1: 0.8792 | 11.8s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():  \n     ^ ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^  ^ ^ ^ ^ ^ \n AssertionError ^: ^can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^  ^^ ^  ^ ^ ^^^^^^^^^^^^^^^^^^\n^^AssertionError^: \ncan only test a child process  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\n    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive(): \n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n ^ ^^ ^ ^ ^ \nAssertionError :  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   5/100 | Loss: 0.1480 | Acc: 0.8881 | F1: 0.8874 | 11.7s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    assert self._parent_pid == os.getpid(), 'can only test a child process'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()  \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive(): \n    ^ ^ ^ ^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^^  ^^  ^^ ^  ^^ \n AssertionError :  ^can only test a child process\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^\n^AssertionError: ^can only test a child process^\n^^Exception ignored in: ^\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nTraceback (most recent call last):\n      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nassert self._parent_pid == os.getpid(), 'can only test a child process'\n     self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive(): \n       ^  ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n ^ ^ ^ ^^  ^ ^ ^ ^ ^ ^^^^\n^AssertionError^^: ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   6/100 | Loss: 0.1223 | Acc: 0.8927 | F1: 0.8925 | 11.7s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n     ^if w.is_alive():^\n^ ^ ^ ^ ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^^ ^ ^ \n AssertionError :   can only test a child process\n  Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\nTraceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nAssertionError:     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n\n  Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n  Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n ^    ^if w.is_alive():^\n ^ ^ ^ ^ ^^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^  ^ \n AssertionError : can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   7/100 | Loss: 0.1047 | Acc: 0.8947 | F1: 0.8931 | 11.7s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():  \n     ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n ^ ^^  ^ ^ ^ ^ ^ ^ ^ ^^\n^AssertionError^: ^can only test a child process\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    self._shutdown_workers()^^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^\n^AssertionError^^: ^can only test a child process^\n^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\nself._shutdown_workers()\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n              ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^^ ^ ^ ^ ^^ ^ ^ ^ ^^\n^AssertionError: ^^can only test a child process\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   8/100 | Loss: 0.0913 | Acc: 0.8947 | F1: 0.8947 | 11.7s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \nTraceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()  \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():^\n^ ^^ ^ ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n ^ ^  \n AssertionError :  can only test a child process \n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^^if w.is_alive():^\n^  ^ ^ ^^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    \nassert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n:  can only test a child process \n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \nTraceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^^ ^ ^^^  ^^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n ^ \n AssertionError :  can only test a child process  \n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep   9/100 | Loss: 0.0798 | Acc: 0.8921 | F1: 0.8922 | 11.6s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nException ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n     ^if w.is_alive():^\n^ ^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n ^ ^^ ^ \n AssertionError :   can only test a child process \n  Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    if w.is_alive():^\n ^^  ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\n    AssertionError: assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n\n  Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \n Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^ ^^ ^ ^ ^ \n AssertionError :  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  10/100 | Loss: 0.0716 | Acc: 0.8954 | F1: 0.8940 | 11.7s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^Exception ignored in: ^\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\n    assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()  \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n     if w.is_alive(): \n        ^^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^\n^AssertionError^: ^can only test a child process^\n^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^ ^  ^ ^ ^^^^^^^^^^\n^^AssertionError^: ^^can only test a child process^\n^Exception ignored in: \n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    \nTraceback (most recent call last):\nassert self._parent_pid == os.getpid(), 'can only test a child process'\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive(): \n      ^ ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n ^ ^ ^^  ^^  ^ ^ ^ ^ ^^\n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  11/100 | Loss: 0.0631 | Acc: 0.8970 | F1: 0.8961 | 11.7s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nException ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n  Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()  \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():^\n^ ^ ^ ^ ^ ^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^  \n AssertionError :  can only test a child process\n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> ^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    if w.is_alive():^^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nAssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n\n   Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n  Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n\n AssertionError : can only test a child process \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  12/100 | Loss: 0.0558 | Acc: 0.8921 | F1: 0.8898 | 11.7s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n\n  Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n^ ^ ^^  ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^\n AssertionError :  can only test a child process \n  Exception ignored in:  ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\nTraceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^^self._shutdown_workers()\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^\n\nAssertionError  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n:     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n\nException ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n  Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():^\n^^ ^ ^ ^ ^  ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^  ^ ^ \n  AssertionError:  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  13/100 | Loss: 0.0508 | Acc: 0.8947 | F1: 0.8945 | 11.6s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()  \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():  \n  ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^^  ^ ^ ^^  \nAssertionError  :  can only test a child process \n Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^ ^ ^ ^  ^^  ^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nAssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n\n Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n     ^if w.is_alive():^\n^ ^ ^ ^ ^ ^^  ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^ ^ ^ \n AssertionError  :  can only test a child process \n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  14/100 | Loss: 0.0479 | Acc: 0.8960 | F1: 0.8957 | 11.6s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n     ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^ ^^ ^ \n AssertionError :  can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^^self._shutdown_workers()\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^^if w.is_alive():^\n^ ^ ^  ^^ ^^ ^ ^^^^^^^^^^^^\nAssertionError^: ^^can only test a child process^\n^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\nTraceback (most recent call last):\n      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\nassert self._parent_pid == os.getpid(), 'can only test a child process'\n     self._shutdown_workers()  \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n          ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^ ^^  ^ ^ ^ \n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  15/100 | Loss: 0.0422 | Acc: 0.8976 | F1: 0.8975 | 11.9s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive():^\n^^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ \n  AssertionError  :  can only test a child process\n  Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\nTraceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^self._shutdown_workers()^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():^\n^^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssertionError  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n: can only test a child process    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \nTraceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n^ ^^  ^^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^^  ^ ^ ^ \nAssertionError :  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  16/100 | Loss: 0.0386 | Acc: 0.8942 | F1: 0.8951 | 11.7s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      ^if w.is_alive():\n^ ^ ^ ^^  ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^^  ^ \n AssertionError :  can only test a child process\n  Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():\n^ ^ ^^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^\nAssertionError  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n: can only test a child process    assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \n Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers()\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^ ^  ^ ^ ^  \n AssertionError : can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  17/100 | Loss: 0.0351 | Acc: 0.9001 | F1: 0.9003 | 11.8s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>  \nTraceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive():\n^ ^^ ^ ^  ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^  ^^ ^ ^ \n  AssertionError :  can only test a child process^^\n^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^  ^^^^^^^^^^^^^^^^^^^^^\n^^AssertionError\n:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\ncan only test a child process    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\n Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers()\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      ^if w.is_alive():\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^^ ^ ^ ^ ^ ^ ^ \n AssertionError :  can only test a child process ^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  18/100 | Loss: 0.0326 | Acc: 0.8999 | F1: 0.9002 | 11.7s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  19/100 | Loss: 0.0311 | Acc: 0.8987 | F1: 0.8983 | 11.9s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\nTraceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive():\n       ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^^   ^ ^ ^^ \n^AssertionError^: ^can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^  ^ ^ ^ ^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^\nException ignored in:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive():\n        ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^^  ^ ^ ^ ^ ^ ^ ^ ^ ^ \n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  20/100 | Loss: 0.0279 | Acc: 0.9011 | F1: 0.9010 | 11.6s ‚úÖ\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n  Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^^ \n AssertionError :  can only test a child process \n   Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n^Traceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^^    ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: \ncan only test a child process  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\n    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\n \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n  ^^  ^ ^^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^ ^ \n AssertionError  :  can only test a child process\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  21/100 | Loss: 0.0272 | Acc: 0.8982 | F1: 0.8976 | 12.0s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n      Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    if w.is_alive():^^\n \n   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n      assert self._parent_pid == os.getpid(), 'can only test a child process' \n    ^ ^ ^ ^ ^^ ^ ^ ^ ^ ^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n^ ^ ^^  ^ ^ ^^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  22/100 | Loss: 0.0256 | Acc: 0.8988 | F1: 0.8986 | 12.0s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nException ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers()\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n^  ^ ^  ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^  ^^ ^ ^ \nAssertionError  :  can only test a child process \n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^^ ^ ^ ^  ^ ^^^^^^^^^^^^^^^^^^^^^\n^AssertionError\n:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\ncan only test a child process    \nassert self._parent_pid == os.getpid(), 'can only test a child process'\nException ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      ^if w.is_alive():^\n^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^^  ^^ ^ \n AssertionError :  can only test a child process \n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  23/100 | Loss: 0.0227 | Acc: 0.8939 | F1: 0.8928 | 11.6s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'Exception ignored in: \n<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers() \n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive():^\n^^ ^ ^ ^  ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n^ ^ ^  ^ ^ ^ ^ \n AssertionError :  can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    if w.is_alive():^\n^^  ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^\n^AssertionError: ^can only test a child process^\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>    \nassert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n\n     self._shutdown_workers() \n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n           ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ ^ ^^  ^ ^ ^ \nAssertionError : ^can only test a child process^\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  24/100 | Loss: 0.0213 | Acc: 0.8957 | F1: 0.8953 | 12.2s   \n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n          Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^    ^self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^\n^ ^ ^  ^^  ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n:   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n\n Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40> \n Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n       self._shutdown_workers()\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n       if w.is_alive():^\n^^  ^ ^^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^\n^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^ ^ \nAssertionError  :  can only test a child process \n   Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>^\n^Traceback (most recent call last):\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    ^self._shutdown_workers()^\n^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():^^\n^ ^ ^ ^ ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssertionError  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n:     can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    Ep  25/100 | Loss: 0.0216 | Acc: 0.8898 | F1: 0.8880 | 12.1s   \n    ‚èπÔ∏è  Early stopping at epoch 25. Best: 20\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7aaa59286d40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"    üèÜ Final (best) | Acc: 0.9011 | F1: 0.9010\n\n============================================================\nüìÅ svhn | IF=1\n  ‚Üí svhn (IF=1): train=73257, test=26032, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 1.4814 | Acc: 0.7157 | F1: 0.6838 | 14.5s ‚úÖ\n    Ep   2/100 | Loss: 0.5905 | Acc: 0.7800 | F1: 0.7566 | 14.4s ‚úÖ\n    Ep   3/100 | Loss: 0.4290 | Acc: 0.8430 | F1: 0.8261 | 14.4s ‚úÖ\n    Ep   4/100 | Loss: 0.3465 | Acc: 0.8537 | F1: 0.8384 | 14.4s ‚úÖ\n    Ep   5/100 | Loss: 0.2876 | Acc: 0.8632 | F1: 0.8497 | 14.5s ‚úÖ\n    Ep   6/100 | Loss: 0.2392 | Acc: 0.8630 | F1: 0.8478 | 14.4s   \n    Ep   7/100 | Loss: 0.1973 | Acc: 0.8733 | F1: 0.8617 | 14.5s ‚úÖ\n    Ep   8/100 | Loss: 0.1633 | Acc: 0.8627 | F1: 0.8488 | 14.4s   \n    Ep   9/100 | Loss: 0.1330 | Acc: 0.8752 | F1: 0.8638 | 14.4s ‚úÖ\n    Ep  10/100 | Loss: 0.1093 | Acc: 0.8726 | F1: 0.8619 | 14.4s   \n    Ep  11/100 | Loss: 0.0901 | Acc: 0.8763 | F1: 0.8633 | 14.4s ‚úÖ\n    Ep  12/100 | Loss: 0.0746 | Acc: 0.8747 | F1: 0.8641 | 14.4s   \n    Ep  13/100 | Loss: 0.0625 | Acc: 0.8677 | F1: 0.8558 | 14.3s   \n    Ep  14/100 | Loss: 0.0521 | Acc: 0.8873 | F1: 0.8767 | 14.4s ‚úÖ\n    Ep  15/100 | Loss: 0.0456 | Acc: 0.8836 | F1: 0.8702 | 14.3s   \n    Ep  16/100 | Loss: 0.0428 | Acc: 0.8816 | F1: 0.8692 | 14.4s   \n    Ep  17/100 | Loss: 0.0377 | Acc: 0.8701 | F1: 0.8565 | 14.3s   \n    Ep  18/100 | Loss: 0.0354 | Acc: 0.8814 | F1: 0.8680 | 14.3s   \n    Ep  19/100 | Loss: 0.0317 | Acc: 0.8830 | F1: 0.8721 | 14.3s   \n    ‚èπÔ∏è  Early stopping at epoch 19. Best: 14\n    üèÜ Final (best) | Acc: 0.8873 | F1: 0.8767\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 1.2789 | Acc: 0.6648 | F1: 0.6326 | 14.5s ‚úÖ\n    Ep   2/100 | Loss: 0.4706 | Acc: 0.7360 | F1: 0.7155 | 14.5s ‚úÖ\n    Ep   3/100 | Loss: 0.3389 | Acc: 0.8037 | F1: 0.7822 | 14.4s ‚úÖ\n    Ep   4/100 | Loss: 0.2660 | Acc: 0.8427 | F1: 0.8279 | 14.4s ‚úÖ\n    Ep   5/100 | Loss: 0.2168 | Acc: 0.8508 | F1: 0.8366 | 14.4s ‚úÖ\n    Ep   6/100 | Loss: 0.1790 | Acc: 0.8662 | F1: 0.8507 | 14.5s ‚úÖ\n    Ep   7/100 | Loss: 0.1427 | Acc: 0.8294 | F1: 0.8207 | 14.3s   \n    Ep   8/100 | Loss: 0.1131 | Acc: 0.8601 | F1: 0.8464 | 14.3s   \n    Ep   9/100 | Loss: 0.0882 | Acc: 0.8621 | F1: 0.8487 | 14.4s   \n    Ep  10/100 | Loss: 0.0699 | Acc: 0.8512 | F1: 0.8391 | 14.3s   \n    Ep  11/100 | Loss: 0.0569 | Acc: 0.8634 | F1: 0.8496 | 14.4s   \n    ‚èπÔ∏è  Early stopping at epoch 11. Best: 6\n    üèÜ Final (best) | Acc: 0.8662 | F1: 0.8507\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 1.3077 | Acc: 0.6787 | F1: 0.6472 | 14.7s ‚úÖ\n    Ep   2/100 | Loss: 0.4799 | Acc: 0.7813 | F1: 0.7580 | 15.0s ‚úÖ\n    Ep   3/100 | Loss: 0.3340 | Acc: 0.8128 | F1: 0.7963 | 14.9s ‚úÖ\n    Ep   4/100 | Loss: 0.2619 | Acc: 0.8264 | F1: 0.8144 | 14.9s ‚úÖ\n    Ep   5/100 | Loss: 0.2127 | Acc: 0.8561 | F1: 0.8413 | 14.8s ‚úÖ\n    Ep   6/100 | Loss: 0.1711 | Acc: 0.8628 | F1: 0.8472 | 14.9s ‚úÖ\n    Ep   7/100 | Loss: 0.1374 | Acc: 0.8484 | F1: 0.8343 | 14.8s   \n    Ep   8/100 | Loss: 0.1096 | Acc: 0.8679 | F1: 0.8540 | 14.9s ‚úÖ\n    Ep   9/100 | Loss: 0.0866 | Acc: 0.8680 | F1: 0.8537 | 14.9s ‚úÖ\n    Ep  10/100 | Loss: 0.0701 | Acc: 0.8732 | F1: 0.8607 | 14.8s ‚úÖ\n    Ep  11/100 | Loss: 0.0577 | Acc: 0.8708 | F1: 0.8589 | 14.7s   \n    Ep  12/100 | Loss: 0.0470 | Acc: 0.8752 | F1: 0.8641 | 14.9s ‚úÖ\n    Ep  13/100 | Loss: 0.0393 | Acc: 0.8791 | F1: 0.8670 | 14.9s ‚úÖ\n    Ep  14/100 | Loss: 0.0342 | Acc: 0.8775 | F1: 0.8644 | 14.8s   \n    Ep  15/100 | Loss: 0.0313 | Acc: 0.8811 | F1: 0.8690 | 14.8s ‚úÖ\n    Ep  16/100 | Loss: 0.0270 | Acc: 0.8834 | F1: 0.8718 | 14.8s ‚úÖ\n    Ep  17/100 | Loss: 0.0250 | Acc: 0.8821 | F1: 0.8702 | 14.7s   \n    Ep  18/100 | Loss: 0.0225 | Acc: 0.8793 | F1: 0.8672 | 14.8s   \n    Ep  19/100 | Loss: 0.0217 | Acc: 0.8633 | F1: 0.8540 | 14.6s   \n    Ep  20/100 | Loss: 0.0210 | Acc: 0.8813 | F1: 0.8694 | 14.7s   \n    Ep  21/100 | Loss: 0.0172 | Acc: 0.8816 | F1: 0.8697 | 14.8s   \n    ‚èπÔ∏è  Early stopping at epoch 21. Best: 16\n    üèÜ Final (best) | Acc: 0.8834 | F1: 0.8718\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 1.5383 | Acc: 0.6874 | F1: 0.6594 | 14.9s ‚úÖ\n    Ep   2/100 | Loss: 0.6012 | Acc: 0.7990 | F1: 0.7761 | 14.9s ‚úÖ\n    Ep   3/100 | Loss: 0.4066 | Acc: 0.8419 | F1: 0.8267 | 15.0s ‚úÖ\n    Ep   4/100 | Loss: 0.3044 | Acc: 0.8318 | F1: 0.8139 | 14.8s   \n    Ep   5/100 | Loss: 0.2347 | Acc: 0.8248 | F1: 0.8126 | 14.8s   \n    Ep   6/100 | Loss: 0.1867 | Acc: 0.8559 | F1: 0.8397 | 14.9s ‚úÖ\n    Ep   7/100 | Loss: 0.1517 | Acc: 0.8639 | F1: 0.8529 | 14.9s ‚úÖ\n    Ep   8/100 | Loss: 0.1206 | Acc: 0.8601 | F1: 0.8458 | 14.8s   \n    Ep   9/100 | Loss: 0.0983 | Acc: 0.8607 | F1: 0.8490 | 14.8s   \n    Ep  10/100 | Loss: 0.0784 | Acc: 0.8676 | F1: 0.8574 | 14.8s ‚úÖ\n    Ep  11/100 | Loss: 0.0628 | Acc: 0.8728 | F1: 0.8609 | 14.9s ‚úÖ\n    Ep  12/100 | Loss: 0.0513 | Acc: 0.8722 | F1: 0.8618 | 14.8s   \n    Ep  13/100 | Loss: 0.0431 | Acc: 0.8739 | F1: 0.8620 | 14.9s ‚úÖ\n    Ep  14/100 | Loss: 0.0372 | Acc: 0.8712 | F1: 0.8585 | 14.8s   \n    Ep  15/100 | Loss: 0.0330 | Acc: 0.8672 | F1: 0.8567 | 14.8s   \n    Ep  16/100 | Loss: 0.0306 | Acc: 0.8753 | F1: 0.8628 | 14.9s ‚úÖ\n    Ep  17/100 | Loss: 0.0258 | Acc: 0.8762 | F1: 0.8642 | 14.8s ‚úÖ\n    Ep  18/100 | Loss: 0.0267 | Acc: 0.8768 | F1: 0.8626 | 14.9s ‚úÖ\n    Ep  19/100 | Loss: 0.0235 | Acc: 0.8818 | F1: 0.8692 | 14.9s ‚úÖ\n    Ep  20/100 | Loss: 0.0227 | Acc: 0.8757 | F1: 0.8650 | 14.8s   \n    Ep  21/100 | Loss: 0.0216 | Acc: 0.8844 | F1: 0.8725 | 14.9s ‚úÖ\n    Ep  22/100 | Loss: 0.0191 | Acc: 0.8803 | F1: 0.8696 | 14.7s   \n    Ep  23/100 | Loss: 0.0190 | Acc: 0.8868 | F1: 0.8766 | 14.9s ‚úÖ\n    Ep  24/100 | Loss: 0.0168 | Acc: 0.8816 | F1: 0.8707 | 14.9s   \n    Ep  25/100 | Loss: 0.0172 | Acc: 0.8841 | F1: 0.8736 | 14.8s   \n    Ep  26/100 | Loss: 0.0151 | Acc: 0.8849 | F1: 0.8741 | 14.8s   \n    Ep  27/100 | Loss: 0.0133 | Acc: 0.8811 | F1: 0.8698 | 14.8s   \n    Ep  28/100 | Loss: 0.0131 | Acc: 0.8856 | F1: 0.8740 | 14.9s   \n    ‚èπÔ∏è  Early stopping at epoch 28. Best: 23\n    üèÜ Final (best) | Acc: 0.8868 | F1: 0.8766\n\n============================================================\nüìÅ cifar10 | IF=1\n  ‚Üí cifar10 (IF=1): train=50000, test=10000, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 1.9200 | Acc: 0.4017 | F1: 0.3957 | 12.3s ‚úÖ\n    Ep   2/100 | Loss: 1.5847 | Acc: 0.4650 | F1: 0.4581 | 12.8s ‚úÖ\n    Ep   3/100 | Loss: 1.4536 | Acc: 0.4990 | F1: 0.4947 | 12.5s ‚úÖ\n    Ep   4/100 | Loss: 1.3528 | Acc: 0.5442 | F1: 0.5425 | 12.5s ‚úÖ\n    Ep   5/100 | Loss: 1.2707 | Acc: 0.5822 | F1: 0.5818 | 12.5s ‚úÖ\n    Ep   6/100 | Loss: 1.2046 | Acc: 0.5958 | F1: 0.5964 | 12.6s ‚úÖ\n    Ep   7/100 | Loss: 1.1395 | Acc: 0.6104 | F1: 0.6074 | 12.7s ‚úÖ\n    Ep   8/100 | Loss: 1.0921 | Acc: 0.6261 | F1: 0.6210 | 12.6s ‚úÖ\n    Ep   9/100 | Loss: 1.0492 | Acc: 0.6401 | F1: 0.6366 | 12.7s ‚úÖ\n    Ep  10/100 | Loss: 1.0117 | Acc: 0.6269 | F1: 0.6255 | 12.5s   \n    Ep  11/100 | Loss: 0.9816 | Acc: 0.6617 | F1: 0.6593 | 12.6s ‚úÖ\n    Ep  12/100 | Loss: 0.9456 | Acc: 0.6698 | F1: 0.6669 | 12.4s ‚úÖ\n    Ep  13/100 | Loss: 0.9229 | Acc: 0.6756 | F1: 0.6748 | 12.5s ‚úÖ\n    Ep  14/100 | Loss: 0.8906 | Acc: 0.6918 | F1: 0.6881 | 12.5s ‚úÖ\n    Ep  15/100 | Loss: 0.8669 | Acc: 0.6938 | F1: 0.6911 | 12.7s ‚úÖ\n    Ep  16/100 | Loss: 0.8416 | Acc: 0.7012 | F1: 0.6974 | 12.8s ‚úÖ\n    Ep  17/100 | Loss: 0.8232 | Acc: 0.6899 | F1: 0.6844 | 12.8s   \n    Ep  18/100 | Loss: 0.8123 | Acc: 0.7026 | F1: 0.7000 | 12.8s ‚úÖ\n    Ep  19/100 | Loss: 0.7857 | Acc: 0.7091 | F1: 0.7074 | 12.5s ‚úÖ\n    Ep  20/100 | Loss: 0.7659 | Acc: 0.7151 | F1: 0.7124 | 12.8s ‚úÖ\n    Ep  21/100 | Loss: 0.7536 | Acc: 0.7280 | F1: 0.7283 | 12.5s ‚úÖ\n    Ep  22/100 | Loss: 0.7405 | Acc: 0.7318 | F1: 0.7302 | 12.6s ‚úÖ\n    Ep  23/100 | Loss: 0.7235 | Acc: 0.7263 | F1: 0.7249 | 12.5s   \n    Ep  24/100 | Loss: 0.7068 | Acc: 0.7301 | F1: 0.7302 | 12.4s   \n    Ep  25/100 | Loss: 0.6881 | Acc: 0.7278 | F1: 0.7290 | 12.5s   \n    Ep  26/100 | Loss: 0.6797 | Acc: 0.7339 | F1: 0.7342 | 12.5s ‚úÖ\n    Ep  27/100 | Loss: 0.6675 | Acc: 0.7457 | F1: 0.7444 | 12.5s ‚úÖ\n    Ep  28/100 | Loss: 0.6539 | Acc: 0.7509 | F1: 0.7501 | 12.6s ‚úÖ\n    Ep  29/100 | Loss: 0.6413 | Acc: 0.7515 | F1: 0.7505 | 12.6s ‚úÖ\n    Ep  30/100 | Loss: 0.6333 | Acc: 0.7381 | F1: 0.7360 | 12.7s   \n    Ep  31/100 | Loss: 0.6186 | Acc: 0.7576 | F1: 0.7575 | 12.5s ‚úÖ\n    Ep  32/100 | Loss: 0.6037 | Acc: 0.7497 | F1: 0.7512 | 12.6s   \n    Ep  33/100 | Loss: 0.5915 | Acc: 0.7542 | F1: 0.7520 | 12.3s   \n    Ep  34/100 | Loss: 0.5863 | Acc: 0.7588 | F1: 0.7589 | 12.7s ‚úÖ\n    Ep  35/100 | Loss: 0.5770 | Acc: 0.7562 | F1: 0.7552 | 12.5s   \n    Ep  36/100 | Loss: 0.5616 | Acc: 0.7517 | F1: 0.7512 | 12.6s   \n    Ep  37/100 | Loss: 0.5577 | Acc: 0.7527 | F1: 0.7539 | 12.6s   \n    Ep  38/100 | Loss: 0.5470 | Acc: 0.7581 | F1: 0.7565 | 12.7s   \n    Ep  39/100 | Loss: 0.5368 | Acc: 0.7661 | F1: 0.7657 | 12.6s ‚úÖ\n    Ep  40/100 | Loss: 0.5286 | Acc: 0.7704 | F1: 0.7706 | 13.0s ‚úÖ\n    Ep  41/100 | Loss: 0.5181 | Acc: 0.7665 | F1: 0.7658 | 12.8s   \n    Ep  42/100 | Loss: 0.5088 | Acc: 0.7724 | F1: 0.7715 | 12.6s ‚úÖ\n    Ep  43/100 | Loss: 0.4957 | Acc: 0.7684 | F1: 0.7664 | 12.6s   \n    Ep  44/100 | Loss: 0.4880 | Acc: 0.7711 | F1: 0.7710 | 12.6s   \n    Ep  45/100 | Loss: 0.4787 | Acc: 0.7699 | F1: 0.7692 | 12.5s   \n    Ep  46/100 | Loss: 0.4707 | Acc: 0.7710 | F1: 0.7707 | 12.6s   \n    Ep  47/100 | Loss: 0.4650 | Acc: 0.7713 | F1: 0.7697 | 12.4s   \n    ‚èπÔ∏è  Early stopping at epoch 47. Best: 42\n    üèÜ Final (best) | Acc: 0.7724 | F1: 0.7715\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 1.6582 | Acc: 0.3846 | F1: 0.3829 | 12.7s ‚úÖ\n    Ep   2/100 | Loss: 1.3131 | Acc: 0.4572 | F1: 0.4521 | 12.6s ‚úÖ\n    Ep   3/100 | Loss: 1.1890 | Acc: 0.4914 | F1: 0.4854 | 12.5s ‚úÖ\n    Ep   4/100 | Loss: 1.0945 | Acc: 0.5328 | F1: 0.5318 | 12.5s ‚úÖ\n    Ep   5/100 | Loss: 1.0112 | Acc: 0.5540 | F1: 0.5536 | 12.7s ‚úÖ\n    Ep   6/100 | Loss: 0.9543 | Acc: 0.5686 | F1: 0.5679 | 12.6s ‚úÖ\n    Ep   7/100 | Loss: 0.8923 | Acc: 0.5991 | F1: 0.5949 | 12.7s ‚úÖ\n    Ep   8/100 | Loss: 0.8517 | Acc: 0.6118 | F1: 0.6093 | 12.8s ‚úÖ\n    Ep   9/100 | Loss: 0.8087 | Acc: 0.6363 | F1: 0.6348 | 12.6s ‚úÖ\n    Ep  10/100 | Loss: 0.7752 | Acc: 0.6449 | F1: 0.6440 | 12.5s ‚úÖ\n    Ep  11/100 | Loss: 0.7423 | Acc: 0.6566 | F1: 0.6569 | 12.6s ‚úÖ\n    Ep  12/100 | Loss: 0.7153 | Acc: 0.6624 | F1: 0.6580 | 12.9s ‚úÖ\n    Ep  13/100 | Loss: 0.6903 | Acc: 0.6584 | F1: 0.6580 | 12.6s   \n    Ep  14/100 | Loss: 0.6703 | Acc: 0.6735 | F1: 0.6704 | 12.6s ‚úÖ\n    Ep  15/100 | Loss: 0.6438 | Acc: 0.6877 | F1: 0.6823 | 13.0s ‚úÖ\n    Ep  16/100 | Loss: 0.6244 | Acc: 0.6973 | F1: 0.6948 | 12.6s ‚úÖ\n    Ep  17/100 | Loss: 0.6101 | Acc: 0.6941 | F1: 0.6848 | 12.7s   \n    Ep  18/100 | Loss: 0.5973 | Acc: 0.6992 | F1: 0.6921 | 12.5s ‚úÖ\n    Ep  19/100 | Loss: 0.5849 | Acc: 0.7005 | F1: 0.6962 | 12.8s ‚úÖ\n    Ep  20/100 | Loss: 0.5668 | Acc: 0.7030 | F1: 0.6995 | 12.7s ‚úÖ\n    Ep  21/100 | Loss: 0.5500 | Acc: 0.7217 | F1: 0.7210 | 12.6s ‚úÖ\n    Ep  22/100 | Loss: 0.5372 | Acc: 0.7163 | F1: 0.7133 | 12.6s   \n    Ep  23/100 | Loss: 0.5281 | Acc: 0.7227 | F1: 0.7232 | 12.9s ‚úÖ\n    Ep  24/100 | Loss: 0.5135 | Acc: 0.7268 | F1: 0.7282 | 12.8s ‚úÖ\n    Ep  25/100 | Loss: 0.5032 | Acc: 0.7333 | F1: 0.7327 | 12.7s ‚úÖ\n    Ep  26/100 | Loss: 0.4919 | Acc: 0.7382 | F1: 0.7369 | 12.9s ‚úÖ\n    Ep  27/100 | Loss: 0.4782 | Acc: 0.7345 | F1: 0.7300 | 12.8s   \n    Ep  28/100 | Loss: 0.4677 | Acc: 0.7374 | F1: 0.7359 | 12.6s   \n    Ep  29/100 | Loss: 0.4597 | Acc: 0.7405 | F1: 0.7411 | 12.6s ‚úÖ\n    Ep  30/100 | Loss: 0.4483 | Acc: 0.7461 | F1: 0.7447 | 12.9s ‚úÖ\n    Ep  31/100 | Loss: 0.4406 | Acc: 0.7347 | F1: 0.7328 | 12.5s   \n    Ep  32/100 | Loss: 0.4296 | Acc: 0.7391 | F1: 0.7395 | 12.9s   \n    Ep  33/100 | Loss: 0.4238 | Acc: 0.7443 | F1: 0.7396 | 12.8s   \n    Ep  34/100 | Loss: 0.4160 | Acc: 0.7501 | F1: 0.7506 | 12.6s ‚úÖ\n    Ep  35/100 | Loss: 0.4078 | Acc: 0.7470 | F1: 0.7454 | 12.6s   \n    Ep  36/100 | Loss: 0.3987 | Acc: 0.7458 | F1: 0.7445 | 12.6s   \n    Ep  37/100 | Loss: 0.3886 | Acc: 0.7478 | F1: 0.7480 | 12.2s   \n    Ep  38/100 | Loss: 0.3859 | Acc: 0.7593 | F1: 0.7576 | 13.0s ‚úÖ\n    Ep  39/100 | Loss: 0.3736 | Acc: 0.7581 | F1: 0.7589 | 12.7s   \n    Ep  40/100 | Loss: 0.3637 | Acc: 0.7630 | F1: 0.7631 | 13.2s ‚úÖ\n    Ep  41/100 | Loss: 0.3583 | Acc: 0.7580 | F1: 0.7584 | 13.0s   \n    Ep  42/100 | Loss: 0.3531 | Acc: 0.7602 | F1: 0.7602 | 12.7s   \n    Ep  43/100 | Loss: 0.3407 | Acc: 0.7680 | F1: 0.7661 | 12.8s ‚úÖ\n    Ep  44/100 | Loss: 0.3349 | Acc: 0.7653 | F1: 0.7643 | 13.0s   \n    Ep  45/100 | Loss: 0.3317 | Acc: 0.7670 | F1: 0.7653 | 13.0s   \n    Ep  46/100 | Loss: 0.3240 | Acc: 0.7684 | F1: 0.7682 | 13.0s ‚úÖ\n    Ep  47/100 | Loss: 0.3161 | Acc: 0.7654 | F1: 0.7642 | 12.6s   \n    Ep  48/100 | Loss: 0.3141 | Acc: 0.7600 | F1: 0.7593 | 12.7s   \n    Ep  49/100 | Loss: 0.3019 | Acc: 0.7651 | F1: 0.7654 | 12.5s   \n    Ep  50/100 | Loss: 0.2976 | Acc: 0.7647 | F1: 0.7642 | 12.4s   \n    Ep  51/100 | Loss: 0.2924 | Acc: 0.7765 | F1: 0.7759 | 12.6s ‚úÖ\n    Ep  52/100 | Loss: 0.2883 | Acc: 0.7653 | F1: 0.7658 | 12.5s   \n    Ep  53/100 | Loss: 0.2793 | Acc: 0.7699 | F1: 0.7693 | 12.7s   \n    Ep  54/100 | Loss: 0.2737 | Acc: 0.7680 | F1: 0.7679 | 12.7s   \n    Ep  55/100 | Loss: 0.2705 | Acc: 0.7736 | F1: 0.7732 | 12.7s   \n    Ep  56/100 | Loss: 0.2636 | Acc: 0.7765 | F1: 0.7751 | 13.0s   \n    ‚èπÔ∏è  Early stopping at epoch 56. Best: 51\n    üèÜ Final (best) | Acc: 0.7765 | F1: 0.7759\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 1.6626 | Acc: 0.3939 | F1: 0.3916 | 12.8s ‚úÖ\n    Ep   2/100 | Loss: 1.3085 | Acc: 0.4654 | F1: 0.4576 | 12.9s ‚úÖ\n    Ep   3/100 | Loss: 1.1825 | Acc: 0.4928 | F1: 0.4877 | 12.4s ‚úÖ\n    Ep   4/100 | Loss: 1.0896 | Acc: 0.5334 | F1: 0.5310 | 12.2s ‚úÖ\n    Ep   5/100 | Loss: 1.0189 | Acc: 0.5561 | F1: 0.5558 | 13.0s ‚úÖ\n    Ep   6/100 | Loss: 0.9567 | Acc: 0.5783 | F1: 0.5767 | 13.0s ‚úÖ\n    Ep   7/100 | Loss: 0.9016 | Acc: 0.5949 | F1: 0.5916 | 12.5s ‚úÖ\n    Ep   8/100 | Loss: 0.8601 | Acc: 0.6183 | F1: 0.6163 | 12.4s ‚úÖ\n    Ep   9/100 | Loss: 0.8176 | Acc: 0.6217 | F1: 0.6183 | 12.5s ‚úÖ\n    Ep  10/100 | Loss: 0.7849 | Acc: 0.6370 | F1: 0.6339 | 12.6s ‚úÖ\n    Ep  11/100 | Loss: 0.7559 | Acc: 0.6476 | F1: 0.6476 | 12.5s ‚úÖ\n    Ep  12/100 | Loss: 0.7293 | Acc: 0.6552 | F1: 0.6481 | 12.5s ‚úÖ\n    Ep  13/100 | Loss: 0.7070 | Acc: 0.6519 | F1: 0.6505 | 12.1s   \n    Ep  14/100 | Loss: 0.6817 | Acc: 0.6733 | F1: 0.6705 | 12.2s ‚úÖ\n    Ep  15/100 | Loss: 0.6580 | Acc: 0.6810 | F1: 0.6759 | 12.3s ‚úÖ\n    Ep  16/100 | Loss: 0.6391 | Acc: 0.6834 | F1: 0.6808 | 12.4s ‚úÖ\n    Ep  17/100 | Loss: 0.6232 | Acc: 0.6856 | F1: 0.6825 | 12.3s ‚úÖ\n    Ep  18/100 | Loss: 0.6095 | Acc: 0.6882 | F1: 0.6827 | 12.4s ‚úÖ\n    Ep  19/100 | Loss: 0.5907 | Acc: 0.6873 | F1: 0.6851 | 12.4s   \n    Ep  20/100 | Loss: 0.5758 | Acc: 0.7109 | F1: 0.7069 | 12.4s ‚úÖ\n    Ep  21/100 | Loss: 0.5590 | Acc: 0.7196 | F1: 0.7194 | 12.5s ‚úÖ\n    Ep  22/100 | Loss: 0.5499 | Acc: 0.7204 | F1: 0.7187 | 12.3s ‚úÖ\n    Ep  23/100 | Loss: 0.5361 | Acc: 0.7176 | F1: 0.7177 | 12.1s   \n    Ep  24/100 | Loss: 0.5227 | Acc: 0.7265 | F1: 0.7264 | 12.3s ‚úÖ\n    Ep  25/100 | Loss: 0.5112 | Acc: 0.7213 | F1: 0.7215 | 12.0s   \n    Ep  26/100 | Loss: 0.4990 | Acc: 0.7297 | F1: 0.7297 | 12.1s ‚úÖ\n    Ep  27/100 | Loss: 0.4832 | Acc: 0.7336 | F1: 0.7313 | 12.4s ‚úÖ\n    Ep  28/100 | Loss: 0.4798 | Acc: 0.7395 | F1: 0.7372 | 12.2s ‚úÖ\n    Ep  29/100 | Loss: 0.4691 | Acc: 0.7349 | F1: 0.7338 | 12.1s   \n    Ep  30/100 | Loss: 0.4566 | Acc: 0.7376 | F1: 0.7360 | 12.1s   \n    Ep  31/100 | Loss: 0.4490 | Acc: 0.7412 | F1: 0.7405 | 12.4s ‚úÖ\n    Ep  32/100 | Loss: 0.4387 | Acc: 0.7401 | F1: 0.7399 | 12.1s   \n    Ep  33/100 | Loss: 0.4285 | Acc: 0.7464 | F1: 0.7444 | 12.3s ‚úÖ\n    Ep  34/100 | Loss: 0.4221 | Acc: 0.7468 | F1: 0.7452 | 12.6s ‚úÖ\n    Ep  35/100 | Loss: 0.4120 | Acc: 0.7453 | F1: 0.7436 | 12.4s   \n    Ep  36/100 | Loss: 0.4040 | Acc: 0.7480 | F1: 0.7454 | 12.6s ‚úÖ\n    Ep  37/100 | Loss: 0.3932 | Acc: 0.7413 | F1: 0.7415 | 12.5s   \n    Ep  38/100 | Loss: 0.3890 | Acc: 0.7580 | F1: 0.7571 | 12.3s ‚úÖ\n    Ep  39/100 | Loss: 0.3801 | Acc: 0.7565 | F1: 0.7559 | 12.4s   \n    Ep  40/100 | Loss: 0.3743 | Acc: 0.7505 | F1: 0.7496 | 12.6s   \n    Ep  41/100 | Loss: 0.3644 | Acc: 0.7521 | F1: 0.7500 | 12.6s   \n    Ep  42/100 | Loss: 0.3578 | Acc: 0.7614 | F1: 0.7602 | 12.6s ‚úÖ\n    Ep  43/100 | Loss: 0.3452 | Acc: 0.7630 | F1: 0.7608 | 12.5s ‚úÖ\n    Ep  44/100 | Loss: 0.3412 | Acc: 0.7659 | F1: 0.7657 | 12.6s ‚úÖ\n    Ep  45/100 | Loss: 0.3335 | Acc: 0.7629 | F1: 0.7618 | 12.4s   \n    Ep  46/100 | Loss: 0.3261 | Acc: 0.7645 | F1: 0.7646 | 12.4s   \n    Ep  47/100 | Loss: 0.3183 | Acc: 0.7630 | F1: 0.7609 | 12.5s   \n    Ep  48/100 | Loss: 0.3121 | Acc: 0.7659 | F1: 0.7651 | 12.6s   \n    Ep  49/100 | Loss: 0.3079 | Acc: 0.7645 | F1: 0.7649 | 12.4s   \n    ‚èπÔ∏è  Early stopping at epoch 49. Best: 44\n    üèÜ Final (best) | Acc: 0.7659 | F1: 0.7657\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 1.9180 | Acc: 0.4019 | F1: 0.3990 | 12.6s ‚úÖ\n    Ep   2/100 | Loss: 1.5575 | Acc: 0.4701 | F1: 0.4663 | 12.7s ‚úÖ\n    Ep   3/100 | Loss: 1.3700 | Acc: 0.4968 | F1: 0.4919 | 12.7s ‚úÖ\n    Ep   4/100 | Loss: 1.2146 | Acc: 0.5289 | F1: 0.5231 | 12.6s ‚úÖ\n    Ep   5/100 | Loss: 1.0890 | Acc: 0.5686 | F1: 0.5690 | 12.6s ‚úÖ\n    Ep   6/100 | Loss: 1.0032 | Acc: 0.5884 | F1: 0.5838 | 12.6s ‚úÖ\n    Ep   7/100 | Loss: 0.9389 | Acc: 0.6022 | F1: 0.5951 | 12.7s ‚úÖ\n    Ep   8/100 | Loss: 0.8973 | Acc: 0.6250 | F1: 0.6219 | 12.4s ‚úÖ\n    Ep   9/100 | Loss: 0.8605 | Acc: 0.6297 | F1: 0.6278 | 12.5s ‚úÖ\n    Ep  10/100 | Loss: 0.8215 | Acc: 0.6360 | F1: 0.6331 | 12.4s ‚úÖ\n    Ep  11/100 | Loss: 0.7940 | Acc: 0.6520 | F1: 0.6506 | 12.6s ‚úÖ\n    Ep  12/100 | Loss: 0.7635 | Acc: 0.6524 | F1: 0.6494 | 12.7s ‚úÖ\n    Ep  13/100 | Loss: 0.7368 | Acc: 0.6642 | F1: 0.6618 | 12.6s ‚úÖ\n    Ep  14/100 | Loss: 0.7152 | Acc: 0.6733 | F1: 0.6714 | 12.6s ‚úÖ\n    Ep  15/100 | Loss: 0.6952 | Acc: 0.6847 | F1: 0.6806 | 12.6s ‚úÖ\n    Ep  16/100 | Loss: 0.6741 | Acc: 0.6839 | F1: 0.6796 | 12.6s   \n    Ep  17/100 | Loss: 0.6527 | Acc: 0.6968 | F1: 0.6934 | 12.8s ‚úÖ\n    Ep  18/100 | Loss: 0.6428 | Acc: 0.6964 | F1: 0.6903 | 12.5s   \n    Ep  19/100 | Loss: 0.6226 | Acc: 0.6968 | F1: 0.6939 | 12.3s   \n    Ep  20/100 | Loss: 0.6072 | Acc: 0.7009 | F1: 0.6966 | 12.5s ‚úÖ\n    Ep  21/100 | Loss: 0.5956 | Acc: 0.7180 | F1: 0.7193 | 12.5s ‚úÖ\n    Ep  22/100 | Loss: 0.5807 | Acc: 0.7210 | F1: 0.7174 | 12.6s ‚úÖ\n    Ep  23/100 | Loss: 0.5689 | Acc: 0.7160 | F1: 0.7154 | 12.5s   \n    Ep  24/100 | Loss: 0.5497 | Acc: 0.7171 | F1: 0.7173 | 12.5s   \n    Ep  25/100 | Loss: 0.5435 | Acc: 0.7258 | F1: 0.7240 | 12.6s ‚úÖ\n    Ep  26/100 | Loss: 0.5275 | Acc: 0.7358 | F1: 0.7354 | 12.6s ‚úÖ\n    Ep  27/100 | Loss: 0.5176 | Acc: 0.7385 | F1: 0.7372 | 12.5s ‚úÖ\n    Ep  28/100 | Loss: 0.5087 | Acc: 0.7350 | F1: 0.7356 | 12.7s   \n    Ep  29/100 | Loss: 0.4959 | Acc: 0.7406 | F1: 0.7410 | 12.7s ‚úÖ\n    Ep  30/100 | Loss: 0.4830 | Acc: 0.7436 | F1: 0.7438 | 12.8s ‚úÖ\n    Ep  31/100 | Loss: 0.4795 | Acc: 0.7448 | F1: 0.7436 | 12.9s ‚úÖ\n    Ep  32/100 | Loss: 0.4671 | Acc: 0.7388 | F1: 0.7387 | 12.7s   \n    Ep  33/100 | Loss: 0.4549 | Acc: 0.7495 | F1: 0.7476 | 12.9s ‚úÖ\n    Ep  34/100 | Loss: 0.4511 | Acc: 0.7498 | F1: 0.7518 | 12.8s ‚úÖ\n    Ep  35/100 | Loss: 0.4358 | Acc: 0.7518 | F1: 0.7512 | 12.8s ‚úÖ\n    Ep  36/100 | Loss: 0.4304 | Acc: 0.7510 | F1: 0.7494 | 12.6s   \n    Ep  37/100 | Loss: 0.4226 | Acc: 0.7535 | F1: 0.7538 | 12.7s ‚úÖ\n    Ep  38/100 | Loss: 0.4138 | Acc: 0.7618 | F1: 0.7606 | 12.8s ‚úÖ\n    Ep  39/100 | Loss: 0.4081 | Acc: 0.7563 | F1: 0.7566 | 12.5s   \n    Ep  40/100 | Loss: 0.3997 | Acc: 0.7574 | F1: 0.7568 | 12.5s   \n    Ep  41/100 | Loss: 0.3887 | Acc: 0.7575 | F1: 0.7564 | 12.8s   \n    Ep  42/100 | Loss: 0.3820 | Acc: 0.7628 | F1: 0.7623 | 12.6s ‚úÖ\n    Ep  43/100 | Loss: 0.3681 | Acc: 0.7632 | F1: 0.7613 | 12.7s ‚úÖ\n    Ep  44/100 | Loss: 0.3645 | Acc: 0.7656 | F1: 0.7653 | 12.7s ‚úÖ\n    Ep  45/100 | Loss: 0.3614 | Acc: 0.7625 | F1: 0.7616 | 12.7s   \n    Ep  46/100 | Loss: 0.3508 | Acc: 0.7689 | F1: 0.7682 | 12.5s ‚úÖ\n    Ep  47/100 | Loss: 0.3432 | Acc: 0.7690 | F1: 0.7669 | 12.5s ‚úÖ\n    Ep  48/100 | Loss: 0.3384 | Acc: 0.7615 | F1: 0.7609 | 12.4s   \n    Ep  49/100 | Loss: 0.3339 | Acc: 0.7654 | F1: 0.7659 | 12.6s   \n    Ep  50/100 | Loss: 0.3268 | Acc: 0.7661 | F1: 0.7644 | 12.5s   \n    Ep  51/100 | Loss: 0.3166 | Acc: 0.7660 | F1: 0.7648 | 12.6s   \n    Ep  52/100 | Loss: 0.3125 | Acc: 0.7665 | F1: 0.7671 | 12.4s   \n    ‚èπÔ∏è  Early stopping at epoch 52. Best: 47\n    üèÜ Final (best) | Acc: 0.7690 | F1: 0.7669\n\n============================================================\nüìÅ cifar10 | IF=100\n  ‚Üí cifar10 (IF=100): train=12408, test=10000, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 1.6282 | Acc: 0.1933 | F1: 0.1088 | 4.4s ‚úÖ\n    Ep   2/100 | Loss: 1.2581 | Acc: 0.2196 | F1: 0.1281 | 4.5s ‚úÖ\n    Ep   3/100 | Loss: 1.1544 | Acc: 0.2481 | F1: 0.1475 | 4.5s ‚úÖ\n    Ep   4/100 | Loss: 1.0892 | Acc: 0.2640 | F1: 0.1675 | 4.5s ‚úÖ\n    Ep   5/100 | Loss: 1.0496 | Acc: 0.2653 | F1: 0.1678 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 1.0000 | Acc: 0.2755 | F1: 0.1894 | 4.5s ‚úÖ\n    Ep   7/100 | Loss: 0.9788 | Acc: 0.2880 | F1: 0.1947 | 4.5s ‚úÖ\n    Ep   8/100 | Loss: 0.9453 | Acc: 0.2974 | F1: 0.2084 | 4.6s ‚úÖ\n    Ep   9/100 | Loss: 0.9244 | Acc: 0.3139 | F1: 0.2310 | 4.5s ‚úÖ\n    Ep  10/100 | Loss: 0.9005 | Acc: 0.2961 | F1: 0.2088 | 4.3s   \n    Ep  11/100 | Loss: 0.8805 | Acc: 0.3074 | F1: 0.2171 | 4.4s   \n    Ep  12/100 | Loss: 0.8693 | Acc: 0.3142 | F1: 0.2443 | 4.4s ‚úÖ\n    Ep  13/100 | Loss: 0.8523 | Acc: 0.3182 | F1: 0.2335 | 4.5s ‚úÖ\n    Ep  14/100 | Loss: 0.8402 | Acc: 0.3302 | F1: 0.2464 | 4.5s ‚úÖ\n    Ep  15/100 | Loss: 0.8224 | Acc: 0.3363 | F1: 0.2670 | 4.6s ‚úÖ\n    Ep  16/100 | Loss: 0.8061 | Acc: 0.3340 | F1: 0.2521 | 4.4s   \n    Ep  17/100 | Loss: 0.7863 | Acc: 0.3522 | F1: 0.2775 | 4.5s ‚úÖ\n    Ep  18/100 | Loss: 0.7758 | Acc: 0.3476 | F1: 0.2708 | 4.4s   \n    Ep  19/100 | Loss: 0.7607 | Acc: 0.3603 | F1: 0.2815 | 4.4s ‚úÖ\n    Ep  20/100 | Loss: 0.7482 | Acc: 0.3737 | F1: 0.3017 | 4.5s ‚úÖ\n    Ep  21/100 | Loss: 0.7376 | Acc: 0.3705 | F1: 0.3006 | 4.3s   \n    Ep  22/100 | Loss: 0.7234 | Acc: 0.3672 | F1: 0.2996 | 4.5s   \n    Ep  23/100 | Loss: 0.7007 | Acc: 0.3677 | F1: 0.3002 | 4.3s   \n    Ep  24/100 | Loss: 0.6973 | Acc: 0.3740 | F1: 0.3012 | 4.5s ‚úÖ\n    Ep  25/100 | Loss: 0.6854 | Acc: 0.3925 | F1: 0.3328 | 4.4s ‚úÖ\n    Ep  26/100 | Loss: 0.6750 | Acc: 0.3849 | F1: 0.3221 | 4.3s   \n    Ep  27/100 | Loss: 0.6611 | Acc: 0.4002 | F1: 0.3384 | 4.5s ‚úÖ\n    Ep  28/100 | Loss: 0.6506 | Acc: 0.4056 | F1: 0.3385 | 4.5s ‚úÖ\n    Ep  29/100 | Loss: 0.6528 | Acc: 0.4055 | F1: 0.3396 | 4.5s   \n    Ep  30/100 | Loss: 0.6362 | Acc: 0.3930 | F1: 0.3337 | 4.2s   \n    Ep  31/100 | Loss: 0.6292 | Acc: 0.3951 | F1: 0.3348 | 4.4s   \n    Ep  32/100 | Loss: 0.6149 | Acc: 0.4110 | F1: 0.3475 | 4.4s ‚úÖ\n    Ep  33/100 | Loss: 0.6139 | Acc: 0.4230 | F1: 0.3646 | 4.4s ‚úÖ\n    Ep  34/100 | Loss: 0.5983 | Acc: 0.4198 | F1: 0.3646 | 4.4s   \n    Ep  35/100 | Loss: 0.5784 | Acc: 0.4152 | F1: 0.3583 | 4.4s   \n    Ep  36/100 | Loss: 0.5780 | Acc: 0.4220 | F1: 0.3661 | 4.4s   \n    Ep  37/100 | Loss: 0.5812 | Acc: 0.4310 | F1: 0.3759 | 4.4s ‚úÖ\n    Ep  38/100 | Loss: 0.5632 | Acc: 0.4275 | F1: 0.3716 | 4.3s   \n    Ep  39/100 | Loss: 0.5543 | Acc: 0.4278 | F1: 0.3696 | 4.3s   \n    Ep  40/100 | Loss: 0.5493 | Acc: 0.4107 | F1: 0.3556 | 4.3s   \n    Ep  41/100 | Loss: 0.5406 | Acc: 0.4420 | F1: 0.3897 | 4.4s ‚úÖ\n    Ep  42/100 | Loss: 0.5235 | Acc: 0.4418 | F1: 0.3882 | 4.4s   \n    Ep  43/100 | Loss: 0.5204 | Acc: 0.4378 | F1: 0.3874 | 4.4s   \n    Ep  44/100 | Loss: 0.5071 | Acc: 0.4320 | F1: 0.3772 | 4.3s   \n    Ep  45/100 | Loss: 0.5156 | Acc: 0.4259 | F1: 0.3690 | 4.5s   \n    Ep  46/100 | Loss: 0.4902 | Acc: 0.4323 | F1: 0.3780 | 4.4s   \n    ‚èπÔ∏è  Early stopping at epoch 46. Best: 41\n    üèÜ Final (best) | Acc: 0.4420 | F1: 0.3897\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 1.3678 | Acc: 0.1897 | F1: 0.1030 | 4.5s ‚úÖ\n    Ep   2/100 | Loss: 1.0160 | Acc: 0.2207 | F1: 0.1351 | 4.4s ‚úÖ\n    Ep   3/100 | Loss: 0.9212 | Acc: 0.2446 | F1: 0.1530 | 4.4s ‚úÖ\n    Ep   4/100 | Loss: 0.8682 | Acc: 0.2547 | F1: 0.1625 | 4.8s ‚úÖ\n    Ep   5/100 | Loss: 0.8317 | Acc: 0.2738 | F1: 0.1905 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 0.7893 | Acc: 0.2787 | F1: 0.1901 | 4.5s ‚úÖ\n    Ep   7/100 | Loss: 0.7698 | Acc: 0.2919 | F1: 0.2055 | 4.4s ‚úÖ\n    Ep   8/100 | Loss: 0.7373 | Acc: 0.2974 | F1: 0.2130 | 4.4s ‚úÖ\n    Ep   9/100 | Loss: 0.7227 | Acc: 0.3080 | F1: 0.2331 | 4.5s ‚úÖ\n    Ep  10/100 | Loss: 0.6997 | Acc: 0.2854 | F1: 0.2029 | 4.4s   \n    Ep  11/100 | Loss: 0.6859 | Acc: 0.3062 | F1: 0.2225 | 4.3s   \n    Ep  12/100 | Loss: 0.6679 | Acc: 0.3203 | F1: 0.2526 | 4.5s ‚úÖ\n    Ep  13/100 | Loss: 0.6500 | Acc: 0.3258 | F1: 0.2478 | 4.4s ‚úÖ\n    Ep  14/100 | Loss: 0.6415 | Acc: 0.3288 | F1: 0.2527 | 4.5s ‚úÖ\n    Ep  15/100 | Loss: 0.6283 | Acc: 0.3353 | F1: 0.2643 | 4.4s ‚úÖ\n    Ep  16/100 | Loss: 0.6148 | Acc: 0.3314 | F1: 0.2507 | 4.2s   \n    Ep  17/100 | Loss: 0.5960 | Acc: 0.3491 | F1: 0.2762 | 4.6s ‚úÖ\n    Ep  18/100 | Loss: 0.5888 | Acc: 0.3625 | F1: 0.2898 | 4.4s ‚úÖ\n    Ep  19/100 | Loss: 0.5767 | Acc: 0.3379 | F1: 0.2594 | 4.4s   \n    Ep  20/100 | Loss: 0.5638 | Acc: 0.3710 | F1: 0.3047 | 4.4s ‚úÖ\n    Ep  21/100 | Loss: 0.5579 | Acc: 0.3743 | F1: 0.3077 | 4.4s ‚úÖ\n    Ep  22/100 | Loss: 0.5440 | Acc: 0.3757 | F1: 0.3116 | 4.5s ‚úÖ\n    Ep  23/100 | Loss: 0.5329 | Acc: 0.3638 | F1: 0.2957 | 4.3s   \n    Ep  24/100 | Loss: 0.5224 | Acc: 0.3969 | F1: 0.3354 | 4.3s ‚úÖ\n    Ep  25/100 | Loss: 0.5125 | Acc: 0.3958 | F1: 0.3386 | 4.2s   \n    Ep  26/100 | Loss: 0.5039 | Acc: 0.3949 | F1: 0.3387 | 4.4s   \n    Ep  27/100 | Loss: 0.5013 | Acc: 0.3926 | F1: 0.3283 | 4.3s   \n    Ep  28/100 | Loss: 0.4811 | Acc: 0.4112 | F1: 0.3575 | 4.6s ‚úÖ\n    Ep  29/100 | Loss: 0.4804 | Acc: 0.3965 | F1: 0.3295 | 4.3s   \n    Ep  30/100 | Loss: 0.4724 | Acc: 0.4054 | F1: 0.3511 | 4.4s   \n    Ep  31/100 | Loss: 0.4617 | Acc: 0.3906 | F1: 0.3374 | 4.4s   \n    Ep  32/100 | Loss: 0.4477 | Acc: 0.4032 | F1: 0.3471 | 4.3s   \n    Ep  33/100 | Loss: 0.4439 | Acc: 0.4203 | F1: 0.3691 | 4.5s ‚úÖ\n    Ep  34/100 | Loss: 0.4410 | Acc: 0.4219 | F1: 0.3678 | 4.4s ‚úÖ\n    Ep  35/100 | Loss: 0.4323 | Acc: 0.4107 | F1: 0.3585 | 4.4s   \n    Ep  36/100 | Loss: 0.4182 | Acc: 0.4309 | F1: 0.3822 | 4.5s ‚úÖ\n    Ep  37/100 | Loss: 0.4192 | Acc: 0.4215 | F1: 0.3668 | 4.4s   \n    Ep  38/100 | Loss: 0.4062 | Acc: 0.4137 | F1: 0.3581 | 4.5s   \n    Ep  39/100 | Loss: 0.3890 | Acc: 0.4321 | F1: 0.3803 | 4.4s ‚úÖ\n    Ep  40/100 | Loss: 0.3895 | Acc: 0.4067 | F1: 0.3508 | 4.4s   \n    Ep  41/100 | Loss: 0.3865 | Acc: 0.4319 | F1: 0.3821 | 4.4s   \n    Ep  42/100 | Loss: 0.3723 | Acc: 0.4337 | F1: 0.3815 | 4.5s ‚úÖ\n    Ep  43/100 | Loss: 0.3703 | Acc: 0.4154 | F1: 0.3645 | 4.3s   \n    Ep  44/100 | Loss: 0.3560 | Acc: 0.4324 | F1: 0.3836 | 4.4s   \n    Ep  45/100 | Loss: 0.3619 | Acc: 0.4298 | F1: 0.3799 | 4.3s   \n    Ep  46/100 | Loss: 0.3483 | Acc: 0.4253 | F1: 0.3736 | 4.3s   \n    Ep  47/100 | Loss: 0.3420 | Acc: 0.4331 | F1: 0.3839 | 4.4s   \n    ‚èπÔ∏è  Early stopping at epoch 47. Best: 42\n    üèÜ Final (best) | Acc: 0.4337 | F1: 0.3815\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 0.5203 | Acc: 0.1862 | F1: 0.1283 | 4.5s ‚úÖ\n    Ep   2/100 | Loss: 0.4385 | Acc: 0.2202 | F1: 0.1756 | 4.4s ‚úÖ\n    Ep   3/100 | Loss: 0.4090 | Acc: 0.2630 | F1: 0.2215 | 4.4s ‚úÖ\n    Ep   4/100 | Loss: 0.3796 | Acc: 0.2701 | F1: 0.2325 | 4.5s ‚úÖ\n    Ep   5/100 | Loss: 0.3649 | Acc: 0.2994 | F1: 0.2517 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 0.3530 | Acc: 0.3084 | F1: 0.2832 | 4.4s ‚úÖ\n    Ep   7/100 | Loss: 0.3354 | Acc: 0.3250 | F1: 0.2951 | 4.7s ‚úÖ\n    Ep   8/100 | Loss: 0.3298 | Acc: 0.3353 | F1: 0.3096 | 4.5s ‚úÖ\n    Ep   9/100 | Loss: 0.3183 | Acc: 0.3690 | F1: 0.3531 | 4.5s ‚úÖ\n    Ep  10/100 | Loss: 0.3094 | Acc: 0.3695 | F1: 0.3511 | 4.5s ‚úÖ\n    Ep  11/100 | Loss: 0.3043 | Acc: 0.3541 | F1: 0.3422 | 4.5s   \n    Ep  12/100 | Loss: 0.2983 | Acc: 0.3749 | F1: 0.3491 | 4.6s ‚úÖ\n    Ep  13/100 | Loss: 0.2867 | Acc: 0.3836 | F1: 0.3717 | 4.5s ‚úÖ\n    Ep  14/100 | Loss: 0.2778 | Acc: 0.3773 | F1: 0.3562 | 4.4s   \n    Ep  15/100 | Loss: 0.2715 | Acc: 0.3897 | F1: 0.3641 | 4.5s ‚úÖ\n    Ep  16/100 | Loss: 0.2679 | Acc: 0.3902 | F1: 0.3710 | 4.4s ‚úÖ\n    Ep  17/100 | Loss: 0.2605 | Acc: 0.4253 | F1: 0.4141 | 4.6s ‚úÖ\n    Ep  18/100 | Loss: 0.2593 | Acc: 0.4063 | F1: 0.3794 | 4.3s   \n    Ep  19/100 | Loss: 0.2489 | Acc: 0.3909 | F1: 0.3628 | 4.3s   \n    Ep  20/100 | Loss: 0.2466 | Acc: 0.4217 | F1: 0.4024 | 4.4s   \n    Ep  21/100 | Loss: 0.2404 | Acc: 0.3831 | F1: 0.3488 | 4.4s   \n    Ep  22/100 | Loss: 0.2342 | Acc: 0.4021 | F1: 0.3734 | 4.3s   \n    ‚èπÔ∏è  Early stopping at epoch 22. Best: 17\n    üèÜ Final (best) | Acc: 0.4253 | F1: 0.4141\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 1.6211 | Acc: 0.1939 | F1: 0.1009 | 4.5s ‚úÖ\n    Ep   2/100 | Loss: 1.2396 | Acc: 0.2231 | F1: 0.1338 | 4.6s ‚úÖ\n    Ep   3/100 | Loss: 1.0712 | Acc: 0.2529 | F1: 0.1561 | 4.5s ‚úÖ\n    Ep   4/100 | Loss: 0.9626 | Acc: 0.2605 | F1: 0.1610 | 4.6s ‚úÖ\n    Ep   5/100 | Loss: 0.8697 | Acc: 0.2709 | F1: 0.1794 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 0.8209 | Acc: 0.2924 | F1: 0.1966 | 4.5s ‚úÖ\n    Ep   7/100 | Loss: 0.7942 | Acc: 0.2958 | F1: 0.2041 | 4.5s ‚úÖ\n    Ep   8/100 | Loss: 0.7695 | Acc: 0.3047 | F1: 0.2155 | 4.4s ‚úÖ\n    Ep   9/100 | Loss: 0.7528 | Acc: 0.3138 | F1: 0.2337 | 4.4s ‚úÖ\n    Ep  10/100 | Loss: 0.7254 | Acc: 0.2769 | F1: 0.1916 | 4.4s   \n    Ep  11/100 | Loss: 0.7151 | Acc: 0.3175 | F1: 0.2319 | 4.5s ‚úÖ\n    Ep  12/100 | Loss: 0.6987 | Acc: 0.3368 | F1: 0.2681 | 4.5s ‚úÖ\n    Ep  13/100 | Loss: 0.6827 | Acc: 0.3440 | F1: 0.2641 | 4.5s ‚úÖ\n    Ep  14/100 | Loss: 0.6696 | Acc: 0.3339 | F1: 0.2529 | 4.4s   \n    Ep  15/100 | Loss: 0.6562 | Acc: 0.3506 | F1: 0.2831 | 4.5s ‚úÖ\n    Ep  16/100 | Loss: 0.6384 | Acc: 0.3458 | F1: 0.2625 | 4.4s   \n    Ep  17/100 | Loss: 0.6197 | Acc: 0.3658 | F1: 0.2930 | 4.5s ‚úÖ\n    Ep  18/100 | Loss: 0.6145 | Acc: 0.3658 | F1: 0.2896 | 4.3s   \n    Ep  19/100 | Loss: 0.5983 | Acc: 0.3561 | F1: 0.2733 | 4.4s   \n    Ep  20/100 | Loss: 0.5886 | Acc: 0.3842 | F1: 0.3174 | 4.5s ‚úÖ\n    Ep  21/100 | Loss: 0.5715 | Acc: 0.3673 | F1: 0.2992 | 4.4s   \n    Ep  22/100 | Loss: 0.5655 | Acc: 0.3795 | F1: 0.3146 | 4.4s   \n    Ep  23/100 | Loss: 0.5534 | Acc: 0.3838 | F1: 0.3158 | 4.3s   \n    Ep  24/100 | Loss: 0.5465 | Acc: 0.3996 | F1: 0.3344 | 4.4s ‚úÖ\n    Ep  25/100 | Loss: 0.5243 | Acc: 0.3999 | F1: 0.3434 | 4.5s ‚úÖ\n    Ep  26/100 | Loss: 0.5265 | Acc: 0.3975 | F1: 0.3376 | 4.4s   \n    Ep  27/100 | Loss: 0.5108 | Acc: 0.3968 | F1: 0.3356 | 4.3s   \n    Ep  28/100 | Loss: 0.4979 | Acc: 0.4009 | F1: 0.3381 | 4.4s ‚úÖ\n    Ep  29/100 | Loss: 0.5031 | Acc: 0.3959 | F1: 0.3245 | 4.4s   \n    Ep  30/100 | Loss: 0.4867 | Acc: 0.4022 | F1: 0.3414 | 4.4s ‚úÖ\n    Ep  31/100 | Loss: 0.4812 | Acc: 0.4002 | F1: 0.3419 | 4.3s   \n    Ep  32/100 | Loss: 0.4622 | Acc: 0.4024 | F1: 0.3374 | 4.4s ‚úÖ\n    Ep  33/100 | Loss: 0.4651 | Acc: 0.4199 | F1: 0.3609 | 4.5s ‚úÖ\n    Ep  34/100 | Loss: 0.4450 | Acc: 0.4389 | F1: 0.3850 | 4.4s ‚úÖ\n    Ep  35/100 | Loss: 0.4439 | Acc: 0.4180 | F1: 0.3650 | 4.3s   \n    Ep  36/100 | Loss: 0.4358 | Acc: 0.4261 | F1: 0.3724 | 4.4s   \n    Ep  37/100 | Loss: 0.4298 | Acc: 0.4146 | F1: 0.3562 | 4.4s   \n    Ep  38/100 | Loss: 0.4226 | Acc: 0.4205 | F1: 0.3686 | 4.2s   \n    Ep  39/100 | Loss: 0.4095 | Acc: 0.4318 | F1: 0.3808 | 4.5s   \n    ‚èπÔ∏è  Early stopping at epoch 39. Best: 34\n    üèÜ Final (best) | Acc: 0.4389 | F1: 0.3850\n\n============================================================\nüìÅ cifar100 | IF=1\n  ‚Üí cifar100 (IF=1): train=50000, test=10000, classes=100\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 4.3970 | Acc: 0.0998 | F1: 0.0757 | 12.5s ‚úÖ\n    Ep   2/100 | Loss: 3.8053 | Acc: 0.1566 | F1: 0.1368 | 12.3s ‚úÖ\n    Ep   3/100 | Loss: 3.5368 | Acc: 0.1922 | F1: 0.1723 | 12.2s ‚úÖ\n    Ep   4/100 | Loss: 3.3504 | Acc: 0.2168 | F1: 0.1965 | 12.4s ‚úÖ\n    Ep   5/100 | Loss: 3.1934 | Acc: 0.2470 | F1: 0.2314 | 12.5s ‚úÖ\n    Ep   6/100 | Loss: 3.0685 | Acc: 0.2590 | F1: 0.2414 | 12.4s ‚úÖ\n    Ep   7/100 | Loss: 2.9640 | Acc: 0.2778 | F1: 0.2653 | 12.7s ‚úÖ\n    Ep   8/100 | Loss: 2.8641 | Acc: 0.2962 | F1: 0.2823 | 12.7s ‚úÖ\n    Ep   9/100 | Loss: 2.7779 | Acc: 0.3091 | F1: 0.2943 | 12.5s ‚úÖ\n    Ep  10/100 | Loss: 2.7102 | Acc: 0.3235 | F1: 0.3128 | 12.5s ‚úÖ\n    Ep  11/100 | Loss: 2.6329 | Acc: 0.3363 | F1: 0.3214 | 12.7s ‚úÖ\n    Ep  12/100 | Loss: 2.5712 | Acc: 0.3415 | F1: 0.3328 | 12.4s ‚úÖ\n    Ep  13/100 | Loss: 2.5102 | Acc: 0.3574 | F1: 0.3462 | 12.5s ‚úÖ\n    Ep  14/100 | Loss: 2.4497 | Acc: 0.3657 | F1: 0.3527 | 12.5s ‚úÖ\n    Ep  15/100 | Loss: 2.4020 | Acc: 0.3660 | F1: 0.3560 | 12.4s ‚úÖ\n    Ep  16/100 | Loss: 2.3459 | Acc: 0.3819 | F1: 0.3724 | 12.6s ‚úÖ\n    Ep  17/100 | Loss: 2.3070 | Acc: 0.3776 | F1: 0.3659 | 12.3s   \n    Ep  18/100 | Loss: 2.2654 | Acc: 0.3823 | F1: 0.3751 | 12.4s ‚úÖ\n    Ep  19/100 | Loss: 2.2164 | Acc: 0.3907 | F1: 0.3815 | 12.3s ‚úÖ\n    Ep  20/100 | Loss: 2.1803 | Acc: 0.3990 | F1: 0.3894 | 12.3s ‚úÖ\n    Ep  21/100 | Loss: 2.1401 | Acc: 0.3898 | F1: 0.3818 | 12.0s   \n    Ep  22/100 | Loss: 2.1036 | Acc: 0.4032 | F1: 0.3967 | 12.3s ‚úÖ\n    Ep  23/100 | Loss: 2.0706 | Acc: 0.4037 | F1: 0.3968 | 12.3s ‚úÖ\n    Ep  24/100 | Loss: 2.0406 | Acc: 0.4117 | F1: 0.4049 | 12.3s ‚úÖ\n    Ep  25/100 | Loss: 2.0044 | Acc: 0.4124 | F1: 0.4050 | 12.4s ‚úÖ\n    Ep  26/100 | Loss: 1.9712 | Acc: 0.4141 | F1: 0.4045 | 12.5s ‚úÖ\n    Ep  27/100 | Loss: 1.9407 | Acc: 0.4195 | F1: 0.4153 | 12.2s ‚úÖ\n    Ep  28/100 | Loss: 1.9064 | Acc: 0.4307 | F1: 0.4250 | 12.5s ‚úÖ\n    Ep  29/100 | Loss: 1.8746 | Acc: 0.4322 | F1: 0.4292 | 12.6s ‚úÖ\n    Ep  30/100 | Loss: 1.8527 | Acc: 0.4294 | F1: 0.4251 | 12.3s   \n    Ep  31/100 | Loss: 1.8183 | Acc: 0.4372 | F1: 0.4307 | 12.4s ‚úÖ\n    Ep  32/100 | Loss: 1.7920 | Acc: 0.4340 | F1: 0.4304 | 12.2s   \n    Ep  33/100 | Loss: 1.7641 | Acc: 0.4441 | F1: 0.4398 | 12.6s ‚úÖ\n    Ep  34/100 | Loss: 1.7389 | Acc: 0.4438 | F1: 0.4383 | 12.4s   \n    Ep  35/100 | Loss: 1.7174 | Acc: 0.4479 | F1: 0.4450 | 12.6s ‚úÖ\n    Ep  36/100 | Loss: 1.6857 | Acc: 0.4463 | F1: 0.4414 | 12.9s   \n    Ep  37/100 | Loss: 1.6605 | Acc: 0.4495 | F1: 0.4453 | 12.6s ‚úÖ\n    Ep  38/100 | Loss: 1.6435 | Acc: 0.4399 | F1: 0.4351 | 12.3s   \n    Ep  39/100 | Loss: 1.6182 | Acc: 0.4483 | F1: 0.4442 | 12.4s   \n    Ep  40/100 | Loss: 1.5936 | Acc: 0.4478 | F1: 0.4439 | 12.4s   \n    Ep  41/100 | Loss: 1.5704 | Acc: 0.4566 | F1: 0.4520 | 12.5s ‚úÖ\n    Ep  42/100 | Loss: 1.5458 | Acc: 0.4539 | F1: 0.4514 | 12.3s   \n    Ep  43/100 | Loss: 1.5243 | Acc: 0.4513 | F1: 0.4450 | 12.4s   \n    Ep  44/100 | Loss: 1.5060 | Acc: 0.4544 | F1: 0.4498 | 12.5s   \n    Ep  45/100 | Loss: 1.4802 | Acc: 0.4505 | F1: 0.4451 | 12.6s   \n    Ep  46/100 | Loss: 1.4737 | Acc: 0.4603 | F1: 0.4555 | 12.8s ‚úÖ\n    Ep  47/100 | Loss: 1.4354 | Acc: 0.4556 | F1: 0.4511 | 12.5s   \n    Ep  48/100 | Loss: 1.4185 | Acc: 0.4646 | F1: 0.4614 | 12.4s ‚úÖ\n    Ep  49/100 | Loss: 1.4067 | Acc: 0.4600 | F1: 0.4567 | 12.5s   \n    Ep  50/100 | Loss: 1.3822 | Acc: 0.4632 | F1: 0.4603 | 12.6s   \n    Ep  51/100 | Loss: 1.3659 | Acc: 0.4611 | F1: 0.4576 | 12.3s   \n    Ep  52/100 | Loss: 1.3522 | Acc: 0.4628 | F1: 0.4600 | 12.6s   \n    Ep  53/100 | Loss: 1.3286 | Acc: 0.4680 | F1: 0.4635 | 12.3s ‚úÖ\n    Ep  54/100 | Loss: 1.3155 | Acc: 0.4700 | F1: 0.4663 | 12.5s ‚úÖ\n    Ep  55/100 | Loss: 1.2929 | Acc: 0.4617 | F1: 0.4586 | 12.2s   \n    Ep  56/100 | Loss: 1.2754 | Acc: 0.4649 | F1: 0.4617 | 12.4s   \n    Ep  57/100 | Loss: 1.2646 | Acc: 0.4654 | F1: 0.4615 | 12.7s   \n    Ep  58/100 | Loss: 1.2439 | Acc: 0.4697 | F1: 0.4666 | 12.4s   \n    Ep  59/100 | Loss: 1.2345 | Acc: 0.4711 | F1: 0.4680 | 12.6s ‚úÖ\n    Ep  60/100 | Loss: 1.2235 | Acc: 0.4658 | F1: 0.4618 | 12.4s   \n    Ep  61/100 | Loss: 1.1986 | Acc: 0.4622 | F1: 0.4587 | 12.4s   \n    Ep  62/100 | Loss: 1.1904 | Acc: 0.4733 | F1: 0.4700 | 12.4s ‚úÖ\n    Ep  63/100 | Loss: 1.1749 | Acc: 0.4736 | F1: 0.4703 | 12.3s ‚úÖ\n    Ep  64/100 | Loss: 1.1642 | Acc: 0.4719 | F1: 0.4680 | 12.5s   \n    Ep  65/100 | Loss: 1.1466 | Acc: 0.4718 | F1: 0.4691 | 12.6s   \n    Ep  66/100 | Loss: 1.1353 | Acc: 0.4734 | F1: 0.4712 | 12.5s   \n    Ep  67/100 | Loss: 1.1252 | Acc: 0.4724 | F1: 0.4698 | 12.4s   \n    Ep  68/100 | Loss: 1.1178 | Acc: 0.4750 | F1: 0.4728 | 12.5s ‚úÖ\n    Ep  69/100 | Loss: 1.1025 | Acc: 0.4753 | F1: 0.4715 | 12.4s ‚úÖ\n    Ep  70/100 | Loss: 1.0911 | Acc: 0.4757 | F1: 0.4728 | 12.4s ‚úÖ\n    Ep  71/100 | Loss: 1.0786 | Acc: 0.4744 | F1: 0.4715 | 12.3s   \n    Ep  72/100 | Loss: 1.0715 | Acc: 0.4742 | F1: 0.4719 | 12.3s   \n    Ep  73/100 | Loss: 1.0657 | Acc: 0.4781 | F1: 0.4746 | 12.5s ‚úÖ\n    Ep  74/100 | Loss: 1.0489 | Acc: 0.4734 | F1: 0.4704 | 12.2s   \n    Ep  75/100 | Loss: 1.0451 | Acc: 0.4752 | F1: 0.4725 | 12.3s   \n    Ep  76/100 | Loss: 1.0322 | Acc: 0.4755 | F1: 0.4722 | 12.3s   \n    Ep  77/100 | Loss: 1.0331 | Acc: 0.4747 | F1: 0.4726 | 12.4s   \n    Ep  78/100 | Loss: 1.0163 | Acc: 0.4766 | F1: 0.4736 | 12.3s   \n    ‚èπÔ∏è  Early stopping at epoch 78. Best: 73\n    üèÜ Final (best) | Acc: 0.4781 | F1: 0.4746\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 4.3663 | Acc: 0.0997 | F1: 0.0753 | 12.4s ‚úÖ\n    Ep   2/100 | Loss: 3.6948 | Acc: 0.1571 | F1: 0.1350 | 12.4s ‚úÖ\n    Ep   3/100 | Loss: 3.3960 | Acc: 0.1890 | F1: 0.1704 | 12.3s ‚úÖ\n    Ep   4/100 | Loss: 3.1919 | Acc: 0.2199 | F1: 0.2036 | 12.7s ‚úÖ\n    Ep   5/100 | Loss: 3.0252 | Acc: 0.2442 | F1: 0.2287 | 12.7s ‚úÖ\n    Ep   6/100 | Loss: 2.8828 | Acc: 0.2701 | F1: 0.2565 | 12.7s ‚úÖ\n    Ep   7/100 | Loss: 2.7592 | Acc: 0.2797 | F1: 0.2709 | 12.7s ‚úÖ\n    Ep   8/100 | Loss: 2.6569 | Acc: 0.3010 | F1: 0.2884 | 12.6s ‚úÖ\n    Ep   9/100 | Loss: 2.5621 | Acc: 0.3115 | F1: 0.2981 | 12.4s ‚úÖ\n    Ep  10/100 | Loss: 2.4871 | Acc: 0.3283 | F1: 0.3179 | 12.3s ‚úÖ\n    Ep  11/100 | Loss: 2.4066 | Acc: 0.3344 | F1: 0.3200 | 12.4s ‚úÖ\n    Ep  12/100 | Loss: 2.3386 | Acc: 0.3386 | F1: 0.3305 | 12.3s ‚úÖ\n    Ep  13/100 | Loss: 2.2767 | Acc: 0.3533 | F1: 0.3438 | 12.5s ‚úÖ\n    Ep  14/100 | Loss: 2.2234 | Acc: 0.3637 | F1: 0.3553 | 12.4s ‚úÖ\n    Ep  15/100 | Loss: 2.1652 | Acc: 0.3685 | F1: 0.3628 | 12.5s ‚úÖ\n    Ep  16/100 | Loss: 2.1170 | Acc: 0.3858 | F1: 0.3790 | 12.5s ‚úÖ\n    Ep  17/100 | Loss: 2.0713 | Acc: 0.3870 | F1: 0.3771 | 12.4s ‚úÖ\n    Ep  18/100 | Loss: 2.0270 | Acc: 0.3825 | F1: 0.3758 | 12.4s   \n    Ep  19/100 | Loss: 1.9830 | Acc: 0.3944 | F1: 0.3886 | 12.4s ‚úÖ\n    Ep  20/100 | Loss: 1.9378 | Acc: 0.4032 | F1: 0.3933 | 12.4s ‚úÖ\n    Ep  21/100 | Loss: 1.9048 | Acc: 0.4084 | F1: 0.4032 | 12.1s ‚úÖ\n    Ep  22/100 | Loss: 1.8642 | Acc: 0.4102 | F1: 0.4059 | 12.2s ‚úÖ\n    Ep  23/100 | Loss: 1.8313 | Acc: 0.4114 | F1: 0.4054 | 12.2s ‚úÖ\n    Ep  24/100 | Loss: 1.7981 | Acc: 0.4184 | F1: 0.4132 | 12.5s ‚úÖ\n    Ep  25/100 | Loss: 1.7568 | Acc: 0.4173 | F1: 0.4110 | 12.2s   \n    Ep  26/100 | Loss: 1.7299 | Acc: 0.4275 | F1: 0.4216 | 12.3s ‚úÖ\n    Ep  27/100 | Loss: 1.6916 | Acc: 0.4279 | F1: 0.4233 | 12.5s ‚úÖ\n    Ep  28/100 | Loss: 1.6630 | Acc: 0.4304 | F1: 0.4256 | 12.3s ‚úÖ\n    Ep  29/100 | Loss: 1.6376 | Acc: 0.4342 | F1: 0.4322 | 12.7s ‚úÖ\n    Ep  30/100 | Loss: 1.6097 | Acc: 0.4350 | F1: 0.4310 | 12.7s ‚úÖ\n    Ep  31/100 | Loss: 1.5739 | Acc: 0.4440 | F1: 0.4390 | 12.8s ‚úÖ\n    Ep  32/100 | Loss: 1.5535 | Acc: 0.4447 | F1: 0.4399 | 12.7s ‚úÖ\n    Ep  33/100 | Loss: 1.5194 | Acc: 0.4461 | F1: 0.4419 | 12.5s ‚úÖ\n    Ep  34/100 | Loss: 1.4974 | Acc: 0.4426 | F1: 0.4399 | 12.6s   \n    Ep  35/100 | Loss: 1.4695 | Acc: 0.4482 | F1: 0.4446 | 12.7s ‚úÖ\n    Ep  36/100 | Loss: 1.4443 | Acc: 0.4484 | F1: 0.4446 | 12.6s ‚úÖ\n    Ep  37/100 | Loss: 1.4233 | Acc: 0.4480 | F1: 0.4448 | 12.6s   \n    Ep  38/100 | Loss: 1.4063 | Acc: 0.4481 | F1: 0.4451 | 12.4s   \n    Ep  39/100 | Loss: 1.3776 | Acc: 0.4536 | F1: 0.4500 | 13.0s ‚úÖ\n    Ep  40/100 | Loss: 1.3552 | Acc: 0.4531 | F1: 0.4487 | 12.4s   \n    Ep  41/100 | Loss: 1.3335 | Acc: 0.4577 | F1: 0.4546 | 12.8s ‚úÖ\n    Ep  42/100 | Loss: 1.3116 | Acc: 0.4569 | F1: 0.4537 | 12.7s   \n    Ep  43/100 | Loss: 1.2829 | Acc: 0.4587 | F1: 0.4541 | 12.7s ‚úÖ\n    Ep  44/100 | Loss: 1.2647 | Acc: 0.4535 | F1: 0.4495 | 12.5s   \n    Ep  45/100 | Loss: 1.2385 | Acc: 0.4595 | F1: 0.4554 | 12.8s ‚úÖ\n    Ep  46/100 | Loss: 1.2254 | Acc: 0.4627 | F1: 0.4604 | 12.8s ‚úÖ\n    Ep  47/100 | Loss: 1.1982 | Acc: 0.4614 | F1: 0.4587 | 12.7s   \n    Ep  48/100 | Loss: 1.1819 | Acc: 0.4605 | F1: 0.4582 | 12.5s   \n    Ep  49/100 | Loss: 1.1715 | Acc: 0.4608 | F1: 0.4588 | 12.7s   \n    Ep  50/100 | Loss: 1.1457 | Acc: 0.4622 | F1: 0.4604 | 12.6s   \n    Ep  51/100 | Loss: 1.1246 | Acc: 0.4668 | F1: 0.4636 | 12.6s ‚úÖ\n    Ep  52/100 | Loss: 1.1122 | Acc: 0.4669 | F1: 0.4640 | 12.8s ‚úÖ\n    Ep  53/100 | Loss: 1.0954 | Acc: 0.4650 | F1: 0.4626 | 12.7s   \n    Ep  54/100 | Loss: 1.0777 | Acc: 0.4597 | F1: 0.4569 | 12.8s   \n    Ep  55/100 | Loss: 1.0581 | Acc: 0.4645 | F1: 0.4623 | 12.4s   \n    Ep  56/100 | Loss: 1.0456 | Acc: 0.4632 | F1: 0.4605 | 12.5s   \n    Ep  57/100 | Loss: 1.0235 | Acc: 0.4707 | F1: 0.4671 | 12.7s ‚úÖ\n    Ep  58/100 | Loss: 1.0091 | Acc: 0.4650 | F1: 0.4619 | 12.8s   \n    Ep  59/100 | Loss: 1.0010 | Acc: 0.4666 | F1: 0.4632 | 12.5s   \n    Ep  60/100 | Loss: 0.9847 | Acc: 0.4696 | F1: 0.4672 | 12.8s   \n    Ep  61/100 | Loss: 0.9694 | Acc: 0.4706 | F1: 0.4688 | 12.6s   \n    Ep  62/100 | Loss: 0.9551 | Acc: 0.4736 | F1: 0.4714 | 12.6s ‚úÖ\n    Ep  63/100 | Loss: 0.9385 | Acc: 0.4705 | F1: 0.4687 | 12.6s   \n    Ep  64/100 | Loss: 0.9266 | Acc: 0.4736 | F1: 0.4716 | 12.6s   \n    Ep  65/100 | Loss: 0.9140 | Acc: 0.4706 | F1: 0.4684 | 12.6s   \n    Ep  66/100 | Loss: 0.9002 | Acc: 0.4711 | F1: 0.4694 | 12.7s   \n    Ep  67/100 | Loss: 0.8941 | Acc: 0.4693 | F1: 0.4671 | 12.5s   \n    ‚èπÔ∏è  Early stopping at epoch 67. Best: 62\n    üèÜ Final (best) | Acc: 0.4736 | F1: 0.4714\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 4.3400 | Acc: 0.1001 | F1: 0.0767 | 12.7s ‚úÖ\n    Ep   2/100 | Loss: 3.6868 | Acc: 0.1650 | F1: 0.1407 | 12.9s ‚úÖ\n    Ep   3/100 | Loss: 3.3916 | Acc: 0.1934 | F1: 0.1735 | 12.8s ‚úÖ\n    Ep   4/100 | Loss: 3.1862 | Acc: 0.2224 | F1: 0.2043 | 13.0s ‚úÖ\n    Ep   5/100 | Loss: 3.0196 | Acc: 0.2458 | F1: 0.2298 | 12.8s ‚úÖ\n    Ep   6/100 | Loss: 2.8775 | Acc: 0.2642 | F1: 0.2485 | 12.8s ‚úÖ\n    Ep   7/100 | Loss: 2.7632 | Acc: 0.2840 | F1: 0.2720 | 13.1s ‚úÖ\n    Ep   8/100 | Loss: 2.6605 | Acc: 0.2931 | F1: 0.2800 | 13.2s ‚úÖ\n    Ep   9/100 | Loss: 2.5682 | Acc: 0.3090 | F1: 0.2944 | 13.4s ‚úÖ\n    Ep  10/100 | Loss: 2.4971 | Acc: 0.3246 | F1: 0.3129 | 13.2s ‚úÖ\n    Ep  11/100 | Loss: 2.4154 | Acc: 0.3285 | F1: 0.3155 | 13.4s ‚úÖ\n    Ep  12/100 | Loss: 2.3487 | Acc: 0.3407 | F1: 0.3320 | 13.1s ‚úÖ\n    Ep  13/100 | Loss: 2.2878 | Acc: 0.3558 | F1: 0.3443 | 13.2s ‚úÖ\n    Ep  14/100 | Loss: 2.2287 | Acc: 0.3612 | F1: 0.3500 | 13.1s ‚úÖ\n    Ep  15/100 | Loss: 2.1705 | Acc: 0.3658 | F1: 0.3564 | 13.0s ‚úÖ\n    Ep  16/100 | Loss: 2.1230 | Acc: 0.3808 | F1: 0.3748 | 13.1s ‚úÖ\n    Ep  17/100 | Loss: 2.0782 | Acc: 0.3815 | F1: 0.3715 | 13.3s ‚úÖ\n    Ep  18/100 | Loss: 2.0360 | Acc: 0.3863 | F1: 0.3775 | 13.1s ‚úÖ\n    Ep  19/100 | Loss: 1.9927 | Acc: 0.3901 | F1: 0.3833 | 13.1s ‚úÖ\n    Ep  20/100 | Loss: 1.9587 | Acc: 0.3944 | F1: 0.3854 | 13.1s ‚úÖ\n    Ep  21/100 | Loss: 1.9204 | Acc: 0.3982 | F1: 0.3917 | 13.1s ‚úÖ\n    Ep  22/100 | Loss: 1.8796 | Acc: 0.4079 | F1: 0.4007 | 13.1s ‚úÖ\n    Ep  23/100 | Loss: 1.8457 | Acc: 0.4070 | F1: 0.4009 | 12.8s   \n    Ep  24/100 | Loss: 1.8092 | Acc: 0.4154 | F1: 0.4062 | 13.2s ‚úÖ\n    Ep  25/100 | Loss: 1.7724 | Acc: 0.4179 | F1: 0.4121 | 13.0s ‚úÖ\n    Ep  26/100 | Loss: 1.7496 | Acc: 0.4210 | F1: 0.4123 | 13.2s ‚úÖ\n    Ep  27/100 | Loss: 1.7186 | Acc: 0.4221 | F1: 0.4151 | 13.0s ‚úÖ\n    Ep  28/100 | Loss: 1.6814 | Acc: 0.4195 | F1: 0.4145 | 13.0s   \n    Ep  29/100 | Loss: 1.6540 | Acc: 0.4245 | F1: 0.4218 | 13.1s ‚úÖ\n    Ep  30/100 | Loss: 1.6304 | Acc: 0.4346 | F1: 0.4290 | 13.2s ‚úÖ\n    Ep  31/100 | Loss: 1.5992 | Acc: 0.4318 | F1: 0.4252 | 13.0s   \n    Ep  32/100 | Loss: 1.5741 | Acc: 0.4378 | F1: 0.4320 | 13.0s ‚úÖ\n    Ep  33/100 | Loss: 1.5422 | Acc: 0.4436 | F1: 0.4380 | 13.1s ‚úÖ\n    Ep  34/100 | Loss: 1.5117 | Acc: 0.4421 | F1: 0.4387 | 12.9s   \n    Ep  35/100 | Loss: 1.4866 | Acc: 0.4448 | F1: 0.4423 | 13.0s ‚úÖ\n    Ep  36/100 | Loss: 1.4575 | Acc: 0.4447 | F1: 0.4401 | 12.7s   \n    Ep  37/100 | Loss: 1.4406 | Acc: 0.4443 | F1: 0.4395 | 12.6s   \n    Ep  38/100 | Loss: 1.4209 | Acc: 0.4349 | F1: 0.4304 | 12.6s   \n    Ep  39/100 | Loss: 1.3929 | Acc: 0.4443 | F1: 0.4390 | 12.5s   \n    Ep  40/100 | Loss: 1.3691 | Acc: 0.4428 | F1: 0.4396 | 12.6s   \n    ‚èπÔ∏è  Early stopping at epoch 40. Best: 35\n    üèÜ Final (best) | Acc: 0.4448 | F1: 0.4423\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 4.4025 | Acc: 0.0975 | F1: 0.0759 | 12.9s ‚úÖ\n    Ep   2/100 | Loss: 3.7902 | Acc: 0.1639 | F1: 0.1437 | 12.8s ‚úÖ\n    Ep   3/100 | Loss: 3.4980 | Acc: 0.1862 | F1: 0.1661 | 12.9s ‚úÖ\n    Ep   4/100 | Loss: 3.2743 | Acc: 0.2182 | F1: 0.1992 | 12.8s ‚úÖ\n    Ep   5/100 | Loss: 3.0817 | Acc: 0.2462 | F1: 0.2297 | 12.9s ‚úÖ\n    Ep   6/100 | Loss: 2.9392 | Acc: 0.2650 | F1: 0.2486 | 12.8s ‚úÖ\n    Ep   7/100 | Loss: 2.8225 | Acc: 0.2802 | F1: 0.2660 | 13.0s ‚úÖ\n    Ep   8/100 | Loss: 2.7221 | Acc: 0.2937 | F1: 0.2800 | 12.9s ‚úÖ\n    Ep   9/100 | Loss: 2.6349 | Acc: 0.3069 | F1: 0.2932 | 12.8s ‚úÖ\n    Ep  10/100 | Loss: 2.5574 | Acc: 0.3207 | F1: 0.3097 | 12.8s ‚úÖ\n    Ep  11/100 | Loss: 2.4731 | Acc: 0.3286 | F1: 0.3174 | 12.7s ‚úÖ\n    Ep  12/100 | Loss: 2.4106 | Acc: 0.3424 | F1: 0.3354 | 12.9s ‚úÖ\n    Ep  13/100 | Loss: 2.3504 | Acc: 0.3521 | F1: 0.3409 | 12.8s ‚úÖ\n    Ep  14/100 | Loss: 2.2944 | Acc: 0.3583 | F1: 0.3454 | 12.7s ‚úÖ\n    Ep  15/100 | Loss: 2.2401 | Acc: 0.3639 | F1: 0.3562 | 12.8s ‚úÖ\n    Ep  16/100 | Loss: 2.1901 | Acc: 0.3765 | F1: 0.3704 | 13.0s ‚úÖ\n    Ep  17/100 | Loss: 2.1472 | Acc: 0.3700 | F1: 0.3610 | 12.8s   \n    Ep  18/100 | Loss: 2.1045 | Acc: 0.3741 | F1: 0.3663 | 12.7s   \n    Ep  19/100 | Loss: 2.0582 | Acc: 0.3856 | F1: 0.3785 | 12.7s ‚úÖ\n    Ep  20/100 | Loss: 2.0237 | Acc: 0.3950 | F1: 0.3856 | 12.7s ‚úÖ\n    Ep  21/100 | Loss: 1.9832 | Acc: 0.3981 | F1: 0.3900 | 12.8s ‚úÖ\n    Ep  22/100 | Loss: 1.9438 | Acc: 0.4071 | F1: 0.4035 | 12.6s ‚úÖ\n    Ep  23/100 | Loss: 1.9120 | Acc: 0.4133 | F1: 0.4062 | 12.7s ‚úÖ\n    Ep  24/100 | Loss: 1.8763 | Acc: 0.4096 | F1: 0.4036 | 12.7s   \n    Ep  25/100 | Loss: 1.8396 | Acc: 0.4115 | F1: 0.4055 | 12.7s   \n    Ep  26/100 | Loss: 1.8082 | Acc: 0.4203 | F1: 0.4134 | 12.9s ‚úÖ\n    Ep  27/100 | Loss: 1.7775 | Acc: 0.4257 | F1: 0.4198 | 12.7s ‚úÖ\n    Ep  28/100 | Loss: 1.7411 | Acc: 0.4192 | F1: 0.4133 | 12.6s   \n    Ep  29/100 | Loss: 1.7155 | Acc: 0.4228 | F1: 0.4180 | 12.4s   \n    Ep  30/100 | Loss: 1.6918 | Acc: 0.4303 | F1: 0.4255 | 12.9s ‚úÖ\n    Ep  31/100 | Loss: 1.6592 | Acc: 0.4339 | F1: 0.4288 | 12.5s ‚úÖ\n    Ep  32/100 | Loss: 1.6320 | Acc: 0.4365 | F1: 0.4307 | 12.6s ‚úÖ\n    Ep  33/100 | Loss: 1.6027 | Acc: 0.4385 | F1: 0.4342 | 13.1s ‚úÖ\n    Ep  34/100 | Loss: 1.5718 | Acc: 0.4433 | F1: 0.4393 | 12.7s ‚úÖ\n    Ep  35/100 | Loss: 1.5473 | Acc: 0.4418 | F1: 0.4385 | 13.0s   \n    Ep  36/100 | Loss: 1.5178 | Acc: 0.4467 | F1: 0.4430 | 13.1s ‚úÖ\n    Ep  37/100 | Loss: 1.4947 | Acc: 0.4480 | F1: 0.4424 | 12.9s ‚úÖ\n    Ep  38/100 | Loss: 1.4820 | Acc: 0.4383 | F1: 0.4328 | 12.9s   \n    Ep  39/100 | Loss: 1.4479 | Acc: 0.4439 | F1: 0.4391 | 13.1s   \n    Ep  40/100 | Loss: 1.4284 | Acc: 0.4508 | F1: 0.4452 | 13.0s ‚úÖ\n    Ep  41/100 | Loss: 1.4004 | Acc: 0.4525 | F1: 0.4466 | 13.0s ‚úÖ\n    Ep  42/100 | Loss: 1.3811 | Acc: 0.4556 | F1: 0.4518 | 13.0s ‚úÖ\n    Ep  43/100 | Loss: 1.3565 | Acc: 0.4529 | F1: 0.4487 | 13.0s   \n    Ep  44/100 | Loss: 1.3357 | Acc: 0.4569 | F1: 0.4527 | 13.0s ‚úÖ\n    Ep  45/100 | Loss: 1.3115 | Acc: 0.4575 | F1: 0.4527 | 12.9s ‚úÖ\n    Ep  46/100 | Loss: 1.3009 | Acc: 0.4560 | F1: 0.4516 | 12.9s   \n    Ep  47/100 | Loss: 1.2700 | Acc: 0.4607 | F1: 0.4562 | 13.1s ‚úÖ\n    Ep  48/100 | Loss: 1.2500 | Acc: 0.4606 | F1: 0.4566 | 13.1s   \n    Ep  49/100 | Loss: 1.2354 | Acc: 0.4576 | F1: 0.4561 | 13.0s   \n    Ep  50/100 | Loss: 1.2114 | Acc: 0.4578 | F1: 0.4543 | 12.8s   \n    Ep  51/100 | Loss: 1.1950 | Acc: 0.4603 | F1: 0.4575 | 12.8s   \n    Ep  52/100 | Loss: 1.1732 | Acc: 0.4585 | F1: 0.4549 | 12.9s   \n    ‚èπÔ∏è  Early stopping at epoch 52. Best: 47\n    üèÜ Final (best) | Acc: 0.4607 | F1: 0.4562\n\n============================================================\nüìÅ cifar100 | IF=100\n  ‚Üí cifar100 (IF=100): train=10899, test=10000, classes=100\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 4.4123 | Acc: 0.0239 | F1: 0.0068 | 4.3s ‚úÖ\n    Ep   2/100 | Loss: 3.9290 | Acc: 0.0423 | F1: 0.0157 | 4.4s ‚úÖ\n    Ep   3/100 | Loss: 3.7159 | Acc: 0.0615 | F1: 0.0238 | 4.3s ‚úÖ\n    Ep   4/100 | Loss: 3.5492 | Acc: 0.0698 | F1: 0.0294 | 4.3s ‚úÖ\n    Ep   5/100 | Loss: 3.4165 | Acc: 0.0786 | F1: 0.0373 | 4.3s ‚úÖ\n    Ep   6/100 | Loss: 3.3084 | Acc: 0.0852 | F1: 0.0404 | 4.4s ‚úÖ\n    Ep   7/100 | Loss: 3.2045 | Acc: 0.0898 | F1: 0.0453 | 4.3s ‚úÖ\n    Ep   8/100 | Loss: 3.1297 | Acc: 0.0984 | F1: 0.0541 | 4.3s ‚úÖ\n    Ep   9/100 | Loss: 3.0672 | Acc: 0.1010 | F1: 0.0562 | 4.3s ‚úÖ\n    Ep  10/100 | Loss: 2.9843 | Acc: 0.1109 | F1: 0.0632 | 4.2s ‚úÖ\n    Ep  11/100 | Loss: 2.9296 | Acc: 0.1121 | F1: 0.0641 | 4.2s ‚úÖ\n    Ep  12/100 | Loss: 2.8667 | Acc: 0.1175 | F1: 0.0695 | 4.4s ‚úÖ\n    Ep  13/100 | Loss: 2.8095 | Acc: 0.1279 | F1: 0.0787 | 4.4s ‚úÖ\n    Ep  14/100 | Loss: 2.7539 | Acc: 0.1321 | F1: 0.0869 | 4.4s ‚úÖ\n    Ep  15/100 | Loss: 2.7072 | Acc: 0.1336 | F1: 0.0862 | 4.3s ‚úÖ\n    Ep  16/100 | Loss: 2.6521 | Acc: 0.1367 | F1: 0.0890 | 4.4s ‚úÖ\n    Ep  17/100 | Loss: 2.6032 | Acc: 0.1375 | F1: 0.0889 | 4.3s ‚úÖ\n    Ep  18/100 | Loss: 2.5675 | Acc: 0.1468 | F1: 0.0966 | 4.2s ‚úÖ\n    Ep  19/100 | Loss: 2.5281 | Acc: 0.1429 | F1: 0.0939 | 4.3s   \n    Ep  20/100 | Loss: 2.4779 | Acc: 0.1516 | F1: 0.1024 | 4.3s ‚úÖ\n    Ep  21/100 | Loss: 2.4281 | Acc: 0.1549 | F1: 0.1047 | 4.3s ‚úÖ\n    Ep  22/100 | Loss: 2.3979 | Acc: 0.1581 | F1: 0.1077 | 4.2s ‚úÖ\n    Ep  23/100 | Loss: 2.3443 | Acc: 0.1562 | F1: 0.1067 | 4.2s   \n    Ep  24/100 | Loss: 2.3022 | Acc: 0.1584 | F1: 0.1086 | 4.3s ‚úÖ\n    Ep  25/100 | Loss: 2.2820 | Acc: 0.1633 | F1: 0.1146 | 4.3s ‚úÖ\n    Ep  26/100 | Loss: 2.2505 | Acc: 0.1636 | F1: 0.1146 | 4.4s ‚úÖ\n    Ep  27/100 | Loss: 2.2090 | Acc: 0.1665 | F1: 0.1183 | 4.3s ‚úÖ\n    Ep  28/100 | Loss: 2.1880 | Acc: 0.1721 | F1: 0.1227 | 4.4s ‚úÖ\n    Ep  29/100 | Loss: 2.1502 | Acc: 0.1708 | F1: 0.1224 | 4.1s   \n    Ep  30/100 | Loss: 2.1212 | Acc: 0.1742 | F1: 0.1263 | 4.3s ‚úÖ\n    Ep  31/100 | Loss: 2.0727 | Acc: 0.1757 | F1: 0.1268 | 4.3s ‚úÖ\n    Ep  32/100 | Loss: 2.0709 | Acc: 0.1753 | F1: 0.1264 | 4.2s   \n    Ep  33/100 | Loss: 2.0375 | Acc: 0.1766 | F1: 0.1276 | 4.4s ‚úÖ\n    Ep  34/100 | Loss: 2.0118 | Acc: 0.1804 | F1: 0.1280 | 4.2s ‚úÖ\n    Ep  35/100 | Loss: 1.9858 | Acc: 0.1759 | F1: 0.1280 | 4.3s   \n    Ep  36/100 | Loss: 1.9590 | Acc: 0.1823 | F1: 0.1305 | 4.4s ‚úÖ\n    Ep  37/100 | Loss: 1.9280 | Acc: 0.1831 | F1: 0.1333 | 4.2s ‚úÖ\n    Ep  38/100 | Loss: 1.8841 | Acc: 0.1823 | F1: 0.1331 | 4.2s   \n    Ep  39/100 | Loss: 1.8626 | Acc: 0.1847 | F1: 0.1335 | 4.3s ‚úÖ\n    Ep  40/100 | Loss: 1.8439 | Acc: 0.1885 | F1: 0.1382 | 4.2s ‚úÖ\n    Ep  41/100 | Loss: 1.8208 | Acc: 0.1825 | F1: 0.1336 | 4.2s   \n    Ep  42/100 | Loss: 1.7947 | Acc: 0.1890 | F1: 0.1398 | 4.4s ‚úÖ\n    Ep  43/100 | Loss: 1.7703 | Acc: 0.1923 | F1: 0.1407 | 4.3s ‚úÖ\n    Ep  44/100 | Loss: 1.7558 | Acc: 0.1920 | F1: 0.1438 | 4.3s   \n    Ep  45/100 | Loss: 1.7264 | Acc: 0.1911 | F1: 0.1417 | 4.2s   \n    Ep  46/100 | Loss: 1.7124 | Acc: 0.1923 | F1: 0.1438 | 4.2s   \n    Ep  47/100 | Loss: 1.6931 | Acc: 0.1928 | F1: 0.1458 | 4.3s ‚úÖ\n    Ep  48/100 | Loss: 1.6810 | Acc: 0.1905 | F1: 0.1426 | 4.1s   \n    Ep  49/100 | Loss: 1.6434 | Acc: 0.1931 | F1: 0.1456 | 4.4s ‚úÖ\n    Ep  50/100 | Loss: 1.6306 | Acc: 0.1952 | F1: 0.1475 | 4.4s ‚úÖ\n    Ep  51/100 | Loss: 1.6174 | Acc: 0.1921 | F1: 0.1448 | 4.3s   \n    Ep  52/100 | Loss: 1.5951 | Acc: 0.1970 | F1: 0.1481 | 4.4s ‚úÖ\n    Ep  53/100 | Loss: 1.5712 | Acc: 0.1947 | F1: 0.1476 | 4.3s   \n    Ep  54/100 | Loss: 1.5661 | Acc: 0.1981 | F1: 0.1501 | 4.5s ‚úÖ\n    Ep  55/100 | Loss: 1.5415 | Acc: 0.1961 | F1: 0.1476 | 4.3s   \n    Ep  56/100 | Loss: 1.5286 | Acc: 0.1954 | F1: 0.1489 | 4.2s   \n    Ep  57/100 | Loss: 1.5060 | Acc: 0.1974 | F1: 0.1494 | 4.3s   \n    Ep  58/100 | Loss: 1.4915 | Acc: 0.1941 | F1: 0.1469 | 4.4s   \n    Ep  59/100 | Loss: 1.4870 | Acc: 0.2012 | F1: 0.1538 | 4.3s ‚úÖ\n    Ep  60/100 | Loss: 1.4602 | Acc: 0.1994 | F1: 0.1537 | 4.3s   \n    Ep  61/100 | Loss: 1.4610 | Acc: 0.2010 | F1: 0.1528 | 4.3s   \n    Ep  62/100 | Loss: 1.4297 | Acc: 0.2007 | F1: 0.1547 | 4.3s   \n    Ep  63/100 | Loss: 1.4287 | Acc: 0.1994 | F1: 0.1526 | 4.3s   \n    Ep  64/100 | Loss: 1.4196 | Acc: 0.1983 | F1: 0.1520 | 4.2s   \n    ‚èπÔ∏è  Early stopping at epoch 64. Best: 59\n    üèÜ Final (best) | Acc: 0.2012 | F1: 0.1538\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 4.3498 | Acc: 0.0267 | F1: 0.0075 | 4.3s ‚úÖ\n    Ep   2/100 | Loss: 3.8324 | Acc: 0.0402 | F1: 0.0142 | 4.5s ‚úÖ\n    Ep   3/100 | Loss: 3.6131 | Acc: 0.0551 | F1: 0.0215 | 4.3s ‚úÖ\n    Ep   4/100 | Loss: 3.4309 | Acc: 0.0651 | F1: 0.0269 | 4.4s ‚úÖ\n    Ep   5/100 | Loss: 3.2915 | Acc: 0.0724 | F1: 0.0336 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 3.1736 | Acc: 0.0807 | F1: 0.0381 | 4.5s ‚úÖ\n    Ep   7/100 | Loss: 3.0745 | Acc: 0.0892 | F1: 0.0464 | 4.3s ‚úÖ\n    Ep   8/100 | Loss: 2.9854 | Acc: 0.0978 | F1: 0.0535 | 4.5s ‚úÖ\n    Ep   9/100 | Loss: 2.9053 | Acc: 0.1013 | F1: 0.0575 | 4.4s ‚úÖ\n    Ep  10/100 | Loss: 2.8331 | Acc: 0.1108 | F1: 0.0636 | 4.4s ‚úÖ\n    Ep  11/100 | Loss: 2.7760 | Acc: 0.1164 | F1: 0.0699 | 4.3s ‚úÖ\n    Ep  12/100 | Loss: 2.7010 | Acc: 0.1203 | F1: 0.0734 | 4.4s ‚úÖ\n    Ep  13/100 | Loss: 2.6388 | Acc: 0.1294 | F1: 0.0822 | 4.5s ‚úÖ\n    Ep  14/100 | Loss: 2.5829 | Acc: 0.1317 | F1: 0.0838 | 4.5s ‚úÖ\n    Ep  15/100 | Loss: 2.5411 | Acc: 0.1329 | F1: 0.0858 | 4.6s ‚úÖ\n    Ep  16/100 | Loss: 2.4901 | Acc: 0.1395 | F1: 0.0894 | 4.4s ‚úÖ\n    Ep  17/100 | Loss: 2.4320 | Acc: 0.1388 | F1: 0.0922 | 4.3s   \n    Ep  18/100 | Loss: 2.3931 | Acc: 0.1416 | F1: 0.0942 | 4.5s ‚úÖ\n    Ep  19/100 | Loss: 2.3477 | Acc: 0.1473 | F1: 0.0971 | 4.6s ‚úÖ\n    Ep  20/100 | Loss: 2.3016 | Acc: 0.1499 | F1: 0.1013 | 4.5s ‚úÖ\n    Ep  21/100 | Loss: 2.2677 | Acc: 0.1549 | F1: 0.1063 | 4.4s ‚úÖ\n    Ep  22/100 | Loss: 2.2190 | Acc: 0.1550 | F1: 0.1033 | 4.5s ‚úÖ\n    Ep  23/100 | Loss: 2.1820 | Acc: 0.1591 | F1: 0.1091 | 4.5s ‚úÖ\n    Ep  24/100 | Loss: 2.1401 | Acc: 0.1544 | F1: 0.1057 | 4.3s   \n    Ep  25/100 | Loss: 2.1022 | Acc: 0.1604 | F1: 0.1110 | 4.4s ‚úÖ\n    Ep  26/100 | Loss: 2.0608 | Acc: 0.1637 | F1: 0.1154 | 4.5s ‚úÖ\n    Ep  27/100 | Loss: 2.0316 | Acc: 0.1660 | F1: 0.1179 | 4.5s ‚úÖ\n    Ep  28/100 | Loss: 1.9856 | Acc: 0.1715 | F1: 0.1220 | 4.5s ‚úÖ\n    Ep  29/100 | Loss: 1.9624 | Acc: 0.1708 | F1: 0.1205 | 4.3s   \n    Ep  30/100 | Loss: 1.9365 | Acc: 0.1703 | F1: 0.1232 | 4.4s   \n    Ep  31/100 | Loss: 1.8918 | Acc: 0.1699 | F1: 0.1228 | 4.3s   \n    Ep  32/100 | Loss: 1.8640 | Acc: 0.1751 | F1: 0.1262 | 4.5s ‚úÖ\n    Ep  33/100 | Loss: 1.8470 | Acc: 0.1766 | F1: 0.1254 | 4.5s ‚úÖ\n    Ep  34/100 | Loss: 1.8085 | Acc: 0.1764 | F1: 0.1265 | 4.2s   \n    Ep  35/100 | Loss: 1.7730 | Acc: 0.1773 | F1: 0.1308 | 4.4s ‚úÖ\n    Ep  36/100 | Loss: 1.7454 | Acc: 0.1764 | F1: 0.1283 | 4.2s   \n    Ep  37/100 | Loss: 1.7241 | Acc: 0.1786 | F1: 0.1308 | 4.4s ‚úÖ\n    Ep  38/100 | Loss: 1.6912 | Acc: 0.1764 | F1: 0.1295 | 4.2s   \n    Ep  39/100 | Loss: 1.6626 | Acc: 0.1807 | F1: 0.1366 | 4.4s ‚úÖ\n    Ep  40/100 | Loss: 1.6377 | Acc: 0.1822 | F1: 0.1337 | 4.4s ‚úÖ\n    Ep  41/100 | Loss: 1.6208 | Acc: 0.1815 | F1: 0.1331 | 4.3s   \n    Ep  42/100 | Loss: 1.5924 | Acc: 0.1833 | F1: 0.1377 | 4.4s ‚úÖ\n    Ep  43/100 | Loss: 1.5641 | Acc: 0.1829 | F1: 0.1358 | 4.3s   \n    Ep  44/100 | Loss: 1.5505 | Acc: 0.1850 | F1: 0.1417 | 4.5s ‚úÖ\n    Ep  45/100 | Loss: 1.5236 | Acc: 0.1862 | F1: 0.1404 | 4.5s ‚úÖ\n    Ep  46/100 | Loss: 1.5158 | Acc: 0.1842 | F1: 0.1389 | 4.3s   \n    Ep  47/100 | Loss: 1.4845 | Acc: 0.1913 | F1: 0.1474 | 4.4s ‚úÖ\n    Ep  48/100 | Loss: 1.4796 | Acc: 0.1906 | F1: 0.1449 | 4.3s   \n    Ep  49/100 | Loss: 1.4306 | Acc: 0.1913 | F1: 0.1445 | 4.3s   \n    Ep  50/100 | Loss: 1.4194 | Acc: 0.1914 | F1: 0.1465 | 4.4s ‚úÖ\n    Ep  51/100 | Loss: 1.4055 | Acc: 0.1888 | F1: 0.1435 | 4.3s   \n    Ep  52/100 | Loss: 1.3996 | Acc: 0.1910 | F1: 0.1459 | 4.2s   \n    Ep  53/100 | Loss: 1.3673 | Acc: 0.1929 | F1: 0.1473 | 4.5s ‚úÖ\n    Ep  54/100 | Loss: 1.3511 | Acc: 0.1933 | F1: 0.1485 | 4.3s ‚úÖ\n    Ep  55/100 | Loss: 1.3436 | Acc: 0.1923 | F1: 0.1467 | 4.4s   \n    Ep  56/100 | Loss: 1.3148 | Acc: 0.1920 | F1: 0.1474 | 4.3s   \n    Ep  57/100 | Loss: 1.3018 | Acc: 0.1922 | F1: 0.1464 | 4.3s   \n    Ep  58/100 | Loss: 1.2914 | Acc: 0.1939 | F1: 0.1496 | 4.5s ‚úÖ\n    Ep  59/100 | Loss: 1.2753 | Acc: 0.1973 | F1: 0.1527 | 4.3s ‚úÖ\n    Ep  60/100 | Loss: 1.2485 | Acc: 0.1958 | F1: 0.1524 | 4.3s   \n    Ep  61/100 | Loss: 1.2547 | Acc: 0.1972 | F1: 0.1529 | 4.2s   \n    Ep  62/100 | Loss: 1.2261 | Acc: 0.1984 | F1: 0.1548 | 4.4s ‚úÖ\n    Ep  63/100 | Loss: 1.2213 | Acc: 0.1990 | F1: 0.1549 | 4.4s ‚úÖ\n    Ep  64/100 | Loss: 1.1994 | Acc: 0.1968 | F1: 0.1541 | 4.2s   \n    Ep  65/100 | Loss: 1.1919 | Acc: 0.1972 | F1: 0.1533 | 4.3s   \n    Ep  66/100 | Loss: 1.2005 | Acc: 0.2001 | F1: 0.1548 | 4.3s ‚úÖ\n    Ep  67/100 | Loss: 1.1663 | Acc: 0.2015 | F1: 0.1581 | 4.5s ‚úÖ\n    Ep  68/100 | Loss: 1.1684 | Acc: 0.1988 | F1: 0.1552 | 4.2s   \n    Ep  69/100 | Loss: 1.1665 | Acc: 0.1984 | F1: 0.1555 | 4.3s   \n    Ep  70/100 | Loss: 1.1338 | Acc: 0.2000 | F1: 0.1557 | 4.3s   \n    Ep  71/100 | Loss: 1.1363 | Acc: 0.2001 | F1: 0.1570 | 4.3s   \n    Ep  72/100 | Loss: 1.1362 | Acc: 0.1986 | F1: 0.1541 | 4.3s   \n    ‚èπÔ∏è  Early stopping at epoch 72. Best: 67\n    üèÜ Final (best) | Acc: 0.2015 | F1: 0.1581\n\n  üîç Training with CBF_b0.999_g1\n    Ep   1/100 | Loss: 1.0467 | Acc: 0.0148 | F1: 0.0101 | 4.3s ‚úÖ\n    Ep   2/100 | Loss: 1.0180 | Acc: 0.0231 | F1: 0.0181 | 4.3s ‚úÖ\n    Ep   3/100 | Loss: 0.9908 | Acc: 0.0320 | F1: 0.0244 | 4.4s ‚úÖ\n    Ep   4/100 | Loss: 0.9581 | Acc: 0.0463 | F1: 0.0333 | 4.5s ‚úÖ\n    Ep   5/100 | Loss: 0.9252 | Acc: 0.0585 | F1: 0.0432 | 4.5s ‚úÖ\n    Ep   6/100 | Loss: 0.9034 | Acc: 0.0646 | F1: 0.0490 | 4.4s ‚úÖ\n    Ep   7/100 | Loss: 0.8730 | Acc: 0.0750 | F1: 0.0589 | 4.4s ‚úÖ\n    Ep   8/100 | Loss: 0.8513 | Acc: 0.0825 | F1: 0.0636 | 4.6s ‚úÖ\n    Ep   9/100 | Loss: 0.8247 | Acc: 0.0866 | F1: 0.0660 | 4.6s ‚úÖ\n    Ep  10/100 | Loss: 0.8066 | Acc: 0.0936 | F1: 0.0735 | 4.4s ‚úÖ\n    Ep  11/100 | Loss: 0.7878 | Acc: 0.0996 | F1: 0.0814 | 4.5s ‚úÖ\n    Ep  12/100 | Loss: 0.7718 | Acc: 0.1052 | F1: 0.0856 | 4.6s ‚úÖ\n    Ep  13/100 | Loss: 0.7536 | Acc: 0.1076 | F1: 0.0899 | 4.5s ‚úÖ\n    Ep  14/100 | Loss: 0.7280 | Acc: 0.1165 | F1: 0.0985 | 4.5s ‚úÖ\n    Ep  15/100 | Loss: 0.7164 | Acc: 0.1159 | F1: 0.0958 | 4.4s   \n    Ep  16/100 | Loss: 0.7000 | Acc: 0.1170 | F1: 0.1002 | 4.4s ‚úÖ\n    Ep  17/100 | Loss: 0.6892 | Acc: 0.1206 | F1: 0.1048 | 4.5s ‚úÖ\n    Ep  18/100 | Loss: 0.6701 | Acc: 0.1237 | F1: 0.1068 | 4.5s ‚úÖ\n    Ep  19/100 | Loss: 0.6581 | Acc: 0.1305 | F1: 0.1144 | 4.5s ‚úÖ\n    Ep  20/100 | Loss: 0.6421 | Acc: 0.1331 | F1: 0.1169 | 4.4s ‚úÖ\n    Ep  21/100 | Loss: 0.6297 | Acc: 0.1343 | F1: 0.1164 | 4.5s ‚úÖ\n    Ep  22/100 | Loss: 0.6217 | Acc: 0.1360 | F1: 0.1186 | 4.5s ‚úÖ\n    Ep  23/100 | Loss: 0.6045 | Acc: 0.1408 | F1: 0.1252 | 4.4s ‚úÖ\n    Ep  24/100 | Loss: 0.5913 | Acc: 0.1393 | F1: 0.1243 | 4.3s   \n    Ep  25/100 | Loss: 0.5808 | Acc: 0.1373 | F1: 0.1197 | 4.2s   \n    Ep  26/100 | Loss: 0.5737 | Acc: 0.1459 | F1: 0.1326 | 4.6s ‚úÖ\n    Ep  27/100 | Loss: 0.5531 | Acc: 0.1466 | F1: 0.1329 | 4.4s ‚úÖ\n    Ep  28/100 | Loss: 0.5456 | Acc: 0.1457 | F1: 0.1317 | 4.2s   \n    Ep  29/100 | Loss: 0.5375 | Acc: 0.1508 | F1: 0.1356 | 4.5s ‚úÖ\n    Ep  30/100 | Loss: 0.5289 | Acc: 0.1482 | F1: 0.1353 | 4.3s   \n    Ep  31/100 | Loss: 0.5120 | Acc: 0.1520 | F1: 0.1390 | 4.4s ‚úÖ\n    Ep  32/100 | Loss: 0.5029 | Acc: 0.1552 | F1: 0.1413 | 4.3s ‚úÖ\n    Ep  33/100 | Loss: 0.4929 | Acc: 0.1564 | F1: 0.1427 | 4.4s ‚úÖ\n    Ep  34/100 | Loss: 0.4804 | Acc: 0.1527 | F1: 0.1389 | 4.2s   \n    Ep  35/100 | Loss: 0.4797 | Acc: 0.1596 | F1: 0.1453 | 4.4s ‚úÖ\n    Ep  36/100 | Loss: 0.4664 | Acc: 0.1568 | F1: 0.1415 | 4.3s   \n    Ep  37/100 | Loss: 0.4633 | Acc: 0.1571 | F1: 0.1442 | 4.3s   \n    Ep  38/100 | Loss: 0.4488 | Acc: 0.1566 | F1: 0.1436 | 4.3s   \n    Ep  39/100 | Loss: 0.4375 | Acc: 0.1623 | F1: 0.1464 | 4.4s ‚úÖ\n    Ep  40/100 | Loss: 0.4347 | Acc: 0.1651 | F1: 0.1483 | 4.5s ‚úÖ\n    Ep  41/100 | Loss: 0.4270 | Acc: 0.1638 | F1: 0.1474 | 4.3s   \n    Ep  42/100 | Loss: 0.4211 | Acc: 0.1647 | F1: 0.1487 | 4.2s   \n    Ep  43/100 | Loss: 0.4118 | Acc: 0.1642 | F1: 0.1478 | 4.2s   \n    Ep  44/100 | Loss: 0.4141 | Acc: 0.1671 | F1: 0.1510 | 4.3s ‚úÖ\n    Ep  45/100 | Loss: 0.3986 | Acc: 0.1690 | F1: 0.1524 | 4.5s ‚úÖ\n    Ep  46/100 | Loss: 0.4002 | Acc: 0.1682 | F1: 0.1533 | 4.3s   \n    Ep  47/100 | Loss: 0.3862 | Acc: 0.1695 | F1: 0.1526 | 4.4s ‚úÖ\n    Ep  48/100 | Loss: 0.3881 | Acc: 0.1695 | F1: 0.1523 | 4.3s   \n    Ep  49/100 | Loss: 0.3813 | Acc: 0.1692 | F1: 0.1529 | 4.3s   \n    Ep  50/100 | Loss: 0.3668 | Acc: 0.1696 | F1: 0.1535 | 4.3s ‚úÖ\n    Ep  51/100 | Loss: 0.3685 | Acc: 0.1695 | F1: 0.1528 | 4.2s   \n    Ep  52/100 | Loss: 0.3652 | Acc: 0.1708 | F1: 0.1536 | 4.3s ‚úÖ\n    Ep  53/100 | Loss: 0.3580 | Acc: 0.1723 | F1: 0.1535 | 4.4s ‚úÖ\n    Ep  54/100 | Loss: 0.3526 | Acc: 0.1744 | F1: 0.1562 | 4.5s ‚úÖ\n    Ep  55/100 | Loss: 0.3509 | Acc: 0.1735 | F1: 0.1565 | 4.2s   \n    Ep  56/100 | Loss: 0.3422 | Acc: 0.1726 | F1: 0.1550 | 4.3s   \n    Ep  57/100 | Loss: 0.3404 | Acc: 0.1770 | F1: 0.1584 | 4.5s ‚úÖ\n    Ep  58/100 | Loss: 0.3330 | Acc: 0.1779 | F1: 0.1587 | 4.4s ‚úÖ\n    Ep  59/100 | Loss: 0.3344 | Acc: 0.1742 | F1: 0.1561 | 4.2s   \n    Ep  60/100 | Loss: 0.3290 | Acc: 0.1787 | F1: 0.1598 | 4.4s ‚úÖ\n    Ep  61/100 | Loss: 0.3276 | Acc: 0.1745 | F1: 0.1566 | 4.5s   \n    Ep  62/100 | Loss: 0.3230 | Acc: 0.1779 | F1: 0.1585 | 4.3s   \n    Ep  63/100 | Loss: 0.3201 | Acc: 0.1776 | F1: 0.1586 | 4.5s   \n    Ep  64/100 | Loss: 0.3160 | Acc: 0.1768 | F1: 0.1579 | 4.2s   \n    Ep  65/100 | Loss: 0.3139 | Acc: 0.1787 | F1: 0.1589 | 4.3s   \n    ‚èπÔ∏è  Early stopping at epoch 65. Best: 60\n    üèÜ Final (best) | Acc: 0.1787 | F1: 0.1598\n\n  üîç Training with CDG\n    Ep   1/100 | Loss: 4.4148 | Acc: 0.0268 | F1: 0.0076 | 4.4s ‚úÖ\n    Ep   2/100 | Loss: 3.9156 | Acc: 0.0408 | F1: 0.0146 | 4.5s ‚úÖ\n    Ep   3/100 | Loss: 3.6939 | Acc: 0.0552 | F1: 0.0221 | 4.5s ‚úÖ\n    Ep   4/100 | Loss: 3.4878 | Acc: 0.0673 | F1: 0.0280 | 4.4s ‚úÖ\n    Ep   5/100 | Loss: 3.3336 | Acc: 0.0780 | F1: 0.0349 | 4.4s ‚úÖ\n    Ep   6/100 | Loss: 3.2123 | Acc: 0.0814 | F1: 0.0376 | 4.4s ‚úÖ\n    Ep   7/100 | Loss: 3.1073 | Acc: 0.0877 | F1: 0.0427 | 4.5s ‚úÖ\n    Ep   8/100 | Loss: 3.0218 | Acc: 0.0923 | F1: 0.0485 | 4.4s ‚úÖ\n    Ep   9/100 | Loss: 2.9509 | Acc: 0.0979 | F1: 0.0521 | 4.3s ‚úÖ\n    Ep  10/100 | Loss: 2.8775 | Acc: 0.1040 | F1: 0.0591 | 4.4s ‚úÖ\n    Ep  11/100 | Loss: 2.8254 | Acc: 0.1090 | F1: 0.0631 | 4.5s ‚úÖ\n    Ep  12/100 | Loss: 2.7503 | Acc: 0.1153 | F1: 0.0704 | 4.3s ‚úÖ\n    Ep  13/100 | Loss: 2.6965 | Acc: 0.1174 | F1: 0.0717 | 4.3s ‚úÖ\n    Ep  14/100 | Loss: 2.6390 | Acc: 0.1300 | F1: 0.0839 | 4.4s ‚úÖ\n    Ep  15/100 | Loss: 2.5750 | Acc: 0.1209 | F1: 0.0777 | 4.2s   \n    Ep  16/100 | Loss: 2.5410 | Acc: 0.1307 | F1: 0.0851 | 4.4s ‚úÖ\n    Ep  17/100 | Loss: 2.4924 | Acc: 0.1320 | F1: 0.0870 | 4.4s ‚úÖ\n    Ep  18/100 | Loss: 2.4421 | Acc: 0.1404 | F1: 0.0944 | 4.4s ‚úÖ\n    Ep  19/100 | Loss: 2.4022 | Acc: 0.1398 | F1: 0.0923 | 4.3s   \n    Ep  20/100 | Loss: 2.3545 | Acc: 0.1420 | F1: 0.0961 | 4.4s ‚úÖ\n    Ep  21/100 | Loss: 2.3128 | Acc: 0.1458 | F1: 0.0976 | 4.4s ‚úÖ\n    Ep  22/100 | Loss: 2.2790 | Acc: 0.1525 | F1: 0.1013 | 4.4s ‚úÖ\n    Ep  23/100 | Loss: 2.2409 | Acc: 0.1441 | F1: 0.0985 | 4.3s   \n    Ep  24/100 | Loss: 2.2043 | Acc: 0.1491 | F1: 0.1028 | 4.3s   \n    Ep  25/100 | Loss: 2.1715 | Acc: 0.1577 | F1: 0.1101 | 4.4s ‚úÖ\n    Ep  26/100 | Loss: 2.1312 | Acc: 0.1572 | F1: 0.1091 | 4.3s   \n    Ep  27/100 | Loss: 2.0880 | Acc: 0.1541 | F1: 0.1056 | 4.2s   \n    Ep  28/100 | Loss: 2.0650 | Acc: 0.1644 | F1: 0.1170 | 4.4s ‚úÖ\n    Ep  29/100 | Loss: 2.0343 | Acc: 0.1620 | F1: 0.1166 | 4.3s   \n    Ep  30/100 | Loss: 2.0068 | Acc: 0.1675 | F1: 0.1207 | 4.5s ‚úÖ\n    Ep  31/100 | Loss: 1.9705 | Acc: 0.1625 | F1: 0.1146 | 4.3s   \n    Ep  32/100 | Loss: 1.9469 | Acc: 0.1678 | F1: 0.1210 | 4.5s ‚úÖ\n    Ep  33/100 | Loss: 1.9158 | Acc: 0.1671 | F1: 0.1207 | 4.3s   \n    Ep  34/100 | Loss: 1.8893 | Acc: 0.1698 | F1: 0.1224 | 4.4s ‚úÖ\n    Ep  35/100 | Loss: 1.8560 | Acc: 0.1693 | F1: 0.1264 | 4.3s   \n    Ep  36/100 | Loss: 1.8214 | Acc: 0.1698 | F1: 0.1224 | 4.3s   \n    Ep  37/100 | Loss: 1.8054 | Acc: 0.1718 | F1: 0.1257 | 4.3s ‚úÖ\n    Ep  38/100 | Loss: 1.7552 | Acc: 0.1729 | F1: 0.1284 | 4.4s ‚úÖ\n    Ep  39/100 | Loss: 1.7434 | Acc: 0.1731 | F1: 0.1263 | 4.4s ‚úÖ\n    Ep  40/100 | Loss: 1.7123 | Acc: 0.1758 | F1: 0.1293 | 4.4s ‚úÖ\n    Ep  41/100 | Loss: 1.6946 | Acc: 0.1735 | F1: 0.1266 | 4.3s   \n    Ep  42/100 | Loss: 1.6745 | Acc: 0.1757 | F1: 0.1308 | 4.3s   \n    Ep  43/100 | Loss: 1.6365 | Acc: 0.1777 | F1: 0.1330 | 4.4s ‚úÖ\n    Ep  44/100 | Loss: 1.6347 | Acc: 0.1774 | F1: 0.1322 | 4.4s   \n    Ep  45/100 | Loss: 1.6043 | Acc: 0.1831 | F1: 0.1374 | 4.4s ‚úÖ\n    Ep  46/100 | Loss: 1.5811 | Acc: 0.1766 | F1: 0.1312 | 4.3s   \n    Ep  47/100 | Loss: 1.5715 | Acc: 0.1827 | F1: 0.1381 | 4.3s   \n    Ep  48/100 | Loss: 1.5431 | Acc: 0.1820 | F1: 0.1355 | 4.3s   \n    Ep  49/100 | Loss: 1.5173 | Acc: 0.1811 | F1: 0.1360 | 4.2s   \n    Ep  50/100 | Loss: 1.4999 | Acc: 0.1837 | F1: 0.1388 | 4.4s ‚úÖ\n    Ep  51/100 | Loss: 1.4774 | Acc: 0.1843 | F1: 0.1391 | 4.3s ‚úÖ\n    Ep  52/100 | Loss: 1.4691 | Acc: 0.1872 | F1: 0.1394 | 4.4s ‚úÖ\n    Ep  53/100 | Loss: 1.4470 | Acc: 0.1847 | F1: 0.1403 | 4.3s   \n    Ep  54/100 | Loss: 1.4370 | Acc: 0.1881 | F1: 0.1441 | 4.5s ‚úÖ\n    Ep  55/100 | Loss: 1.4226 | Acc: 0.1853 | F1: 0.1405 | 4.2s   \n    Ep  56/100 | Loss: 1.3944 | Acc: 0.1861 | F1: 0.1409 | 4.3s   \n    Ep  57/100 | Loss: 1.3695 | Acc: 0.1874 | F1: 0.1416 | 4.3s   \n    Ep  58/100 | Loss: 1.3693 | Acc: 0.1856 | F1: 0.1410 | 4.3s   \n    Ep  59/100 | Loss: 1.3529 | Acc: 0.1899 | F1: 0.1450 | 4.4s ‚úÖ\n    Ep  60/100 | Loss: 1.3211 | Acc: 0.1888 | F1: 0.1457 | 4.4s   \n    Ep  61/100 | Loss: 1.3206 | Acc: 0.1904 | F1: 0.1455 | 4.3s ‚úÖ\n    Ep  62/100 | Loss: 1.2959 | Acc: 0.1891 | F1: 0.1425 | 4.4s   \n    Ep  63/100 | Loss: 1.2948 | Acc: 0.1906 | F1: 0.1450 | 4.4s ‚úÖ\n    Ep  64/100 | Loss: 1.2821 | Acc: 0.1903 | F1: 0.1463 | 4.2s   \n    Ep  65/100 | Loss: 1.2594 | Acc: 0.1910 | F1: 0.1462 | 4.3s ‚úÖ\n    Ep  66/100 | Loss: 1.2717 | Acc: 0.1953 | F1: 0.1497 | 4.4s ‚úÖ\n    Ep  67/100 | Loss: 1.2477 | Acc: 0.1947 | F1: 0.1494 | 4.3s   \n    Ep  68/100 | Loss: 1.2494 | Acc: 0.1903 | F1: 0.1460 | 4.3s   \n    Ep  69/100 | Loss: 1.2239 | Acc: 0.1924 | F1: 0.1467 | 4.3s   \n    Ep  70/100 | Loss: 1.2121 | Acc: 0.1955 | F1: 0.1506 | 4.4s ‚úÖ\n    Ep  71/100 | Loss: 1.2092 | Acc: 0.1970 | F1: 0.1528 | 4.6s ‚úÖ\n    Ep  72/100 | Loss: 1.1989 | Acc: 0.1938 | F1: 0.1501 | 4.4s   \n    Ep  73/100 | Loss: 1.1984 | Acc: 0.1941 | F1: 0.1494 | 4.3s   \n    Ep  74/100 | Loss: 1.1905 | Acc: 0.1940 | F1: 0.1498 | 4.3s   \n    Ep  75/100 | Loss: 1.1831 | Acc: 0.1946 | F1: 0.1505 | 4.2s   \n    Ep  76/100 | Loss: 1.1619 | Acc: 0.1930 | F1: 0.1494 | 4.3s   \n    ‚èπÔ∏è  Early stopping at epoch 76. Best: 71\n    üèÜ Final (best) | Acc: 0.1970 | F1: 0.1528\n\n‚úÖ Training completed! Results saved to:\n/kaggle/working/loss_eval_results/summary_full_results.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ‚úÖ CDG with Head/Mid/Tail accuracy for CIFAR-10-LT\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ---------------------- CDG LOSS (TUNED) ----------------------\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype, device=p.device))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=10):\n        log_p = F.log_softmax(logits, dim=1)\n        log_pt = log_p.gather(1, y[:, None]).squeeze()\n        pt = torch.exp(log_pt).clamp(min=self.eps, max=1.0 - self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        loss = - ((1.0 - pt) ** g) * log_pt\n        return loss.mean()\n\n# ---------------------- SPLIT ACCURACY ----------------------\ndef compute_head_mid_tail_acc(preds, targets, dataset_name, imb_factor):\n    \"\"\"\n    Compute Head/Mid/Tail accuracy for CIFAR-10-LT (IF=100)\n    Standard split: Head=0-2, Mid=3-5, Tail=6-9\n    \"\"\"\n    if dataset_name.lower() != 'cifar10' or imb_factor != 100:\n        return None, None, None\n\n    preds, targets = np.array(preds), np.array(targets)\n    \n    head_classes = [0, 1, 2]\n    mid_classes = [3, 4, 5]\n    tail_classes = [6, 7, 8, 9]\n    \n    def acc_for_classes(classes):\n        mask = np.isin(targets, classes)\n        if mask.sum() == 0:\n            return np.nan\n        return (preds[mask] == targets[mask]).mean()\n    \n    head_acc = acc_for_classes(head_classes)\n    mid_acc = acc_for_classes(mid_classes)\n    tail_acc = acc_for_classes(tail_classes)\n    \n    return head_acc, mid_acc, tail_acc\n\n# ---------------------- DATASET PREP ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\ndef get_sampler_for_imbalance(targets, num_classes):\n    counts = np.bincount(targets, minlength=num_classes)\n    weight_per_class = 1.0 / (counts + 1e-6)\n    weights = weight_per_class[targets]\n    return WeightedRandomSampler(weights, len(weights), replacement=True)\n\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n    if name_l in ('cifar10', 'cifar100'):\n        mean = (0.4914, 0.4822, 0.4465) if name_l == 'cifar10' else (0.5071, 0.4867, 0.4408)\n        std = (0.2023, 0.1994, 0.2010) if name_l == 'cifar10' else (0.2675, 0.2565, 0.2761)\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        test_tr = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n    else:\n        train_tr = transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x\n        ])\n        test_tr = train_tr\n\n    if name_l == 'mnist':\n        base_train = datasets.MNIST(root, train=True, download=True, transform=train_tr)\n        test = datasets.MNIST(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'fashionmnist':\n        base_train = datasets.FashionMNIST(root, train=True, download=True, transform=train_tr)\n        test = datasets.FashionMNIST(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'svhn':\n        base_train = datasets.SVHN(root, split='train', download=True, transform=train_tr)\n        test = datasets.SVHN(root, split='test', download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'cifar10':\n        base_train = datasets.CIFAR10(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'cifar100':\n        base_train = datasets.CIFAR100(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=True, transform=test_tr)\n        C = 100\n    else:\n        raise ValueError(f\"Unsupported: {name}\")\n\n    if name_l in ('cifar10','cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        train_targets = np.array([base_train.targets[i] for i in indices])\n        counts = cls_counts\n    else:\n        train = base_train\n        if hasattr(base_train, 'targets'):\n            train_targets = np.array(base_train.targets)\n        elif hasattr(base_train, 'labels'):\n            train_targets = np.array(base_train.labels)\n        else:\n            train_targets = np.array([base_train[i][1] for i in range(len(base_train))])\n        counts = np.bincount(train_targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí {name} (IF={imb_factor}): train={len(train)}, test={len(test)}, classes={C}\")\n    return train, test, counts, train_targets\n\n# ---------------------- MODEL & TRAINING ----------------------\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef train_one_epoch(model, loss_fn, loader, opt, epoch=None, Ew=10):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        opt.zero_grad()\n        logits = model(xb)\n        loss = loss_fn(logits, yb, epoch=epoch, Ew=Ew)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_model_with_preds(model, loader):\n    \"\"\"Returns predictions and targets for detailed analysis\"\"\"\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    return np.array(preds), np.array(tg)\n\ndef evaluate_model(preds, tg):\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\n# ---------------------- CONFIG ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nEPOCHS = 100\nPATIENCE = 10\nLR = 0.005\nBATCH = 128\nSEED = 0\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\n# ‚úÖ Focus: Include CIFAR-10-LT (IF=100) for Head/Mid/Tail\nDATASETS_TO_RUN = [\n    ('cifar10', 100),  # ‚Üê Main focus\n    # Add others if needed, but Head/Mid/Tail only computed for this\n]\n\nsummary_rows = []\nprint(\"üöÄ CDG training with Head/Mid/Tail accuracy for CIFAR-10-LT...\")\n\n# ---------------------- MAIN LOOP ----------------------\nfor ds_name, IF in DATASETS_TO_RUN:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"üìÅ {ds_name} | IF={IF}\")\n    \n    try:\n        train_ds, test_ds, counts, train_targets = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    except Exception as e:\n        print(f\"‚ùå Failed to load {ds_name}: {e}\")\n        continue\n\n    num_classes = len(counts)\n    \n    if IF > 1:\n        sampler = get_sampler_for_imbalance(train_targets, num_classes)\n        train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\n    else:\n        train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n    \n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\n    print(f\"  üîç Training with CDG (tau=0.01, k=1.0, gamma‚àà[0.5,4.0])\")\n    torch.manual_seed(SEED)\n    \n    model = get_model(num_classes).to(DEVICE)\n    loss_fn = CDG_Focal(counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0)\n    opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n    best_val_acc = 0.0\n    epochs_no_improve = 0\n    best_model_path = os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_tuned_best.pth\")\n    np.save(os.path.join(OUT, f\"{ds_name}_IF{IF}_CDG_tuned_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n    for ep in range(EPOCHS):\n        t0 = time.time()\n        train_loss = train_one_epoch(model, loss_fn, train_loader, opt, epoch=ep, Ew=10)\n        \n        # Evaluate with predictions\n        preds, tg = evaluate_model_with_preds(model, test_loader)\n        val_acc, macrof1 = evaluate_model(preds, tg)\n        \n        # Compute Head/Mid/Tail only for CIFAR-10-LT\n        head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(preds, tg, ds_name, IF)\n        \n        scheduler.step()\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_macrof1 = macrof1\n            best_head, best_mid, best_tail = head_acc, mid_acc, tail_acc\n            best_epoch = ep\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), best_model_path)\n            improved = \"‚úÖ\"\n        else:\n            epochs_no_improve += 1\n            improved = \"  \"\n        \n        # Log Head/Mid/Tail if available\n        hmt_log = \"\"\n        if head_acc is not None:\n            hmt_log = f\" | H:{head_acc:.3f} M:{mid_acc:.3f} T:{tail_acc:.3f}\"\n        \n        print(f\"    Ep {ep+1:3d}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f}{hmt_log} | {time.time()-t0:.1f}s {improved}\")\n        \n        if epochs_no_improve >= PATIENCE:\n            print(f\"    ‚èπÔ∏è Early stopping at epoch {ep+1}. Best: {best_epoch+1}\")\n            break\n\n    # Final evaluation\n    model.load_state_dict(torch.load(best_model_path))\n    final_preds, final_tg = evaluate_model_with_preds(model, test_loader)\n    final_acc, final_f1 = evaluate_model(final_preds, final_tg)\n    head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(final_preds, final_tg, ds_name, IF)\n    \n    print(f\"    üèÜ Final (best) | Acc: {final_acc:.4f} | F1: {final_f1:.4f} | H:{head_acc:.3f} M:{mid_acc:.3f} T:{tail_acc:.3f}\")\n\n    summary_rows.append({\n        'dataset': ds_name,\n        'IF': IF,\n        'loss': 'CDG_tuned',\n        'val_acc': final_acc,\n        'macro_f1': final_f1,\n        'head_acc': head_acc,\n        'mid_acc': mid_acc,\n        'tail_acc': tail_acc,\n        'best_epoch': best_epoch + 1\n    })\n\n# ---------------------- SAVE ----------------------\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"summary_cifar10_lt_headmidtail.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ Head/Mid/Tail results saved to:\\n{summary_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:27:43.668632Z","iopub.execute_input":"2025-11-13T20:27:43.669557Z","iopub.status.idle":"2025-11-13T20:31:43.580326Z","shell.execute_reply.started":"2025-11-13T20:27:43.669522Z","shell.execute_reply":"2025-11-13T20:31:43.579338Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ CDG training with Head/Mid/Tail accuracy for CIFAR-10-LT...\n\n============================================================\nüìÅ cifar10 | IF=100\n  ‚Üí cifar10 (IF=100): train=12408, test=10000, classes=10\n  üîç Training with CDG (tau=0.01, k=1.0, gamma‚àà[0.5,4.0])\n    Ep   1/100 | Loss: 2.2315 | Acc: 0.2588 | F1: 0.2502 | H:0.258 M:0.193 T:0.309 | 7.7s ‚úÖ\n    Ep   2/100 | Loss: 1.8858 | Acc: 0.3062 | F1: 0.3011 | H:0.412 M:0.208 T:0.300 | 7.6s ‚úÖ\n    Ep   3/100 | Loss: 1.6772 | Acc: 0.3528 | F1: 0.3492 | H:0.399 M:0.288 T:0.367 | 7.6s ‚úÖ\n    Ep   4/100 | Loss: 1.4726 | Acc: 0.3735 | F1: 0.3658 | H:0.431 M:0.294 T:0.390 | 7.7s ‚úÖ\n    Ep   5/100 | Loss: 1.2771 | Acc: 0.3930 | F1: 0.3891 | H:0.441 M:0.313 T:0.417 | 7.6s ‚úÖ\n    Ep   6/100 | Loss: 1.1200 | Acc: 0.4046 | F1: 0.3989 | H:0.518 M:0.322 T:0.382 | 7.8s ‚úÖ\n    Ep   7/100 | Loss: 0.9828 | Acc: 0.3967 | F1: 0.3860 | H:0.564 M:0.275 T:0.363 | 7.6s   \n    Ep   8/100 | Loss: 0.8632 | Acc: 0.4110 | F1: 0.4031 | H:0.557 M:0.315 T:0.374 | 7.6s ‚úÖ\n    Ep   9/100 | Loss: 0.7654 | Acc: 0.4092 | F1: 0.3980 | H:0.572 M:0.339 T:0.340 | 7.7s   \n    Ep  10/100 | Loss: 0.7144 | Acc: 0.4166 | F1: 0.4053 | H:0.590 M:0.334 T:0.348 | 7.7s ‚úÖ\n    Ep  13/100 | Loss: 0.5967 | Acc: 0.4121 | F1: 0.3984 | H:0.627 M:0.346 T:0.300 | 7.6s   \n    Ep  14/100 | Loss: 0.5847 | Acc: 0.4129 | F1: 0.3931 | H:0.600 M:0.371 T:0.304 | 7.5s   \n    Ep  15/100 | Loss: 0.5542 | Acc: 0.4159 | F1: 0.3976 | H:0.646 M:0.342 T:0.299 | 7.7s   \n    Ep  16/100 | Loss: 0.5133 | Acc: 0.4098 | F1: 0.3912 | H:0.614 M:0.402 T:0.262 | 7.5s   \n    Ep  17/100 | Loss: 0.5119 | Acc: 0.4179 | F1: 0.3982 | H:0.648 M:0.370 T:0.281 | 7.8s ‚úÖ\n    Ep  18/100 | Loss: 0.5006 | Acc: 0.4225 | F1: 0.4024 | H:0.662 M:0.390 T:0.268 | 7.7s ‚úÖ\n    Ep  19/100 | Loss: 0.4793 | Acc: 0.4306 | F1: 0.4105 | H:0.624 M:0.393 T:0.314 | 7.7s ‚úÖ\n    Ep  20/100 | Loss: 0.4512 | Acc: 0.4340 | F1: 0.4155 | H:0.650 M:0.389 T:0.306 | 7.7s ‚úÖ\n    Ep  21/100 | Loss: 0.4424 | Acc: 0.4343 | F1: 0.4219 | H:0.653 M:0.387 T:0.305 | 7.8s ‚úÖ\n    Ep  22/100 | Loss: 0.4262 | Acc: 0.4237 | F1: 0.4078 | H:0.641 M:0.373 T:0.299 | 7.5s   \n    Ep  23/100 | Loss: 0.4142 | Acc: 0.4148 | F1: 0.3905 | H:0.684 M:0.388 T:0.233 | 7.6s   \n    Ep  24/100 | Loss: 0.4094 | Acc: 0.4239 | F1: 0.4019 | H:0.689 M:0.380 T:0.258 | 7.6s   \n    Ep  25/100 | Loss: 0.3987 | Acc: 0.4269 | F1: 0.4048 | H:0.660 M:0.398 T:0.274 | 7.6s   \n    Ep  26/100 | Loss: 0.3741 | Acc: 0.4252 | F1: 0.4049 | H:0.685 M:0.377 T:0.267 | 7.6s   \n    Ep  27/100 | Loss: 0.3828 | Acc: 0.4204 | F1: 0.3970 | H:0.665 M:0.412 T:0.243 | 7.6s   \n    Ep  28/100 | Loss: 0.3589 | Acc: 0.4241 | F1: 0.3999 | H:0.706 M:0.359 T:0.262 | 7.5s   \n    Ep  29/100 | Loss: 0.3552 | Acc: 0.4227 | F1: 0.3978 | H:0.696 M:0.382 T:0.248 | 7.5s   \n    Ep  30/100 | Loss: 0.3360 | Acc: 0.4267 | F1: 0.4052 | H:0.675 M:0.429 T:0.239 | 7.5s   \n    Ep  31/100 | Loss: 0.3371 | Acc: 0.4237 | F1: 0.4029 | H:0.675 M:0.403 T:0.251 | 7.5s   \n    ‚èπÔ∏è Early stopping at epoch 31. Best: 21\n    üèÜ Final (best) | Acc: 0.4343 | F1: 0.4219 | H:0.653 M:0.387 T:0.305\n\n‚úÖ Head/Mid/Tail results saved to:\n/kaggle/working/loss_eval_results/summary_cifar10_lt_headmidtail.csv\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ‚úÖ All losses on CIFAR-10-LT with Head/Mid/Tail accuracy\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ---------------------- LOSS FUNCTIONS ----------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights.to(logits.device)[y]\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype, device=p.device))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=10):\n        log_p = F.log_softmax(logits, dim=1)\n        log_pt = log_p.gather(1, y[:, None]).squeeze()\n        pt = torch.exp(log_pt).clamp(min=self.eps, max=1.0 - self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        loss = - ((1.0 - pt) ** g) * log_pt\n        return loss.mean()\n\n# ---------------------- HEAD/MID/TAIL ACCURACY ----------------------\ndef compute_head_mid_tail_acc(preds, targets):\n    \"\"\"\n    CIFAR-10-LT (IF=100) standard split:\n    - Head: classes 0,1,2 (most frequent)\n    - Mid:  classes 3,4,5\n    - Tail: classes 6,7,8,9 (rarest)\n    \"\"\"\n    preds, targets = np.array(preds), np.array(targets)\n    \n    head_classes = [0, 1, 2]\n    mid_classes = [3, 4, 5]\n    tail_classes = [6, 7, 8, 9]\n    \n    def acc_for_classes(classes):\n        mask = np.isin(targets, classes)\n        if mask.sum() == 0:\n            return np.nan\n        return (preds[mask] == targets[mask]).mean()\n    \n    return (\n        acc_for_classes(head_classes),\n        acc_for_classes(mid_classes),\n        acc_for_classes(tail_classes)\n    )\n\n# ---------------------- DATASET PREP ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\ndef get_sampler_for_imbalance(targets, num_classes):\n    counts = np.bincount(targets, minlength=num_classes)\n    weight_per_class = 1.0 / (counts + 1e-6)\n    weights = weight_per_class[targets]\n    return WeightedRandomSampler(weights, len(weights), replacement=True)\n\ndef prepare_cifar10_lt(root='./data', imb_factor=100, seed=0):\n    mean = (0.4914, 0.4822, 0.4465)\n    std = (0.2023, 0.1994, 0.2010)\n    train_tr = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    test_tr = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    \n    base_train = datasets.CIFAR10(root, train=True, download=True, transform=train_tr)\n    test = datasets.CIFAR10(root, train=False, download=True, transform=test_tr)\n    \n    targets = np.array(base_train.targets)\n    indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n    train = Subset(base_train, indices)\n    train_targets = np.array([base_train.targets[i] for i in indices])\n    counts = cls_counts\n    \n    print(f\"  ‚Üí CIFAR-10-LT (IF={imb_factor}): train={len(train)}, test={len(test)}, classes=10\")\n    return train, test, counts, train_targets\n\n# ---------------------- MODEL & TRAINING ----------------------\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef train_one_epoch(model, loss_fn, loader, opt, epoch=None, loss_name=None):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        opt.zero_grad()\n        logits = model(xb)\n        if loss_name == 'CDG':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=10)\n        else:\n            loss = loss_fn(logits, yb)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_model_with_preds(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    return np.array(preds), np.array(tg)\n\ndef evaluate_model(preds, tg):\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\n# ---------------------- CONFIG ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG_tuned': lambda counts: CDG_Focal(counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0)\n}\n\nEPOCHS = 100\nPATIENCE = 10\nLR = 0.005  # Stable for all\nBATCH = 128\nSEED = 0\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\nsummary_rows = []\nprint(\"üöÄ Evaluating ALL losses on CIFAR-10-LT (IF=100) with Head/Mid/Tail accuracy...\")\n\n# ---------------------- MAIN LOOP ----------------------\nds_name, IF = 'cifar10', 100\nprint(\"\\n\" + \"=\"*60)\nprint(f\"üìÅ {ds_name} | IF={IF}\")\n\ntrain_ds, test_ds, counts, train_targets = prepare_cifar10_lt(root='./data', imb_factor=IF, seed=SEED)\nnum_classes = len(counts)\n\n# Use class-balanced sampler for fair comparison\nsampler = get_sampler_for_imbalance(train_targets, num_classes)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\nfor loss_name, loss_ctor in LOSSES.items():\n    print(f\"\\n  üîç Training with {loss_name}\")\n    torch.manual_seed(SEED)\n    \n    model = get_model(num_classes).to(DEVICE)\n    loss_fn = loss_ctor(counts)\n    opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n    best_val_acc = 0.0\n    epochs_no_improve = 0\n    best_model_path = os.path.join(OUT, f\"cifar10_IF100_{loss_name}_best.pth\")\n\n    # Save CDG gamma if applicable\n    if 'CDG' in loss_name and hasattr(loss_fn, 'gamma_per_class'):\n        np.save(os.path.join(OUT, f\"cifar10_IF100_{loss_name}_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n    for ep in range(EPOCHS):\n        t0 = time.time()\n        train_loss = train_one_epoch(model, loss_fn, train_loader, opt, epoch=ep, loss_name=loss_name)\n        \n        preds, tg = evaluate_model_with_preds(model, test_loader)\n        val_acc, macrof1 = evaluate_model(preds, tg)\n        head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(preds, tg)\n        \n        scheduler.step()\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_macrof1 = macrof1\n            best_head, best_mid, best_tail = head_acc, mid_acc, tail_acc\n            best_epoch = ep\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), best_model_path)\n            improved = \"‚úÖ\"\n        else:\n            epochs_no_improve += 1\n            improved = \"  \"\n        \n        print(f\"    Ep {ep+1:3d}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f} | \"\n              f\"H:{head_acc:.3f} M:{mid_acc:.3f} T:{tail_acc:.3f} | {time.time()-t0:.1f}s {improved}\")\n        \n        if epochs_no_improve >= PATIENCE:\n            print(f\"    ‚èπÔ∏è Early stopping at epoch {ep+1}. Best: {best_epoch+1}\")\n            break\n\n    # Final evaluation\n    model.load_state_dict(torch.load(best_model_path))\n    final_preds, final_tg = evaluate_model_with_preds(model, test_loader)\n    final_acc, final_f1 = evaluate_model(final_preds, final_tg)\n    head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(final_preds, final_tg)\n    \n    print(f\"    üèÜ Final (best) | Acc: {final_acc:.4f} | F1: {final_f1:.4f} | \"\n          f\"H:{head_acc:.3f} M:{mid_acc:.3f} T:{tail_acc:.3f}\")\n\n    summary_rows.append({\n        'loss': loss_name,\n        'val_acc': final_acc,\n        'macro_f1': final_f1,\n        'head_acc': head_acc,\n        'mid_acc': mid_acc,\n        'tail_acc': tail_acc,\n        'best_epoch': best_epoch + 1\n    })\n\n# ---------------------- SAVE RESULTS ----------------------\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"cifar10_lt_all_losses_headmidtail.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ All losses evaluated! Results saved to:\\n{summary_path}\")\n\n# Display summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: CIFAR-10-LT (IF=100) Head/Mid/Tail Accuracies\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False, float_format=\"%.4f\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:45:32.862105Z","iopub.execute_input":"2025-11-13T20:45:32.862988Z","iopub.status.idle":"2025-11-13T20:52:16.436586Z","shell.execute_reply.started":"2025-11-13T20:45:32.862949Z","shell.execute_reply":"2025-11-13T20:52:16.435406Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ Evaluating ALL losses on CIFAR-10-LT (IF=100) with Head/Mid/Tail accuracy...\n\n============================================================\nüìÅ cifar10 | IF=100\n  ‚Üí CIFAR-10-LT (IF=100): train=12408, test=10000, classes=10\n\n  üîç Training with CE\n    Ep   1/100 | Loss: 2.2343 | Acc: 0.2560 | F1: 0.2465 | H:0.270 M:0.187 T:0.297 | 7.7s ‚úÖ\n    Ep   2/100 | Loss: 1.9296 | Acc: 0.3075 | F1: 0.3030 | H:0.401 M:0.212 T:0.309 | 7.6s ‚úÖ\n    Ep   3/100 | Loss: 1.7913 | Acc: 0.3400 | F1: 0.3365 | H:0.396 M:0.274 T:0.348 | 7.7s ‚úÖ\n    Ep   4/100 | Loss: 1.6907 | Acc: 0.3682 | F1: 0.3600 | H:0.407 M:0.285 T:0.401 | 7.6s ‚úÖ\n    Ep   5/100 | Loss: 1.5869 | Acc: 0.3792 | F1: 0.3744 | H:0.406 M:0.296 T:0.422 | 7.6s ‚úÖ\n    Ep   6/100 | Loss: 1.5057 | Acc: 0.3952 | F1: 0.3913 | H:0.512 M:0.302 T:0.378 | 7.5s ‚úÖ\n    Ep   7/100 | Loss: 1.4343 | Acc: 0.4052 | F1: 0.3965 | H:0.524 M:0.290 T:0.402 | 7.5s ‚úÖ\n    Ep   8/100 | Loss: 1.3613 | Acc: 0.4195 | F1: 0.4101 | H:0.521 M:0.306 T:0.429 | 7.6s ‚úÖ\n    Ep   9/100 | Loss: 1.2917 | Acc: 0.4208 | F1: 0.4167 | H:0.534 M:0.343 T:0.394 | 7.6s ‚úÖ\n    Ep  10/100 | Loss: 1.2451 | Acc: 0.4249 | F1: 0.4183 | H:0.547 M:0.318 T:0.413 | 7.6s ‚úÖ\n    Ep  11/100 | Loss: 1.1840 | Acc: 0.4358 | F1: 0.4286 | H:0.546 M:0.417 T:0.367 | 7.6s ‚úÖ\n    Ep  12/100 | Loss: 1.1410 | Acc: 0.4467 | F1: 0.4377 | H:0.593 M:0.359 T:0.402 | 7.7s ‚úÖ\n    Ep  13/100 | Loss: 1.0794 | Acc: 0.4415 | F1: 0.4321 | H:0.604 M:0.370 T:0.373 | 7.4s   \n    Ep  14/100 | Loss: 1.0572 | Acc: 0.4485 | F1: 0.4371 | H:0.590 M:0.377 T:0.396 | 7.5s ‚úÖ\n    Ep  15/100 | Loss: 1.0201 | Acc: 0.4426 | F1: 0.4301 | H:0.610 M:0.371 T:0.371 | 7.5s   \n    Ep  16/100 | Loss: 0.9663 | Acc: 0.4500 | F1: 0.4382 | H:0.609 M:0.420 T:0.353 | 7.7s ‚úÖ\n    Ep  17/100 | Loss: 0.9674 | Acc: 0.4425 | F1: 0.4263 | H:0.621 M:0.391 T:0.348 | 7.4s   \n    Ep  18/100 | Loss: 0.9334 | Acc: 0.4476 | F1: 0.4370 | H:0.633 M:0.398 T:0.346 | 7.4s   \n    Ep  19/100 | Loss: 0.8904 | Acc: 0.4573 | F1: 0.4433 | H:0.605 M:0.409 T:0.382 | 7.7s ‚úÖ\n    Ep  20/100 | Loss: 0.8670 | Acc: 0.4579 | F1: 0.4420 | H:0.653 M:0.396 T:0.358 | 7.6s ‚úÖ\n    Ep  21/100 | Loss: 0.8557 | Acc: 0.4526 | F1: 0.4402 | H:0.663 M:0.387 T:0.344 | 7.5s   \n    Ep  22/100 | Loss: 0.8299 | Acc: 0.4549 | F1: 0.4425 | H:0.634 M:0.397 T:0.364 | 7.5s   \n    Ep  23/100 | Loss: 0.8024 | Acc: 0.4480 | F1: 0.4328 | H:0.665 M:0.391 T:0.328 | 7.4s   \n    Ep  24/100 | Loss: 0.7894 | Acc: 0.4584 | F1: 0.4464 | H:0.667 M:0.413 T:0.336 | 7.6s ‚úÖ\n    Ep  25/100 | Loss: 0.7921 | Acc: 0.4520 | F1: 0.4334 | H:0.671 M:0.404 T:0.324 | 7.4s   \n    Ep  26/100 | Loss: 0.7379 | Acc: 0.4581 | F1: 0.4410 | H:0.664 M:0.430 T:0.325 | 7.4s   \n    Ep  27/100 | Loss: 0.7467 | Acc: 0.4603 | F1: 0.4460 | H:0.663 M:0.425 T:0.335 | 7.6s ‚úÖ\n    Ep  28/100 | Loss: 0.7198 | Acc: 0.4660 | F1: 0.4509 | H:0.648 M:0.431 T:0.356 | 7.6s ‚úÖ\n    Ep  29/100 | Loss: 0.7121 | Acc: 0.4625 | F1: 0.4441 | H:0.702 M:0.411 T:0.321 | 7.4s   \n    Ep  30/100 | Loss: 0.6878 | Acc: 0.4572 | F1: 0.4423 | H:0.650 M:0.466 T:0.306 | 7.4s   \n    Ep  31/100 | Loss: 0.6793 | Acc: 0.4557 | F1: 0.4352 | H:0.689 M:0.429 T:0.300 | 7.6s   \n    Ep  32/100 | Loss: 0.6486 | Acc: 0.4570 | F1: 0.4375 | H:0.696 M:0.426 T:0.301 | 7.4s   \n    Ep  33/100 | Loss: 0.6476 | Acc: 0.4542 | F1: 0.4317 | H:0.684 M:0.458 T:0.279 | 7.4s   \n    Ep  34/100 | Loss: 0.6312 | Acc: 0.4646 | F1: 0.4479 | H:0.694 M:0.451 T:0.303 | 7.5s   \n    Ep  35/100 | Loss: 0.5996 | Acc: 0.4583 | F1: 0.4369 | H:0.695 M:0.435 T:0.298 | 7.5s   \n    Ep  36/100 | Loss: 0.6226 | Acc: 0.4472 | F1: 0.4216 | H:0.702 M:0.440 T:0.262 | 7.6s   \n    Ep  37/100 | Loss: 0.6036 | Acc: 0.4568 | F1: 0.4341 | H:0.699 M:0.455 T:0.276 | 7.4s   \n    Ep  38/100 | Loss: 0.5772 | Acc: 0.4610 | F1: 0.4414 | H:0.711 M:0.418 T:0.306 | 7.5s   \n    ‚èπÔ∏è Early stopping at epoch 38. Best: 28\n    üèÜ Final (best) | Acc: 0.4660 | F1: 0.4509 | H:0.648 M:0.431 T:0.356\n\n  üîç Training with Focal_g1\n    Ep   1/100 | Loss: 1.9951 | Acc: 0.2366 | F1: 0.2293 | H:0.240 M:0.183 T:0.275 | 7.6s ‚úÖ\n    Ep   2/100 | Loss: 1.6680 | Acc: 0.3016 | F1: 0.2968 | H:0.392 M:0.199 T:0.311 | 7.6s ‚úÖ\n    Ep   3/100 | Loss: 1.5105 | Acc: 0.3486 | F1: 0.3462 | H:0.398 M:0.266 T:0.373 | 7.7s ‚úÖ\n    Ep   4/100 | Loss: 1.3978 | Acc: 0.3691 | F1: 0.3599 | H:0.405 M:0.301 T:0.393 | 7.8s ‚úÖ\n    Ep   5/100 | Loss: 1.2928 | Acc: 0.3860 | F1: 0.3829 | H:0.432 M:0.278 T:0.432 | 7.6s ‚úÖ\n    Ep   6/100 | Loss: 1.2344 | Acc: 0.4009 | F1: 0.3981 | H:0.499 M:0.311 T:0.394 | 7.6s ‚úÖ\n    Ep   7/100 | Loss: 1.1699 | Acc: 0.3922 | F1: 0.3822 | H:0.546 M:0.256 T:0.379 | 7.5s   \n    Ep   8/100 | Loss: 1.1025 | Acc: 0.4205 | F1: 0.4137 | H:0.548 M:0.310 T:0.408 | 7.6s ‚úÖ\n    Ep   9/100 | Loss: 1.0371 | Acc: 0.4158 | F1: 0.4123 | H:0.529 M:0.334 T:0.393 | 7.5s   \n    Ep  10/100 | Loss: 0.9966 | Acc: 0.4301 | F1: 0.4205 | H:0.571 M:0.317 T:0.409 | 7.6s ‚úÖ\n    Ep  11/100 | Loss: 0.9379 | Acc: 0.4319 | F1: 0.4287 | H:0.538 M:0.391 T:0.383 | 7.6s ‚úÖ\n    Ep  12/100 | Loss: 0.9007 | Acc: 0.4367 | F1: 0.4285 | H:0.595 M:0.347 T:0.386 | 7.6s ‚úÖ\n    Ep  13/100 | Loss: 0.8489 | Acc: 0.4383 | F1: 0.4289 | H:0.598 M:0.353 T:0.383 | 7.7s ‚úÖ\n    Ep  14/100 | Loss: 0.8187 | Acc: 0.4334 | F1: 0.4203 | H:0.582 M:0.392 T:0.353 | 7.5s   \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_327/133382217.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_with_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacrof1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mhead_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_head_mid_tail_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_327/133382217.py\u001b[0m in \u001b[0;36mevaluate_model_with_preds\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1444\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1580\u001b[0m                     \u001b[0;31m# wrong, we set a timeout and if the workers fail to join,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m                     \u001b[0;31m# they are killed in the `finally` block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_join_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"# ‚úÖ FULL DATASETS + All Losses + Head/Mid/Tail for CIFAR-10-LT only\nimport os, sys, time, math, random\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import f1_score\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ---------------------- LOSS FUNCTIONS ----------------------\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        return (-((1-pt)**self.gamma) * torch.log(pt)).mean()\n\ndef effective_num_weights(counts, beta):\n    counts = np.array(counts, dtype=np.float64)\n    eff = (1.0 - np.power(beta, counts)) / (1.0 - beta + 1e-12)\n    w = 1.0 / (eff + 1e-12)\n    w = w / w.sum() * len(w)\n    return w.astype(np.float32)\n\nclass CB_Focal(nn.Module):\n    def __init__(self, counts, beta=0.999, gamma=1.0, eps=1e-7):\n        super().__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.register_buffer('weights', torch.tensor(effective_num_weights(counts, beta), dtype=torch.float32))\n    def forward(self, logits, y):\n        p = F.softmax(logits, dim=1)\n        pt = p.gather(1, y[:,None]).squeeze().clamp(min=self.eps)\n        w = self.weights.to(logits.device)[y]\n        return (- w * ((1-pt)**self.gamma) * torch.log(pt)).mean()\n\nclass CDG_Focal(nn.Module):\n    def __init__(self, counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n        counts = torch.tensor(counts, dtype=torch.float32)\n        p = (counts / counts.sum()).clamp(min=1e-12)\n        log_tau_inv = torch.log(torch.tensor(1.0 / tau, dtype=p.dtype, device=p.device))\n        branch2 = log_tau_inv + k * (p - tau)\n        raw = torch.where(p > tau, torch.log(1.0 / p), branch2)\n        gamma = raw.clamp(min=gamma_min, max=gamma_max)\n        self.register_buffer('gamma_per_class', gamma)\n    \n    @staticmethod\n    def cosine_warmup_weight(epoch, Ew):\n        if Ew <= 0: return 1.0\n        e = float(epoch)\n        if e <= 0.0: return 0.0\n        if e >= Ew: return 1.0\n        return 0.5 * (1.0 - math.cos(math.pi * e / Ew))\n    \n    def forward(self, logits, y, epoch=None, Ew=10):\n        log_p = F.log_softmax(logits, dim=1)\n        log_pt = log_p.gather(1, y[:, None]).squeeze()\n        pt = torch.exp(log_pt).clamp(min=self.eps, max=1.0 - self.eps)\n        g = self.gamma_per_class.to(logits.device)[y]\n        if epoch is not None:\n            w = self.cosine_warmup_weight(epoch, Ew)\n            g = g * w\n        loss = - ((1.0 - pt) ** g) * log_pt\n        return loss.mean()\n\n# ---------------------- HEAD/MID/TAIL (CIFAR-10-LT ONLY) ----------------------\ndef compute_head_mid_tail_acc(preds, targets, dataset_name, imb_factor):\n    \"\"\"\n    Only compute for CIFAR-10-LT (IF=100)\n    Head: 0-2, Mid: 3-5, Tail: 6-9\n    \"\"\"\n    if dataset_name.lower() == 'cifar10' and imb_factor == 100:\n        preds, targets = np.array(preds), np.array(targets)\n        head_classes = [0, 1, 2]\n        mid_classes = [3, 4, 5]\n        tail_classes = [6, 7, 8, 9]\n        \n        def acc_for_classes(classes):\n            mask = np.isin(targets, classes)\n            return (preds[mask] == targets[mask]).mean() if mask.sum() > 0 else np.nan\n        \n        return (\n            acc_for_classes(head_classes),\n            acc_for_classes(mid_classes),\n            acc_for_classes(tail_classes)\n        )\n    return None, None, None\n\n# ---------------------- DATASET PREP (ALL) ----------------------\ndef make_lt_indices(targets, imb_factor, seed=0):\n    np.random.seed(seed)\n    targets = np.array(targets)\n    C = int(targets.max()) + 1\n    cls_counts = np.bincount(targets, minlength=C)\n    N_max = cls_counts.max()\n    r = 1.0 / float(imb_factor)\n    cls_num = [int(max(1, round(N_max * (r ** (i / (C - 1.0)))))) for i in range(C)]\n    indices = []\n    for c in range(C):\n        idxs = np.where(targets == c)[0]\n        chosen = np.random.choice(idxs, cls_num[c], replace=False)\n        indices.extend(chosen.tolist())\n    random.shuffle(indices)\n    return indices, cls_num\n\ndef get_sampler_for_imbalance(targets, num_classes):\n    counts = np.bincount(targets, minlength=num_classes)\n    weight_per_class = 1.0 / (counts + 1e-6)\n    weights = weight_per_class[targets]\n    return WeightedRandomSampler(weights, len(weights), replacement=True)\n\ndef prepare_dataset(name, root='./data', imb_factor=1, seed=0):\n    name_l = name.lower()\n    if name_l in ('cifar10', 'cifar100'):\n        mean = (0.4914, 0.4822, 0.4465) if name_l == 'cifar10' else (0.5071, 0.4867, 0.4408)\n        std = (0.2023, 0.1994, 0.2010) if name_l == 'cifar10' else (0.2675, 0.2565, 0.2761)\n        train_tr = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        test_tr = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n    else:\n        train_tr = transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            lambda x: x.repeat(3,1,1) if x.size(0) == 1 else x\n        ])\n        test_tr = train_tr\n\n    if name_l == 'mnist':\n        base_train = datasets.MNIST(root, train=True, download=True, transform=train_tr)\n        test = datasets.MNIST(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'fashionmnist':\n        base_train = datasets.FashionMNIST(root, train=True, download=True, transform=train_tr)\n        test = datasets.FashionMNIST(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'svhn':\n        base_train = datasets.SVHN(root, split='train', download=True, transform=train_tr)\n        test = datasets.SVHN(root, split='test', download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'cifar10':\n        base_train = datasets.CIFAR10(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR10(root, train=False, download=True, transform=test_tr)\n        C = 10\n    elif name_l == 'cifar100':\n        base_train = datasets.CIFAR100(root, train=True, download=True, transform=train_tr)\n        test = datasets.CIFAR100(root, train=False, download=True, transform=test_tr)\n        C = 100\n    else:\n        raise ValueError(f\"Unsupported: {name}\")\n\n    if name_l in ('cifar10','cifar100') and imb_factor > 1:\n        targets = np.array(base_train.targets)\n        indices, cls_counts = make_lt_indices(targets, imb_factor, seed)\n        train = Subset(base_train, indices)\n        train_targets = np.array([base_train.targets[i] for i in indices])\n        counts = cls_counts\n    else:\n        train = base_train\n        if hasattr(base_train, 'targets'):\n            train_targets = np.array(base_train.targets)\n        elif hasattr(base_train, 'labels'):\n            train_targets = np.array(base_train.labels)\n        else:\n            train_targets = np.array([base_train[i][1] for i in range(len(base_train))])\n        counts = np.bincount(train_targets, minlength=C).tolist()\n\n    print(f\"  ‚Üí {name} (IF={imb_factor}): train={len(train)}, test={len(test)}, classes={C}\")\n    return train, test, counts, train_targets\n\n# ---------------------- MODEL & TRAINING ----------------------\ndef get_model(num_classes):\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\ndef train_one_epoch(model, loss_fn, loader, opt, epoch=None, loss_name=None):\n    model.train()\n    total_loss = 0.0\n    for xb, yb in loader:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        opt.zero_grad()\n        logits = model(xb)\n        if loss_name == 'CDG':\n            loss = loss_fn(logits, yb, epoch=epoch, Ew=10)\n        else:\n            loss = loss_fn(logits, yb)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        opt.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_model_with_preds(model, loader):\n    model.eval()\n    preds, tg = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE)\n            out = model(xb)\n            preds.extend(out.argmax(1).cpu().numpy())\n            tg.extend(yb.numpy())\n    return np.array(preds), np.array(tg)\n\ndef evaluate_model(preds, tg):\n    acc = (preds == tg).mean()\n    macro_f1 = f1_score(tg, preds, average='macro', zero_division=0)\n    return acc, macro_f1\n\n# ---------------------- CONFIG ----------------------\nOUT = \"/kaggle/working/loss_eval_results\"\nos.makedirs(OUT, exist_ok=True)\n\nLOSSES = {\n    'CE': lambda counts: nn.CrossEntropyLoss(),\n    'Focal_g1': lambda counts: FocalLoss(gamma=1.0),\n    'CBF_b0.999_g1': lambda counts: CB_Focal(counts, beta=0.999, gamma=1.0),\n    'CDG_tuned': lambda counts: CDG_Focal(counts, tau=0.01, k=1.0, gamma_min=0.5, gamma_max=4.0)\n}\n\nEPOCHS = 100\nPATIENCE = 10\nLR = 0.005\nBATCH = 128\nSEED = 0\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\n# ‚úÖ FULL DATASET LIST\nDATASETS_TO_RUN = [\n    ('mnist', 1),\n    ('mnist', 100),\n    ('fashionmnist', 1),\n    ('fashionmnist', 100),\n    ('svhn', 1),\n    ('svhn', 100),\n    ('cifar10', 1),\n    ('cifar10', 100),   # ‚Üê Head/Mid/Tail computed here\n    ('cifar100', 1),\n    ('cifar100', 100)\n]\n\nsummary_rows = []\nprint(\"üöÄ Full evaluation: all datasets + all losses + H/M/T for CIFAR-10-LT...\")\n\n# ---------------------- MAIN LOOP ----------------------\nfor ds_name, IF in DATASETS_TO_RUN:\n    print(\"\\n\" + \"=\"*60)\n    print(f\"üìÅ {ds_name} | IF={IF}\")\n    \n    try:\n        train_ds, test_ds, counts, train_targets = prepare_dataset(ds_name, root='./data', imb_factor=IF, seed=SEED)\n    except Exception as e:\n        print(f\"‚ùå Failed to load {ds_name}: {e}\")\n        continue\n\n    num_classes = len(counts)\n    \n    # Use class-balanced sampler only for imbalanced cases (IF > 1)\n    if IF > 1:\n        sampler = get_sampler_for_imbalance(train_targets, num_classes)\n        train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\n    else:\n        train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n    \n    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\n    for loss_name, loss_ctor in LOSSES.items():\n        print(f\"\\n  üîç {loss_name}\")\n        torch.manual_seed(SEED)\n        \n        model = get_model(num_classes).to(DEVICE)\n        loss_fn = loss_ctor(counts)\n        opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\n        best_val_acc = 0.0\n        epochs_no_improve = 0\n        best_model_path = os.path.join(OUT, f\"{ds_name}_IF{IF}_{loss_name}_best.pth\")\n\n        if 'CDG' in loss_name and hasattr(loss_fn, 'gamma_per_class'):\n            np.save(os.path.join(OUT, f\"{ds_name}_IF{IF}_{loss_name}_gamma.npy\"), loss_fn.gamma_per_class.cpu().numpy())\n\n        for ep in range(EPOCHS):\n            t0 = time.time()\n            train_loss = train_one_epoch(model, loss_fn, train_loader, opt, epoch=ep, loss_name=loss_name)\n            \n            preds, tg = evaluate_model_with_preds(model, test_loader)\n            val_acc, macrof1 = evaluate_model(preds, tg)\n            head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(preds, tg, ds_name, IF)\n            \n            scheduler.step()\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                best_macrof1 = macrof1\n                best_head, best_mid, best_tail = head_acc, mid_acc, tail_acc\n                best_epoch = ep\n                epochs_no_improve = 0\n                torch.save(model.state_dict(), best_model_path)\n                improved = \"‚úÖ\"\n            else:\n                epochs_no_improve += 1\n                improved = \"  \"\n            \n            # Log H/M/T only for CIFAR-10-LT\n            hmt_log = \"\"\n            if head_acc is not None:\n                hmt_log = f\" | H:{head_acc:.3f} M:{mid_acc:.3f} T:{tail_acc:.3f}\"\n            \n            print(f\"    Ep {ep+1:3d}/{EPOCHS} | Loss: {train_loss:.4f} | Acc: {val_acc:.4f} | F1: {macrof1:.4f}{hmt_log} | {time.time()-t0:.1f}s {improved}\")\n            \n            if epochs_no_improve >= PATIENCE:\n                print(f\"    ‚èπÔ∏è Early stopping at epoch {ep+1}. Best: {best_epoch+1}\")\n                break\n\n        # Final evaluation\n        model.load_state_dict(torch.load(best_model_path))\n        final_preds, final_tg = evaluate_model_with_preds(model, test_loader)\n        final_acc, final_f1 = evaluate_model(final_preds, final_tg)\n        head_acc, mid_acc, tail_acc = compute_head_mid_tail_acc(final_preds, final_tg, ds_name, IF)\n\n        # Build result row\n        result = {\n            'dataset': ds_name,\n            'IF': IF,\n            'loss': loss_name,\n            'val_acc': final_acc,\n            'macro_f1': final_f1,\n            'best_epoch': best_epoch + 1\n        }\n        # Add H/M/T only if available\n        if head_acc is not None:\n            result.update({\n                'head_acc': head_acc,\n                'mid_acc': mid_acc,\n                'tail_acc': tail_acc\n            })\n        \n        summary_rows.append(result)\n\n# ---------------------- SAVE ----------------------\nsummary_df = pd.DataFrame(summary_rows)\nsummary_path = os.path.join(OUT, \"full_results_with_headmidtail.csv\")\nsummary_df.to_csv(summary_path, index=False)\nprint(f\"\\n‚úÖ Full evaluation complete! Results saved to:\\n{summary_path}\")\n\n# Optional: Print only CIFAR-10-LT H/M/T summary\ncifar10_lt = summary_df[(summary_df['dataset'] == 'cifar10') & (summary_df['IF'] == 100)]\nif not cifar10_lt.empty:\n    print(\"\\n\" + \"=\"*80)\n    print(\"CIFAR-10-LT (IF=100) Head/Mid/Tail Summary\")\n    print(\"=\"*80)\n    cols = ['loss', 'val_acc', 'macro_f1', 'head_acc', 'mid_acc', 'tail_acc']\n    print(cifar10_lt[cols].to_string(index=False, float_format=\"%.4f\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T21:16:21.991678Z","iopub.execute_input":"2025-11-13T21:16:21.991936Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nüöÄ Full evaluation: all datasets + all losses + H/M/T for CIFAR-10-LT...\n\n============================================================\nüìÅ mnist | IF=1\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 484kB/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 6.53MB/s]\n","output_type":"stream"},{"name":"stdout","text":"  ‚Üí mnist (IF=1): train=60000, test=10000, classes=10\n\n  üîç CE\n    Ep   1/100 | Loss: 0.3254 | Acc: 0.9818 | F1: 0.9816 | 30.2s ‚úÖ\n    Ep   2/100 | Loss: 0.0502 | Acc: 0.9862 | F1: 0.9861 | 29.4s ‚úÖ\n    Ep   3/100 | Loss: 0.0301 | Acc: 0.9881 | F1: 0.9881 | 29.3s ‚úÖ\n    Ep   4/100 | Loss: 0.0177 | Acc: 0.9868 | F1: 0.9866 | 29.5s   \n    Ep   5/100 | Loss: 0.0123 | Acc: 0.9912 | F1: 0.9911 | 29.4s ‚úÖ\n    Ep   6/100 | Loss: 0.0072 | Acc: 0.9911 | F1: 0.9910 | 29.2s   \n    Ep   7/100 | Loss: 0.0043 | Acc: 0.9920 | F1: 0.9919 | 29.3s ‚úÖ\n    Ep   8/100 | Loss: 0.0029 | Acc: 0.9910 | F1: 0.9909 | 29.0s   \n    Ep   9/100 | Loss: 0.0023 | Acc: 0.9917 | F1: 0.9917 | 29.2s   \n    Ep  10/100 | Loss: 0.0013 | Acc: 0.9925 | F1: 0.9924 | 29.3s ‚úÖ\n    Ep  11/100 | Loss: 0.0009 | Acc: 0.9927 | F1: 0.9927 | 29.3s ‚úÖ\n    Ep  12/100 | Loss: 0.0006 | Acc: 0.9918 | F1: 0.9917 | 29.3s   \n    Ep  13/100 | Loss: 0.0005 | Acc: 0.9925 | F1: 0.9924 | 29.1s   \n    Ep  14/100 | Loss: 0.0004 | Acc: 0.9926 | F1: 0.9925 | 29.3s   \n    Ep  15/100 | Loss: 0.0004 | Acc: 0.9931 | F1: 0.9930 | 29.2s ‚úÖ\n    Ep  16/100 | Loss: 0.0004 | Acc: 0.9928 | F1: 0.9927 | 29.2s   \n    Ep  17/100 | Loss: 0.0002 | Acc: 0.9929 | F1: 0.9929 | 29.3s   \n    Ep  18/100 | Loss: 0.0002 | Acc: 0.9928 | F1: 0.9928 | 29.1s   \n    Ep  19/100 | Loss: 0.0003 | Acc: 0.9933 | F1: 0.9932 | 29.1s ‚úÖ\n    Ep  20/100 | Loss: 0.0002 | Acc: 0.9933 | F1: 0.9933 | 29.3s   \n    Ep  21/100 | Loss: 0.0002 | Acc: 0.9926 | F1: 0.9925 | 29.2s   \n    Ep  22/100 | Loss: 0.0002 | Acc: 0.9935 | F1: 0.9935 | 29.3s ‚úÖ\n    Ep  23/100 | Loss: 0.0002 | Acc: 0.9935 | F1: 0.9934 | 29.2s   \n    Ep  24/100 | Loss: 0.0002 | Acc: 0.9930 | F1: 0.9930 | 29.2s   \n    Ep  25/100 | Loss: 0.0002 | Acc: 0.9932 | F1: 0.9931 | 29.2s   \n    Ep  26/100 | Loss: 0.0002 | Acc: 0.9933 | F1: 0.9933 | 29.2s   \n    Ep  27/100 | Loss: 0.0002 | Acc: 0.9928 | F1: 0.9927 | 29.3s   \n    Ep  28/100 | Loss: 0.0002 | Acc: 0.9932 | F1: 0.9932 | 29.1s   \n    Ep  29/100 | Loss: 0.0002 | Acc: 0.9934 | F1: 0.9934 | 29.1s   \n    Ep  30/100 | Loss: 0.0002 | Acc: 0.9935 | F1: 0.9935 | 29.3s   \n    Ep  31/100 | Loss: 0.0002 | Acc: 0.9934 | F1: 0.9934 | 29.2s   \n    Ep  32/100 | Loss: 0.0002 | Acc: 0.9933 | F1: 0.9933 | 29.1s   \n    ‚èπÔ∏è Early stopping at epoch 32. Best: 22\n\n  üîç Focal_g1\n    Ep   1/100 | Loss: 0.2558 | Acc: 0.9784 | F1: 0.9783 | 29.3s ‚úÖ\n    Ep   2/100 | Loss: 0.0347 | Acc: 0.9837 | F1: 0.9836 | 29.4s ‚úÖ\n    Ep   3/100 | Loss: 0.0194 | Acc: 0.9866 | F1: 0.9866 | 29.4s ‚úÖ\n    Ep   4/100 | Loss: 0.0119 | Acc: 0.9876 | F1: 0.9875 | 29.3s ‚úÖ\n    Ep   5/100 | Loss: 0.0068 | Acc: 0.9885 | F1: 0.9884 | 29.2s ‚úÖ\n    Ep   6/100 | Loss: 0.0041 | Acc: 0.9897 | F1: 0.9896 | 29.3s ‚úÖ\n    Ep   7/100 | Loss: 0.0026 | Acc: 0.9894 | F1: 0.9893 | 29.3s   \n    Ep   8/100 | Loss: 0.0019 | Acc: 0.9900 | F1: 0.9900 | 29.2s ‚úÖ\n    Ep   9/100 | Loss: 0.0011 | Acc: 0.9917 | F1: 0.9917 | 29.1s ‚úÖ\n    Ep  10/100 | Loss: 0.0009 | Acc: 0.9907 | F1: 0.9906 | 29.2s   \n    Ep  11/100 | Loss: 0.0006 | Acc: 0.9915 | F1: 0.9914 | 29.2s   \n    Ep  12/100 | Loss: 0.0003 | Acc: 0.9906 | F1: 0.9905 | 29.0s   \n    Ep  13/100 | Loss: 0.0002 | Acc: 0.9913 | F1: 0.9912 | 29.1s   \n    Ep  14/100 | Loss: 0.0002 | Acc: 0.9917 | F1: 0.9916 | 29.1s   \n    Ep  15/100 | Loss: 0.0002 | Acc: 0.9913 | F1: 0.9913 | 29.1s   \n    Ep  16/100 | Loss: 0.0002 | Acc: 0.9917 | F1: 0.9916 | 29.0s   \n    Ep  17/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9916 | 29.2s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.3s ‚úÖ\n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 29.1s   \n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9915 | F1: 0.9914 | 29.2s   \n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9922 | 29.2s ‚úÖ\n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9918 | 29.4s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9925 | F1: 0.9925 | 29.4s ‚úÖ\n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9922 | 29.1s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9921 | F1: 0.9920 | 29.1s   \n    Ep  26/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9917 | 29.2s   \n    Ep  27/100 | Loss: 0.0001 | Acc: 0.9924 | F1: 0.9923 | 29.1s   \n    Ep  28/100 | Loss: 0.0001 | Acc: 0.9926 | F1: 0.9925 | 29.3s ‚úÖ\n    Ep  29/100 | Loss: 0.0001 | Acc: 0.9927 | F1: 0.9926 | 29.1s ‚úÖ\n    Ep  30/100 | Loss: 0.0001 | Acc: 0.9926 | F1: 0.9925 | 29.2s   \n    Ep  31/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 29.1s   \n    Ep  32/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.0s   \n    Ep  33/100 | Loss: 0.0001 | Acc: 0.9924 | F1: 0.9923 | 29.1s   \n    Ep  35/100 | Loss: 0.0001 | Acc: 0.9923 | F1: 0.9922 | 29.4s   \n    Ep  36/100 | Loss: 0.0001 | Acc: 0.9921 | F1: 0.9920 | 29.1s   \n    Ep  37/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9921 | 29.2s   \n    Ep  38/100 | Loss: 0.0001 | Acc: 0.9923 | F1: 0.9923 | 29.2s   \n    Ep  39/100 | Loss: 0.0001 | Acc: 0.9923 | F1: 0.9922 | 29.3s   \n    ‚èπÔ∏è Early stopping at epoch 39. Best: 29\n\n  üîç CBF_b0.999_g1\n    Ep   1/100 | Loss: 0.2520 | Acc: 0.9774 | F1: 0.9773 | 29.9s ‚úÖ\n    Ep   2/100 | Loss: 0.0360 | Acc: 0.9821 | F1: 0.9821 | 30.0s ‚úÖ\n    Ep   3/100 | Loss: 0.0200 | Acc: 0.9872 | F1: 0.9872 | 30.3s ‚úÖ\n    Ep   4/100 | Loss: 0.0117 | Acc: 0.9891 | F1: 0.9891 | 30.3s ‚úÖ\n    Ep   5/100 | Loss: 0.0067 | Acc: 0.9895 | F1: 0.9895 | 30.2s ‚úÖ\n    Ep   6/100 | Loss: 0.0039 | Acc: 0.9900 | F1: 0.9899 | 30.2s ‚úÖ\n    Ep   7/100 | Loss: 0.0022 | Acc: 0.9909 | F1: 0.9909 | 30.4s ‚úÖ\n    Ep   8/100 | Loss: 0.0017 | Acc: 0.9902 | F1: 0.9901 | 30.2s   \n    Ep   9/100 | Loss: 0.0010 | Acc: 0.9913 | F1: 0.9913 | 30.7s ‚úÖ\n    Ep  10/100 | Loss: 0.0007 | Acc: 0.9919 | F1: 0.9918 | 30.3s ‚úÖ\n    Ep  11/100 | Loss: 0.0004 | Acc: 0.9909 | F1: 0.9909 | 30.3s   \n    Ep  12/100 | Loss: 0.0003 | Acc: 0.9913 | F1: 0.9913 | 30.1s   \n    Ep  13/100 | Loss: 0.0003 | Acc: 0.9907 | F1: 0.9906 | 30.3s   \n    Ep  14/100 | Loss: 0.0002 | Acc: 0.9915 | F1: 0.9914 | 30.3s   \n    Ep  15/100 | Loss: 0.0003 | Acc: 0.9909 | F1: 0.9909 | 30.1s   \n    Ep  16/100 | Loss: 0.0002 | Acc: 0.9922 | F1: 0.9922 | 30.2s ‚úÖ\n    Ep  17/100 | Loss: 0.0002 | Acc: 0.9913 | F1: 0.9912 | 30.1s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9919 | 30.0s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9919 | 30.2s   \n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9918 | 30.5s   \n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9916 | 30.3s   \n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 30.3s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9918 | 30.2s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9917 | 29.7s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 29.7s   \n    Ep  26/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9918 | 29.8s   \n    ‚èπÔ∏è Early stopping at epoch 26. Best: 16\n\n  üîç CDG_tuned\n    Ep   1/100 | Loss: 0.1978 | Acc: 0.9764 | F1: 0.9762 | 29.8s ‚úÖ\n    Ep   2/100 | Loss: 0.0262 | Acc: 0.9796 | F1: 0.9795 | 29.8s ‚úÖ\n    Ep   3/100 | Loss: 0.0139 | Acc: 0.9859 | F1: 0.9858 | 29.7s ‚úÖ\n    Ep   4/100 | Loss: 0.0070 | Acc: 0.9853 | F1: 0.9852 | 29.7s   \n    Ep   5/100 | Loss: 0.0040 | Acc: 0.9896 | F1: 0.9895 | 29.8s ‚úÖ\n    Ep   6/100 | Loss: 0.0019 | Acc: 0.9894 | F1: 0.9893 | 29.7s   \n    Ep   7/100 | Loss: 0.0010 | Acc: 0.9898 | F1: 0.9898 | 29.9s ‚úÖ\n    Ep   8/100 | Loss: 0.0006 | Acc: 0.9892 | F1: 0.9891 | 29.7s   \n    Ep   9/100 | Loss: 0.0004 | Acc: 0.9904 | F1: 0.9903 | 29.7s ‚úÖ\n    Ep  10/100 | Loss: 0.0004 | Acc: 0.9907 | F1: 0.9906 | 29.8s ‚úÖ\n    Ep  11/100 | Loss: 0.0002 | Acc: 0.9912 | F1: 0.9911 | 29.7s ‚úÖ\n    Ep  12/100 | Loss: 0.0002 | Acc: 0.9908 | F1: 0.9907 | 29.7s   \n    Ep  13/100 | Loss: 0.0001 | Acc: 0.9910 | F1: 0.9910 | 29.8s   \n    Ep  14/100 | Loss: 0.0001 | Acc: 0.9905 | F1: 0.9904 | 29.7s   \n    Ep  15/100 | Loss: 0.0001 | Acc: 0.9911 | F1: 0.9911 | 29.7s   \n    Ep  16/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9916 | 29.7s ‚úÖ\n    Ep  17/100 | Loss: 0.0001 | Acc: 0.9913 | F1: 0.9913 | 29.6s   \n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9915 | F1: 0.9915 | 29.6s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9918 | F1: 0.9918 | 29.6s ‚úÖ\n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9912 | F1: 0.9911 | 29.6s   \n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9923 | F1: 0.9923 | 29.7s ‚úÖ\n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9922 | 29.7s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9921 | F1: 0.9920 | 29.7s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9916 | 29.8s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9915 | F1: 0.9914 | 30.0s   \n    Ep  26/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.7s   \n    Ep  27/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9919 | 29.9s   \n    Ep  28/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 29.7s   \n    Ep  29/100 | Loss: 0.0001 | Acc: 0.9912 | F1: 0.9911 | 29.8s   \n    Ep  30/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9916 | 29.8s   \n    Ep  31/100 | Loss: 0.0001 | Acc: 0.9914 | F1: 0.9914 | 29.7s   \n    ‚èπÔ∏è Early stopping at epoch 31. Best: 21\n\n============================================================\nüìÅ mnist | IF=100\n  ‚Üí mnist (IF=100): train=60000, test=10000, classes=10\n\n  üîç CE\n    Ep   1/100 | Loss: 0.3207 | Acc: 0.9784 | F1: 0.9783 | 29.2s ‚úÖ\n    Ep   2/100 | Loss: 0.0431 | Acc: 0.9839 | F1: 0.9837 | 29.4s ‚úÖ\n    Ep   3/100 | Loss: 0.0298 | Acc: 0.9862 | F1: 0.9861 | 29.3s ‚úÖ\n    Ep   4/100 | Loss: 0.0183 | Acc: 0.9892 | F1: 0.9892 | 29.3s ‚úÖ\n    Ep   5/100 | Loss: 0.0130 | Acc: 0.9891 | F1: 0.9890 | 29.2s   \n    Ep   6/100 | Loss: 0.0086 | Acc: 0.9905 | F1: 0.9904 | 29.3s ‚úÖ\n    Ep   8/100 | Loss: 0.0037 | Acc: 0.9902 | F1: 0.9901 | 29.2s   \n    Ep   9/100 | Loss: 0.0031 | Acc: 0.9912 | F1: 0.9911 | 29.5s ‚úÖ\n    Ep  10/100 | Loss: 0.0023 | Acc: 0.9910 | F1: 0.9909 | 29.2s   \n    Ep  11/100 | Loss: 0.0019 | Acc: 0.9926 | F1: 0.9925 | 29.2s ‚úÖ\n    Ep  12/100 | Loss: 0.0011 | Acc: 0.9925 | F1: 0.9924 | 29.1s   \n    Ep  13/100 | Loss: 0.0008 | Acc: 0.9916 | F1: 0.9916 | 29.3s   \n    Ep  14/100 | Loss: 0.0005 | Acc: 0.9917 | F1: 0.9917 | 29.4s   \n    Ep  15/100 | Loss: 0.0008 | Acc: 0.9925 | F1: 0.9924 | 29.4s   \n    Ep  16/100 | Loss: 0.0005 | Acc: 0.9914 | F1: 0.9913 | 29.2s   \n    Ep  17/100 | Loss: 0.0003 | Acc: 0.9929 | F1: 0.9929 | 29.6s ‚úÖ\n    Ep  18/100 | Loss: 0.0003 | Acc: 0.9927 | F1: 0.9927 | 29.2s   \n    Ep  19/100 | Loss: 0.0003 | Acc: 0.9936 | F1: 0.9935 | 29.4s ‚úÖ\n    Ep  20/100 | Loss: 0.0003 | Acc: 0.9926 | F1: 0.9926 | 29.1s   \n    Ep  21/100 | Loss: 0.0002 | Acc: 0.9921 | F1: 0.9921 | 29.3s   \n    Ep  22/100 | Loss: 0.0002 | Acc: 0.9925 | F1: 0.9925 | 29.4s   \n    Ep  23/100 | Loss: 0.0002 | Acc: 0.9928 | F1: 0.9928 | 29.5s   \n    Ep  24/100 | Loss: 0.0002 | Acc: 0.9931 | F1: 0.9931 | 29.3s   \n    Ep  25/100 | Loss: 0.0002 | Acc: 0.9923 | F1: 0.9923 | 29.2s   \n    Ep  26/100 | Loss: 0.0002 | Acc: 0.9926 | F1: 0.9926 | 29.3s   \n    Ep  27/100 | Loss: 0.0002 | Acc: 0.9933 | F1: 0.9933 | 29.3s   \n    Ep  28/100 | Loss: 0.0002 | Acc: 0.9930 | F1: 0.9930 | 29.2s   \n    Ep  29/100 | Loss: 0.0002 | Acc: 0.9925 | F1: 0.9924 | 29.1s   \n    ‚èπÔ∏è Early stopping at epoch 29. Best: 19\n\n  üîç Focal_g1\n    Ep   1/100 | Loss: 0.2487 | Acc: 0.9776 | F1: 0.9775 | 29.3s ‚úÖ\n    Ep   2/100 | Loss: 0.0299 | Acc: 0.9842 | F1: 0.9841 | 29.7s ‚úÖ\n    Ep   3/100 | Loss: 0.0190 | Acc: 0.9868 | F1: 0.9868 | 29.6s ‚úÖ\n    Ep   4/100 | Loss: 0.0118 | Acc: 0.9899 | F1: 0.9899 | 29.6s ‚úÖ\n    Ep   5/100 | Loss: 0.0076 | Acc: 0.9882 | F1: 0.9881 | 29.3s   \n    Ep   6/100 | Loss: 0.0050 | Acc: 0.9890 | F1: 0.9890 | 29.3s   \n    Ep   7/100 | Loss: 0.0039 | Acc: 0.9914 | F1: 0.9913 | 29.3s ‚úÖ\n    Ep   8/100 | Loss: 0.0020 | Acc: 0.9893 | F1: 0.9893 | 29.5s   \n    Ep   9/100 | Loss: 0.0015 | Acc: 0.9918 | F1: 0.9917 | 29.4s ‚úÖ\n    Ep  10/100 | Loss: 0.0010 | Acc: 0.9912 | F1: 0.9912 | 29.3s   \n    Ep  11/100 | Loss: 0.0011 | Acc: 0.9914 | F1: 0.9913 | 29.4s   \n    Ep  12/100 | Loss: 0.0005 | Acc: 0.9922 | F1: 0.9921 | 29.5s ‚úÖ\n    Ep  13/100 | Loss: 0.0005 | Acc: 0.9920 | F1: 0.9919 | 29.4s   \n    Ep  14/100 | Loss: 0.0004 | Acc: 0.9922 | F1: 0.9921 | 29.4s   \n    Ep  15/100 | Loss: 0.0004 | Acc: 0.9925 | F1: 0.9924 | 29.6s ‚úÖ\n    Ep  16/100 | Loss: 0.0002 | Acc: 0.9926 | F1: 0.9925 | 29.5s ‚úÖ\n    Ep  17/100 | Loss: 0.0002 | Acc: 0.9928 | F1: 0.9927 | 29.4s ‚úÖ\n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9929 | F1: 0.9929 | 29.6s ‚úÖ\n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9930 | F1: 0.9929 | 29.5s ‚úÖ\n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9931 | 29.5s ‚úÖ\n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9928 | F1: 0.9927 | 29.5s   \n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9927 | F1: 0.9926 | 29.8s   \n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9929 | F1: 0.9928 | 29.2s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9928 | F1: 0.9927 | 29.3s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9927 | F1: 0.9926 | 29.2s   \n    Ep  26/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9931 | 29.5s   \n    Ep  27/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 29.4s   \n    Ep  28/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 29.4s   \n    Ep  29/100 | Loss: 0.0001 | Acc: 0.9930 | F1: 0.9930 | 29.3s   \n    Ep  30/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9933 | 29.3s ‚úÖ\n    Ep  31/100 | Loss: 0.0001 | Acc: 0.9935 | F1: 0.9935 | 29.5s ‚úÖ\n    Ep  32/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9932 | 29.3s   \n    Ep  33/100 | Loss: 0.0001 | Acc: 0.9927 | F1: 0.9926 | 29.4s   \n    Ep  34/100 | Loss: 0.0001 | Acc: 0.9928 | F1: 0.9927 | 29.3s   \n    Ep  35/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 29.3s   \n    Ep  36/100 | Loss: 0.0001 | Acc: 0.9932 | F1: 0.9931 | 29.4s   \n    Ep  37/100 | Loss: 0.0001 | Acc: 0.9933 | F1: 0.9932 | 29.5s   \n    Ep  38/100 | Loss: 0.0001 | Acc: 0.9929 | F1: 0.9928 | 29.5s   \n    Ep  39/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9931 | 29.3s   \n    Ep  40/100 | Loss: 0.0001 | Acc: 0.9931 | F1: 0.9930 | 29.2s   \n    Ep  41/100 | Loss: 0.0002 | Acc: 0.9932 | F1: 0.9931 | 29.4s   \n    ‚èπÔ∏è Early stopping at epoch 41. Best: 31\n\n  üîç CBF_b0.999_g1\n    Ep   1/100 | Loss: 0.2476 | Acc: 0.9758 | F1: 0.9757 | 29.9s ‚úÖ\n    Ep   2/100 | Loss: 0.0295 | Acc: 0.9832 | F1: 0.9831 | 30.0s ‚úÖ\n    Ep   3/100 | Loss: 0.0192 | Acc: 0.9863 | F1: 0.9863 | 30.1s ‚úÖ\n    Ep   4/100 | Loss: 0.0109 | Acc: 0.9874 | F1: 0.9872 | 29.8s ‚úÖ\n    Ep   5/100 | Loss: 0.0074 | Acc: 0.9888 | F1: 0.9887 | 30.1s ‚úÖ\n    Ep   6/100 | Loss: 0.0058 | Acc: 0.9891 | F1: 0.9890 | 30.0s ‚úÖ\n    Ep   7/100 | Loss: 0.0038 | Acc: 0.9890 | F1: 0.9889 | 29.8s   \n    Ep   8/100 | Loss: 0.0019 | Acc: 0.9894 | F1: 0.9893 | 30.1s ‚úÖ\n    Ep   9/100 | Loss: 0.0020 | Acc: 0.9902 | F1: 0.9901 | 29.9s ‚úÖ\n    Ep  10/100 | Loss: 0.0012 | Acc: 0.9904 | F1: 0.9903 | 29.9s ‚úÖ\n    Ep  11/100 | Loss: 0.0013 | Acc: 0.9899 | F1: 0.9898 | 30.0s   \n    Ep  12/100 | Loss: 0.0007 | Acc: 0.9907 | F1: 0.9906 | 29.9s ‚úÖ\n    Ep  13/100 | Loss: 0.0005 | Acc: 0.9903 | F1: 0.9902 | 29.9s   \n    Ep  14/100 | Loss: 0.0005 | Acc: 0.9915 | F1: 0.9914 | 29.9s ‚úÖ\n    Ep  15/100 | Loss: 0.0004 | Acc: 0.9911 | F1: 0.9910 | 30.0s   \n    Ep  16/100 | Loss: 0.0002 | Acc: 0.9911 | F1: 0.9910 | 30.1s   \n    Ep  17/100 | Loss: 0.0002 | Acc: 0.9916 | F1: 0.9915 | 29.8s ‚úÖ\n    Ep  18/100 | Loss: 0.0001 | Acc: 0.9914 | F1: 0.9913 | 29.8s   \n    Ep  19/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 30.1s ‚úÖ\n    Ep  20/100 | Loss: 0.0001 | Acc: 0.9915 | F1: 0.9914 | 30.3s   \n    Ep  21/100 | Loss: 0.0001 | Acc: 0.9914 | F1: 0.9913 | 29.9s   \n    Ep  22/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 29.9s ‚úÖ\n    Ep  23/100 | Loss: 0.0001 | Acc: 0.9915 | F1: 0.9914 | 30.2s   \n    Ep  24/100 | Loss: 0.0001 | Acc: 0.9919 | F1: 0.9918 | 30.0s   \n    Ep  25/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.9s ‚úÖ\n    Ep  26/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9915 | 30.0s   \n    Ep  27/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.9s   \n    Ep  28/100 | Loss: 0.0001 | Acc: 0.9916 | F1: 0.9915 | 29.8s   \n    Ep  29/100 | Loss: 0.0001 | Acc: 0.9917 | F1: 0.9916 | 29.9s   \n    Ep  30/100 | Loss: 0.0001 | Acc: 0.9920 | F1: 0.9919 | 29.9s   \n    Ep  31/100 | Loss: 0.0001 | Acc: 0.9922 | F1: 0.9921 | 30.0s ‚úÖ\n","output_type":"stream"}],"execution_count":null}]}