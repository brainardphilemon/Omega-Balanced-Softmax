{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14094776,"sourceType":"datasetVersion","datasetId":8975488}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#### ============================================\n# CIFAR-100-LT + ResNet-32\n# Balanced Softmax + (2.7 - f_c)^(0.5 - p_t) weighting\n# Single-stage, 13k iterations, warmup + cosine\n# Augmentation as in EQL / BALMS (AutoAug + Cutout)\n# ============================================\n\nimport os\nimport math\nimport json\nimport random\nimport pickle\nfrom collections import Counter\n\nimport numpy as np\nfrom PIL import Image, ImageEnhance, ImageOps\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# ----------------------------\n#  Config\n# ----------------------------\n\nIMB_FACTOR = 100.0       # imbalance ratio\nBATCH_SIZE = 512\n\nTOTAL_ITERS = 13000\nWARMUP_ITERS = 800\n\nBASE_LR = 0.05           # start of warmup\nWARMUP_LR = 0.1          # target LR after warmup\nMOMENTUM = 0.9\nWEIGHT_DECAY = 5e-4\nEPS = 1e-6\nSEED = 42\n\n# >>> SET THIS to your CIFAR-100 pickle folder <<<\n# Folder must contain \"train\" and \"test\" pickles like standard CIFAR-100\nDATA_ROOT = \"/kaggle/input/hihihi\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\n# =========================================================\n#  Cutout\n# =========================================================\n\nclass Cutout(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        # img: Tensor [C, H, W]\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for _ in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1:y2, x1:x2] = 0.\n\n        mask = torch.from_numpy(mask).to(img.device if img.is_cuda else \"cpu\", img.dtype)\n        mask = mask.expand_as(img)\n        img = img * mask\n        return img\n\n# =========================================================\n#  AutoAugment CIFAR10 Policy (as used in EQL / BALMS)\n# =========================================================\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1,\n                 p2, operation2, magnitude_idx2,\n                 fillcolor=(128, 128, 128)):\n        ranges = {\n            \"shearX\": np.linspace(0, 0.3, 10),\n            \"shearY\": np.linspace(0, 0.3, 10),\n            \"translateX\": np.linspace(0, 150 / 331, 10),\n            \"translateY\": np.linspace(0, 150 / 331, 10),\n            \"rotate\": np.linspace(0, 30, 10),\n            \"color\": np.linspace(0.0, 0.9, 10),\n            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(int),\n            \"solarize\": np.linspace(256, 0, 10),\n            \"contrast\": np.linspace(0.0, 0.9, 10),\n            \"sharpness\": np.linspace(0.0, 0.9, 10),\n            \"brightness\": np.linspace(0.0, 0.9, 10),\n            \"autocontrast\": [0] * 10,\n            \"equalize\": [0] * 10,\n            \"invert\": [0] * 10,\n        }\n\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(\"RGBA\").rotate(magnitude)\n            return Image.composite(\n                rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot\n            ).convert(img.mode)\n\n        func = {\n            \"shearX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE,\n                (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"shearY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE,\n                (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"translateX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE,\n                (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            \"translateY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE,\n                (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, int(magnitude)),\n            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n            \"invert\": lambda img, magnitude: ImageOps.invert(img),\n        }\n\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n    def __call__(self, img):\n        if random.random() < self.p1:\n            img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2:\n            img = self.operation2(img, self.magnitude2)\n        return img\n\n\nclass CIFAR10Policy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor),\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR10 Policy\"\n\n# =========================================================\n#  Custom CIFAR-100-LT loader (uses raw train/test/meta)\n# =========================================================\n\nclass CIFAR100LT(Dataset):\n    cls_num = 100\n    dataset_name = \"CIFAR-100-LT\"\n\n    def __init__(self, root, phase, imbalance_ratio=1.0, imb_type=\"exp\"):\n        \"\"\"\n        root: folder containing 'train', 'test', 'meta'\n        phase: 'train' or 'test'\n        \"\"\"\n        self.root = root\n        self.train = (phase == \"train\")\n\n        data_file = \"train\" if self.train else \"test\"\n        path = os.path.join(root, data_file)\n        print(f\"Loading CIFAR-100 from {path}\")\n        with open(path, \"rb\") as f:\n            entry = pickle.load(f, encoding=\"latin1\")\n\n        self.data = entry[\"data\"]\n        self.data = self.data.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n        self.targets = entry[\"fine_labels\"]\n\n        if self.train:\n            self.img_num_per_cls = self.get_img_num_per_cls(\n                self.cls_num, imb_type, imbalance_ratio\n            )\n            self.gen_imbalanced_data(self.img_num_per_cls)\n\n            # Augmentation as in EQL/BALMS: crop + flip + AutoAug + Cutout + norm\n            self.transform = transforms.Compose([\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                CIFAR10Policy(),\n                transforms.ToTensor(),\n                Cutout(n_holes=1, length=16),\n                transforms.Normalize(\n                    (0.4914, 0.4822, 0.4465),\n                    (0.2023, 0.1994, 0.2010),\n                ),\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    (0.4914, 0.4822, 0.4465),\n                    (0.2023, 0.1994, 0.2010),\n                ),\n            ])\n\n        self.labels = self.targets\n        print(f\"{phase} Mode: Contain {len(self.data)} images\")\n\n    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n        gamma = 1.0 / imb_factor\n        img_max = len(self.data) / cls_num\n        img_num_per_cls = []\n        if imb_type == \"exp\":\n            for cls_idx in range(cls_num):\n                num = img_max * (gamma ** (cls_idx / (cls_num - 1.0)))\n                img_num_per_cls.append(int(num))\n        elif imb_type == \"step\":\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max))\n            for cls_idx in range(cls_num // 2):\n                img_num_per_cls.append(int(img_max * gamma))\n        else:\n            img_num_per_cls.extend([int(img_max)] * cls_num)\n\n        os.makedirs(\"cls_freq\", exist_ok=True)\n        freq_path = os.path.join(\n            \"cls_freq\", self.dataset_name + \"_IMBA{}.json\".format(imb_factor)\n        )\n        with open(freq_path, \"w\") as fd:\n            json.dump(img_num_per_cls, fd)\n\n        return img_num_per_cls\n\n    def gen_imbalanced_data(self, img_num_per_cls):\n        new_data = []\n        new_targets = []\n        targets_np = np.array(self.targets, dtype=np.int64)\n        classes = np.unique(targets_np)\n\n        self.num_per_cls_dict = {}\n        for the_class, the_img_num in zip(classes, img_num_per_cls):\n            self.num_per_cls_dict[the_class] = the_img_num\n            idx = np.where(targets_np == the_class)[0]\n            np.random.shuffle(idx)\n            selec_idx = idx[:the_img_num]\n            new_data.append(self.data[selec_idx, ...])\n            new_targets.extend([the_class] * the_img_num)\n        new_data = np.vstack(new_data)\n        self.data = new_data\n        self.targets = new_targets\n\n    def __getitem__(self, index):\n        img, label = self.data[index], self.labels[index]\n        img = Image.fromarray(img)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, label, index\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_num_classes(self):\n        return self.cls_num\n\n    def get_cls_num_list(self):\n        count = Counter(self.labels)\n        return [count[i] for i in range(self.cls_num)]\n\n# =========================================================\n#  CIFAR ResNet-32 (feature backbone)\n# =========================================================\n\ndef _weights_init(m):\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super().__init__()\n        self.lambd = lambd\n    def forward(self, x):\n        return self.lambd(x)\n\nclass BasicBlockCifar(nn.Module):\n    expansion = 1\n    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == \"A\":\n                self.shortcut = LambdaLayer(\n                    lambda x: F.pad(\n                        x[:, :, ::2, ::2],\n                        (0, 0, 0, 0, planes // 4, planes // 4),\n                        \"constant\", 0,\n                    )\n                )\n            elif option == \"B\":\n                self.shortcut = nn.Sequential(\n                    nn.Conv2d(\n                        in_planes,\n                        self.expansion * planes,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False,\n                    ),\n                    nn.BatchNorm2d(self.expansion * planes),\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out, inplace=True)\n        return out\n\nclass ResNet_Cifar(nn.Module):\n    def __init__(self, block, num_blocks):\n        super().__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(\n            3, 16, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n\n        self.apply(_weights_init)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for s in strides:\n            layers.append(block(self.in_planes, planes, s))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, **kwargs):\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n\n        feature_maps = out\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)   # [B, 64]\n        return out, feature_maps\n\nclass DotProductClassifier(nn.Module):\n    def __init__(self, feat_dim=64, num_classes=100):\n        super().__init__()\n        self.fc = nn.Linear(feat_dim, num_classes)\n    def forward(self, x):\n        return self.fc(x)\n\n# =========================================================\n#  Balanced Softmax + (2.7 - f_c)^(0.5 - p_t) weighting\n#   logits'_k = z_k + log(n_k)\n#   p = softmax(logits')\n#   L_i = (2.7 - f_c)^(0.5 - p_t) * (-log p_t)\n# =========================================================\n\nclass BalancedSoftmaxExpWeightedLoss(nn.Module):\n    \"\"\"\n    Balanced Softmax with per-sample weight:\n\n        logits'_k = z_k + log(n_k)\n        p = softmax(logits')\n        p_t = p_{true}\n\n        L_i = (2.7 - f_{c_i})^(0.5 - p_t) * (-log p_t)\n\n    where:\n        f_c = normalized class frequency for class c.\n        2.7 ~ exp(1).\n    \"\"\"\n    def __init__(self, class_counts, eps=1e-12, base=2.7):\n        super().__init__()\n        counts = np.array(class_counts, dtype=np.float32)\n        counts = counts + eps           # avoid log(0)\n        freqs = counts / counts.sum()   # f_c\n\n        log_counts = np.log(counts)\n\n        self.register_buffer(\"log_counts\",\n                             torch.tensor(log_counts, dtype=torch.float32))\n        self.register_buffer(\"freqs\",\n                             torch.tensor(freqs, dtype=torch.float32))\n\n        self.base = float(base)\n\n    def set_state(self, it=None, lr=None):\n        # no-op to keep training loop unchanged\n        return\n\n    def forward(self, logits, targets):\n        # Balanced Softmax logits: z_k + log(n_k)\n        balanced_logits = logits + self.log_counts.unsqueeze(0)   # [B, C]\n\n        # log p and p under balanced softmax\n        log_probs = F.log_softmax(balanced_logits, dim=1)         # [B, C]\n        probs = log_probs.exp()                                   # [B, C]\n\n        # p_t for each sample\n        pt = probs.gather(1, targets.view(-1, 1)).squeeze(1)      # [B]\n\n        # f_c for each sample\n        freqs_t = self.freqs[targets]                             # [B]\n\n        # (2.7 - f_c)^(0.5 - p_t)\n        base_t = self.base - freqs_t                              # [B], > 0\n        weight = torch.pow(base_t, 0.75 - pt)                      # [B]\n\n        # CE: -log p_t\n        ce_per_sample = -log_probs.gather(1, targets.view(-1, 1)).squeeze(1)  # [B]\n\n        loss = (weight * ce_per_sample).mean()\n        return loss\n\n# =========================================================\n#  Iter-based LR scheduler (cosine with warmup)\n# =========================================================\n\nclass IterLRScheduler:\n    def __init__(self, optimizer, lr_init, lr_base, warmup_iters, total_iters):\n        self.optimizer = optimizer\n        self.lr_init = lr_init\n        self.lr_base = lr_base\n        self.warmup_iters = max(1, warmup_iters)\n        self.total_iters = max(1, total_iters)\n        self.iter = 0\n\n    def step(self):\n        self.iter += 1\n        t = self.iter\n        if t <= self.warmup_iters:\n            ratio = t / float(self.warmup_iters)\n            lr = self.lr_init + (self.lr_base - self.lr_init) * ratio\n        else:\n            decay_iter = t - self.warmup_iters\n            decay_total = max(1, self.total_iters - self.warmup_iters)\n            cosine = 0.5 * (1.0 + math.cos(math.pi * decay_iter / decay_total))\n            lr = self.lr_base * cosine\n\n        for pg in self.optimizer.param_groups:\n            pg[\"lr\"] = lr\n\n# =========================================================\n#  Build datasets & loaders\n# =========================================================\n\ntrain_dataset = CIFAR100LT(\n    root=DATA_ROOT,\n    phase=\"train\",\n    imbalance_ratio=IMB_FACTOR,\n    imb_type='exp',\n)\ntest_dataset = CIFAR100LT(\n    root=DATA_ROOT,\n    phase=\"test\",\n    imbalance_ratio=1.0,\n    imb_type='exp',   # unused for test, just for consistency\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=0,\n    pin_memory=True,\n    drop_last=True,\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=256,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True,\n)\n\nclass_counts = train_dataset.get_cls_num_list()\nprint(\"Class counts (first 10):\", class_counts[:10])\n\n# =========================================================\n#  Model, loss, optimizer, scheduler\n# =========================================================\n\nbackbone = ResNet_Cifar(BasicBlockCifar, [5, 5, 5]).to(device)  # ResNet-32\nclassifier = DotProductClassifier(feat_dim=64, num_classes=100).to(device)\n\ncriterion = BalancedSoftmaxExpWeightedLoss(\n    class_counts,\n    eps=1e-12,\n    base=2.718,   # ~ exp(1)\n).to(device)\n\noptimizer = torch.optim.SGD(\n    list(backbone.parameters()) + list(classifier.parameters()),\n    lr=BASE_LR,\n    momentum=MOMENTUM,\n    weight_decay=WEIGHT_DECAY,\n    nesterov=True,\n)\n\nscheduler = IterLRScheduler(\n    optimizer,\n    lr_init=BASE_LR,\n    lr_base=WARMUP_LR,\n    warmup_iters=WARMUP_ITERS,\n    total_iters=TOTAL_ITERS,\n)\n\n# =========================================================\n#  Eval helper\n# =========================================================\n\n@torch.no_grad()\ndef evaluate():\n    backbone.eval()\n    classifier.eval()\n    ce = nn.CrossEntropyLoss(reduction=\"sum\").to(device)\n    total_loss = 0.0\n    total_correct = 0\n    total = 0\n    for x, y, idx in test_loader:\n        x, y = x.to(device), y.to(device)\n        feats, _ = backbone(x)\n        logits = classifier(feats)\n        loss = ce(logits, y)\n        total_loss += loss.item()\n        preds = logits.argmax(1)\n        total_correct += (preds == y).sum().item()\n        total += y.size(0)\n    return total_loss / total, total_correct / total\n\n# =========================================================\n#  Training loop (13k iterations, single stage)\n# =========================================================\n\nbest_acc = 0.0\nit = 0\n\nprint(f\"Starting single-stage training: TOTAL_ITERS={TOTAL_ITERS}, \"\n      f\"WARMUP_ITERS={WARMUP_ITERS}, IMB_FACTOR={IMB_FACTOR}\")\n\nwhile it < TOTAL_ITERS:\n    for x, y, idx in train_loader:\n        it += 1\n        if it > TOTAL_ITERS:\n            break\n\n        x, y = x.to(device), y.to(device)\n\n        backbone.train()\n        classifier.train()\n        scheduler.step()\n\n        # get current LR and update loss state (no-op here, but kept for compatibility)\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        criterion.set_state(it, current_lr)\n\n        feats, _ = backbone(x)\n        logits = classifier(feats)\n        loss = criterion(logits, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if it % 100 == 0:\n            lr = optimizer.param_groups[0][\"lr\"]\n            with torch.no_grad():\n                batch_acc = (logits.argmax(1) == y).float().mean().item()\n            print(\n                f\"[Iter {it}/{TOTAL_ITERS}] \"\n                f\"loss={loss.item():.4f}, \"\n                f\"train_acc={batch_acc*100:.2f}%, \"\n                f\"lr={lr:.5f}\"\n            )\n\n        if it % 1000 == 0 or it == TOTAL_ITERS:\n            val_loss, val_acc = evaluate()\n            best_acc = max(best_acc, val_acc)\n            print(\n                f\"  Eval @ iter {it}: \"\n                f\"val_loss={val_loss:.4f}, \"\n                f\"val_acc={val_acc*100:.2f}%, \"\n                f\"best={best_acc*100:.2f}%\"\n            )\n\nprint(f\"Training complete. Best test accuracy: {best_acc*100:.2f}%\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T15:35:51.901001Z","iopub.execute_input":"2025-12-11T15:35:51.901312Z","iopub.status.idle":"2025-12-11T16:42:26.429656Z","shell.execute_reply.started":"2025-12-11T15:35:51.901288Z","shell.execute_reply":"2025-12-11T16:42:26.428856Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading CIFAR-100 from /kaggle/input/hihihi/train\ntrain Mode: Contain 12608 images\nLoading CIFAR-100 from /kaggle/input/hihihi/test\ntest Mode: Contain 10000 images\nClass counts (first 10): [500, 480, 462, 444, 426, 410, 394, 379, 364, 350]\nStarting single-stage training: TOTAL_ITERS=13000, WARMUP_ITERS=800, IMB_FACTOR=50.0\n[Iter 100/13000] loss=3.8855, train_acc=9.57%, lr=0.05625\n[Iter 200/13000] loss=3.4933, train_acc=16.99%, lr=0.06250\n[Iter 300/13000] loss=3.2418, train_acc=21.29%, lr=0.06875\n[Iter 400/13000] loss=3.2090, train_acc=23.05%, lr=0.07500\n[Iter 500/13000] loss=3.0154, train_acc=28.32%, lr=0.08125\n[Iter 600/13000] loss=2.6500, train_acc=32.42%, lr=0.08750\n[Iter 700/13000] loss=2.4229, train_acc=38.48%, lr=0.09375\n[Iter 800/13000] loss=2.5885, train_acc=33.98%, lr=0.10000\n[Iter 900/13000] loss=2.1899, train_acc=41.21%, lr=0.09998\n[Iter 1000/13000] loss=2.2780, train_acc=43.75%, lr=0.09993\n  Eval @ iter 1000: val_loss=3.7443, val_acc=23.44%, best=23.44%\n[Iter 1100/13000] loss=2.2582, train_acc=40.04%, lr=0.09985\n[Iter 1200/13000] loss=2.0709, train_acc=45.12%, lr=0.09973\n[Iter 1300/13000] loss=1.9418, train_acc=48.05%, lr=0.09959\n[Iter 1400/13000] loss=1.9243, train_acc=50.59%, lr=0.09940\n[Iter 1500/13000] loss=2.0058, train_acc=46.09%, lr=0.09919\n[Iter 1600/13000] loss=1.8932, train_acc=50.59%, lr=0.09894\n[Iter 1700/13000] loss=1.8177, train_acc=51.37%, lr=0.09866\n[Iter 1800/13000] loss=1.9243, train_acc=48.24%, lr=0.09835\n[Iter 1900/13000] loss=1.7830, train_acc=54.10%, lr=0.09801\n[Iter 2000/13000] loss=1.7901, train_acc=51.17%, lr=0.09763\n  Eval @ iter 2000: val_loss=3.1561, val_acc=31.18%, best=31.18%\n[Iter 2100/13000] loss=1.7024, train_acc=54.88%, lr=0.09722\n[Iter 2200/13000] loss=1.7212, train_acc=56.25%, lr=0.09679\n[Iter 2300/13000] loss=1.6197, train_acc=56.84%, lr=0.09632\n[Iter 2400/13000] loss=1.6072, train_acc=53.91%, lr=0.09582\n[Iter 2500/13000] loss=1.4939, train_acc=57.23%, lr=0.09529\n[Iter 2600/13000] loss=1.6643, train_acc=53.12%, lr=0.09472\n[Iter 2700/13000] loss=1.5427, train_acc=58.40%, lr=0.09413\n[Iter 2800/13000] loss=1.4964, train_acc=60.35%, lr=0.09351\n[Iter 2900/13000] loss=1.5987, train_acc=57.42%, lr=0.09287\n[Iter 3000/13000] loss=1.6186, train_acc=54.69%, lr=0.09219\n  Eval @ iter 3000: val_loss=2.8731, val_acc=35.08%, best=35.08%\n[Iter 3100/13000] loss=1.4934, train_acc=60.55%, lr=0.09148\n[Iter 3200/13000] loss=1.4941, train_acc=60.35%, lr=0.09075\n[Iter 3300/13000] loss=1.5216, train_acc=58.20%, lr=0.08999\n[Iter 3400/13000] loss=1.4444, train_acc=60.94%, lr=0.08921\n[Iter 3500/13000] loss=1.5337, train_acc=58.59%, lr=0.08839\n[Iter 3600/13000] loss=1.5278, train_acc=59.18%, lr=0.08756\n[Iter 3700/13000] loss=1.4399, train_acc=61.52%, lr=0.08669\n[Iter 3800/13000] loss=1.3242, train_acc=63.28%, lr=0.08581\n[Iter 3900/13000] loss=1.3087, train_acc=64.84%, lr=0.08490\n[Iter 4000/13000] loss=1.2298, train_acc=66.21%, lr=0.08396\n  Eval @ iter 4000: val_loss=3.0023, val_acc=36.10%, best=36.10%\n[Iter 4100/13000] loss=1.3541, train_acc=64.26%, lr=0.08301\n[Iter 4200/13000] loss=1.3988, train_acc=62.30%, lr=0.08203\n[Iter 4300/13000] loss=1.3207, train_acc=64.45%, lr=0.08103\n[Iter 4400/13000] loss=1.3706, train_acc=60.74%, lr=0.08001\n[Iter 4500/13000] loss=1.4196, train_acc=60.55%, lr=0.07897\n[Iter 4600/13000] loss=1.2191, train_acc=65.23%, lr=0.07791\n[Iter 4700/13000] loss=1.4212, train_acc=58.59%, lr=0.07683\n[Iter 4800/13000] loss=1.3213, train_acc=64.06%, lr=0.07574\n[Iter 4900/13000] loss=1.1980, train_acc=65.43%, lr=0.07463\n[Iter 5000/13000] loss=1.0800, train_acc=69.14%, lr=0.07350\n  Eval @ iter 5000: val_loss=2.6843, val_acc=41.39%, best=41.39%\n[Iter 5100/13000] loss=1.2279, train_acc=64.84%, lr=0.07235\n[Iter 5200/13000] loss=1.2751, train_acc=64.84%, lr=0.07120\n[Iter 5300/13000] loss=1.1827, train_acc=67.19%, lr=0.07002\n[Iter 5400/13000] loss=1.3070, train_acc=62.89%, lr=0.06884\n[Iter 5500/13000] loss=1.0673, train_acc=70.31%, lr=0.06764\n[Iter 5600/13000] loss=1.2225, train_acc=65.62%, lr=0.06643\n[Iter 5700/13000] loss=1.2814, train_acc=63.67%, lr=0.06521\n[Iter 5800/13000] loss=1.2068, train_acc=67.97%, lr=0.06397\n[Iter 5900/13000] loss=1.2360, train_acc=67.38%, lr=0.06273\n[Iter 6000/13000] loss=1.2237, train_acc=65.62%, lr=0.06148\n  Eval @ iter 6000: val_loss=3.0402, val_acc=39.73%, best=41.39%\n[Iter 6100/13000] loss=0.9589, train_acc=71.88%, lr=0.06023\n[Iter 6200/13000] loss=1.0371, train_acc=72.66%, lr=0.05896\n[Iter 6300/13000] loss=1.0746, train_acc=70.70%, lr=0.05769\n[Iter 6400/13000] loss=1.1327, train_acc=68.95%, lr=0.05642\n[Iter 6500/13000] loss=1.0840, train_acc=69.34%, lr=0.05514\n[Iter 6600/13000] loss=1.1401, train_acc=68.55%, lr=0.05386\n[Iter 6700/13000] loss=1.1093, train_acc=69.34%, lr=0.05257\n[Iter 6800/13000] loss=1.0159, train_acc=72.27%, lr=0.05129\n[Iter 6900/13000] loss=1.0858, train_acc=70.51%, lr=0.05000\n[Iter 7000/13000] loss=0.9637, train_acc=70.51%, lr=0.04871\n  Eval @ iter 7000: val_loss=2.6840, val_acc=43.53%, best=43.53%\n[Iter 7100/13000] loss=1.0883, train_acc=70.70%, lr=0.04743\n[Iter 7200/13000] loss=1.0751, train_acc=70.12%, lr=0.04614\n[Iter 7300/13000] loss=1.0503, train_acc=71.68%, lr=0.04486\n[Iter 7400/13000] loss=1.0762, train_acc=70.12%, lr=0.04358\n[Iter 7500/13000] loss=0.8968, train_acc=73.44%, lr=0.04231\n[Iter 7600/13000] loss=1.0890, train_acc=69.73%, lr=0.04104\n[Iter 7700/13000] loss=1.0135, train_acc=72.85%, lr=0.03977\n[Iter 7800/13000] loss=1.0441, train_acc=72.46%, lr=0.03852\n[Iter 7900/13000] loss=0.9819, train_acc=73.05%, lr=0.03727\n[Iter 8000/13000] loss=0.9968, train_acc=70.31%, lr=0.03603\n  Eval @ iter 8000: val_loss=2.6849, val_acc=43.92%, best=43.92%\n[Iter 8100/13000] loss=1.0276, train_acc=71.09%, lr=0.03479\n[Iter 8200/13000] loss=0.9193, train_acc=74.02%, lr=0.03357\n[Iter 8300/13000] loss=0.9264, train_acc=74.22%, lr=0.03236\n[Iter 8400/13000] loss=0.9499, train_acc=74.61%, lr=0.03116\n[Iter 8500/13000] loss=0.8466, train_acc=76.17%, lr=0.02998\n[Iter 8600/13000] loss=0.8922, train_acc=75.98%, lr=0.02880\n[Iter 8700/13000] loss=0.9693, train_acc=73.05%, lr=0.02765\n[Iter 8800/13000] loss=0.8352, train_acc=75.98%, lr=0.02650\n[Iter 8900/13000] loss=0.8237, train_acc=75.00%, lr=0.02537\n[Iter 9000/13000] loss=0.8965, train_acc=73.63%, lr=0.02426\n  Eval @ iter 9000: val_loss=2.7414, val_acc=44.74%, best=44.74%\n[Iter 9100/13000] loss=0.8132, train_acc=77.93%, lr=0.02317\n[Iter 9200/13000] loss=0.8554, train_acc=75.39%, lr=0.02209\n[Iter 9300/13000] loss=0.6954, train_acc=80.47%, lr=0.02103\n[Iter 9400/13000] loss=0.7746, train_acc=76.56%, lr=0.01999\n[Iter 9500/13000] loss=0.7576, train_acc=76.56%, lr=0.01897\n[Iter 9600/13000] loss=0.8438, train_acc=78.71%, lr=0.01797\n[Iter 9700/13000] loss=0.7131, train_acc=79.30%, lr=0.01699\n[Iter 9800/13000] loss=0.6863, train_acc=80.47%, lr=0.01604\n[Iter 9900/13000] loss=0.7801, train_acc=77.93%, lr=0.01510\n[Iter 10000/13000] loss=0.7845, train_acc=77.54%, lr=0.01419\n  Eval @ iter 10000: val_loss=2.6274, val_acc=46.84%, best=46.84%\n[Iter 10100/13000] loss=0.7124, train_acc=81.84%, lr=0.01331\n[Iter 10200/13000] loss=0.7122, train_acc=80.27%, lr=0.01244\n[Iter 10300/13000] loss=0.7233, train_acc=81.05%, lr=0.01161\n[Iter 10400/13000] loss=0.7176, train_acc=81.45%, lr=0.01079\n[Iter 10500/13000] loss=0.8103, train_acc=77.54%, lr=0.01001\n[Iter 10600/13000] loss=0.5650, train_acc=83.98%, lr=0.00925\n[Iter 10700/13000] loss=0.7067, train_acc=80.08%, lr=0.00852\n[Iter 10800/13000] loss=0.7340, train_acc=79.49%, lr=0.00781\n[Iter 10900/13000] loss=0.6522, train_acc=81.84%, lr=0.00713\n[Iter 11000/13000] loss=0.6261, train_acc=81.64%, lr=0.00649\n  Eval @ iter 11000: val_loss=2.4404, val_acc=49.25%, best=49.25%\n[Iter 11100/13000] loss=0.6254, train_acc=82.23%, lr=0.00587\n[Iter 11200/13000] loss=0.6322, train_acc=83.40%, lr=0.00528\n[Iter 11300/13000] loss=0.6478, train_acc=81.64%, lr=0.00471\n[Iter 11400/13000] loss=0.5729, train_acc=83.79%, lr=0.00418\n[Iter 11500/13000] loss=0.6708, train_acc=81.84%, lr=0.00368\n[Iter 11600/13000] loss=0.6452, train_acc=82.81%, lr=0.00321\n[Iter 11700/13000] loss=0.6425, train_acc=82.62%, lr=0.00278\n[Iter 11800/13000] loss=0.5766, train_acc=84.57%, lr=0.00237\n[Iter 11900/13000] loss=0.6144, train_acc=82.81%, lr=0.00199\n[Iter 12000/13000] loss=0.6547, train_acc=82.42%, lr=0.00165\n  Eval @ iter 12000: val_loss=2.3831, val_acc=50.40%, best=50.40%\n[Iter 12100/13000] loss=0.5429, train_acc=84.18%, lr=0.00134\n[Iter 12200/13000] loss=0.6065, train_acc=82.03%, lr=0.00106\n[Iter 12300/13000] loss=0.5346, train_acc=85.74%, lr=0.00081\n[Iter 12400/13000] loss=0.6308, train_acc=83.01%, lr=0.00060\n[Iter 12500/13000] loss=0.5611, train_acc=84.57%, lr=0.00041\n[Iter 12600/13000] loss=0.5454, train_acc=85.55%, lr=0.00027\n[Iter 12700/13000] loss=0.6300, train_acc=81.84%, lr=0.00015\n[Iter 12800/13000] loss=0.5515, train_acc=85.16%, lr=0.00007\n[Iter 12900/13000] loss=0.6157, train_acc=83.59%, lr=0.00002\n[Iter 13000/13000] loss=0.5325, train_acc=85.55%, lr=0.00000\n  Eval @ iter 13000: val_loss=2.3631, val_acc=50.50%, best=50.50%\nTraining complete. Best test accuracy: 50.50%\n","output_type":"stream"}],"execution_count":7}]}